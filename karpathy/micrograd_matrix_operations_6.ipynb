{"cells":[{"cell_type":"markdown","metadata":{},"source":["### [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":271,"metadata":{},"outputs":[],"source":["import math, random, torch\n","import numpy as np\n","# import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":272,"metadata":{},"outputs":[],"source":["def plot_losses(losses):\n","  # import matplotlib.pyplot as plt\n","  \n","  # Create a list of iterations\n","  iterations = range(len(losses))\n","\n","  # Plot the loss as a function of iteration\n","  plt.plot(iterations, losses)\n","\n","  # Add a title to the plot\n","  plt.title('Loss vs. Iteration')\n","\n","  # Add labels to the x-axis and y-axis\n","  plt.xlabel('Iteration')\n","  plt.ylabel('Loss')"]},{"cell_type":"code","execution_count":273,"metadata":{},"outputs":[],"source":["def print_parameters(parameters):\n","  # number of parameters (e.g sum (weights + bias to each neuron and output))\n","  # MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","  print(f'Number of parameters in MLP(2, [3, 3, 1]): {len(parameters())}\\n')\n","\n","  # print first 5 parameters\n","  for i, v in enumerate(parameters()):\n","    if i < 5:\n","      print(f'i: {i:>2}, {v.data:>14.10f}')\n","  \n","  print('---')\n","\n","  # print last 5 parameters   \n","  for i, v in enumerate(parameters()):\n","    if i >= len(parameters()) - 5:\n","      print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"code","execution_count":274,"metadata":{},"outputs":[],"source":["def get_wt_n_b_mats(layers, verbose=False):\n","  ''' Get neuron's weights and bias for each layer.\n","  Inputs: If n = MLP(2, [3, 3, 1]), input is n.layers.\n","\n","  return: two lists of np.arrays. The first list is weight matrix for each layer\n","          The second list is the bias matrix for each layer \n","  '''\n","  layer_cnt = len(layers)  # number of layers\n","  w_mats = []  # list of weights matrix for each layer \n","  b_mats = []  # list of bias matrix for each layer\n","  if verbose:\n","    print(f'layer_cnt: {layer_cnt}\\n')\n","  for i, layer in enumerate(layers):\n","      neuron_cnt = len(layer.neurons)  # numbers of neurons in the layer\n","      if verbose: \n","        print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","        print('----')\n","      b_mat = []  # accumulate neuon's bias for each row     \n","      for j, neuron in enumerate(layer.neurons):\n","          if verbose:\n","            print(f'layer: {i}, neuron {j}')\n","          b = neuron.b.data  # bias of neuron \n","          w_row = []  # accumulate neuon's weights for each row\n","          b_row = []  # accumulate neuon's bias for each row\n","          for k, w in enumerate(neuron.w):\n","              w_row.append(w.data)\n","              if verbose:\n","                print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","          if j == 0:            \n","              w_mat = np.array([w_row])\n","          else:\n","              w_mat = np.vstack((w_mat, w_row))\n","          \n","          b_mat.append(b)\n","          if verbose:\n","            print(f'b:  {b:10.7f}\\n')\n","            print(f'b:  {b:10.7f}')        \n","            print(f'b_mat:  {b_mat}\\n')\n","      w_mats.append(w_mat)  \n","      b_mats.append(np.array([b_mat]))        \n","      if verbose:\n","          print('------')\n","\n","  zipped_w_n_b = zip(w_mats, b_mats)\n","  if verbose:\n","    for i, w_n_b in enumerate(zipped_w_n_b):\n","      print(f'layer: {i}')  # 1st layer is 0    \n","      print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","      print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","\n","  return w_mats, b_mats"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions<br>* limited to neural network with one output, e.g. MLP(2, [3, 1])<br>* neural network with multiple outputs, e.g.  MLP(2, [3, 3]), will produce errors in backward pass "]},{"cell_type":"code","execution_count":275,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":276,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":277,"metadata":{},"outputs":[],"source":["class Neuron:\n","    \n","    def __init__(self, nin):\n","        # random numbers evenly distributed between -1 and 1    \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]  \n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["### Neurons of Human Brain\n","![](..\\karpathy\\img\\neuron_of_human_brain.png)"]},{"cell_type":"markdown","metadata":{},"source":["### Basic Artificial Neuron Function\n","\n","<img src=\"..\\karpathy\\img\\Basic Neuron Function.png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Simple Artificial Neural Network<br>* input layer: 2 nodes<br>* hidden layer 1: 3 nodes<br>* hidden layer 2: 3 nodes<br>*  output layer: 1 node<br>* bias and activation of nodes are not shown\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\MLP (2, [3, 3, 1]).png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Hidden Layer Matrix Operations<br>* Hidden layer with two inputs (X1, X2), and three neurons (b1, b2, b3)<br>* Two sets of inputs (X1, X2) are shown in different shades of gray<br>* Two sets of outputs (Y1, Y2, Y3) are shown in corresponding shades of gray<br>* Multiple sets of inputs are processed in one matrix operation \n","\n","<img src=\"..\\karpathy\\img\\Hidden Layer Matrix Operations.png\">"]},{"cell_type":"code","execution_count":278,"metadata":{},"outputs":[],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only"]},{"cell_type":"markdown","metadata":{},"source":["##### ---- Create Simple Neural Network MLP(2, [3, 3, 1]) ----<br>* 2 input nodes<br>* 3 neurons in hidden layer 1<br>* 3 neurons in hidden layer 2<br>* 1 output node\n","##### ---- Parameters ----<br>* initialize neuron parameters with random numbers<br>* parameters in layer 1: 3 neurons * (2 inputs + 1 bias) = 9<br>* parameters in layer 2: 3 neurons * (3 neurons + 1 bias) = 12<br>* parameters in layer 3: 1 output * (3 neurons + 1 bias) = 4<br>*  parameters total: 25\n","##### ---- Inputs ----<br>* 1st set: [2.0, 3.0]<br>* 2nd set: [3.0, -1.0]\n","##### ---- Desired Output ----<br>* [1.0, -1.0] for all inputs\n","##### ---- Learning Rate ----<br>* 0.05"]},{"cell_type":"code","execution_count":279,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(2, [3, 3, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0],\n","  [3.0, -1.0]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":280,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initalize neuron weights and bias with random numbers\n","Number of parameters in MLP(2, [3, 3, 1]): 25\n","\n","i:  0,   0.9594606936\n","i:  1,  -0.2064132580\n","i:  2,  -0.6543419960\n","i:  3,  -0.0930072127\n","i:  4,   0.9062582394\n","---\n","i: 20,  -0.5573518384\n","i: 21,   0.6954353876\n","i: 22,  -0.5315587348\n","i: 23,  -0.5156266281\n","i: 24,  -0.6228903441\n"]}],"source":["if verbose:\n","  print(\"Initalize neuron weights and bias with random numbers\")\n","  print_parameters(n.parameters)"]},{"cell_type":"code","execution_count":281,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer: 0, neuron_cnt: 3, layer: Layer(neurons = [Neuron(w = [Value(data = 0.9594606936252317), Value(data = -0.20641325804466804)], b = Value(data = -0.654341995999097)), Neuron(w = [Value(data = -0.09300721266459444), Value(data = 0.9062582394415153)], b = Value(data = -0.6312177860520167)), Neuron(w = [Value(data = 0.7659389917117154), Value(data = 0.04917237166555721)], b = Value(data = -0.8649635248197047))])\n","layer: 1, neuron_cnt: 3, layer: Layer(neurons = [Neuron(w = [Value(data = 0.6248857609168794), Value(data = 0.2611649262148903), Value(data = 0.10736389190511098)], b = Value(data = 0.7851150969174898)), Neuron(w = [Value(data = -0.2392080069525504), Value(data = 0.5818996603367728), Value(data = -0.7804072987649004)], b = Value(data = 0.8967501641990212)), Neuron(w = [Value(data = 0.9172012096912538), Value(data = -0.8143631212810594), Value(data = 0.3158594044258638)], b = Value(data = -0.5573518384397362))])\n","layer: 2, neuron_cnt: 1, layer: Layer(neurons = [Neuron(w = [Value(data = 0.6954353875576735), Value(data = -0.531558734816248), Value(data = -0.5156266281355579)], b = Value(data = -0.6228903440704217))])\n"]}],"source":["# if True:\n","if verbose:\n","\t# print weights and bias of each layer\n","\tfor i, layer in enumerate(n.layers):\n","\t\tneuron_cnt = len(layer.neurons)  # numbers of neurons in the layer \n","\t\tprint(f'layer: {i}, neuron_cnt: {neuron_cnt}, layer: {layer}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":282,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 2)\n","xs_mats:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n","\n","xs_mats_T[0].shape: (2, 2)\n","xs_mats_T:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### ---- Start: Manual Calculation of Neural Network Output and Prediction Error ----<br>* calculate neural network output, (a.k.a) Forward Pass<br>* calculate prediction error, (a.k.a) Loss"]},{"cell_type":"code","execution_count":283,"metadata":{},"outputs":[],"source":["def forward_pass(layers, verbose=verbose):\n","  # Get Neural Network's Weights and Biases Matrices\n","  # w_mats, b_mats = get_wt_n_b_mats(n.layers, verbose=verbose)\n","  w_mats, b_mats = get_wt_n_b_mats(layers, verbose=verbose)\n","\n","  # Calculate Neural Network Output and Loss with Matrix Multiplication\n","  for layer in range(len(layers)):\n","    if layer == 0:  # first layer, use given inputs xs as inputs\n","      input = xs_mats_T[layer]\n","    else:  # after first layer, use outputs from preceding layers as inputs\n","      input = output\n","\n","    weights = w_mats[layer]\n","    bias = np.transpose(b_mats[layer])\n","\n","    weights_x_input = np.matmul(weights, input)\n","    weights_x_input_plus_bias = weights_x_input + bias\n","\n","    # output = np.tanh(np.matmul(weights, input) + bias)\n","    output = np.tanh(weights_x_input_plus_bias)\n","\n","    print(f'{\"-\"*50}')\n","    print(f'Calculate Output of Layer: {layer}')    \n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_+_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')    \n","\n","    # print(f'output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n","    print(f'Layer {layer} Output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n","\n","  yout = output[0]\n","  err_sq = ((yout - ys)**2)\n","  loss_sum = err_sq.sum()\n","  loss_mean = err_sq.mean()\n","\n","  print(f'-- Manual calculation results of neural network output and prediction error --')\n","  print(f'yout:           {yout}')   \n","  print(f'desired output: {ys}')   \n","  print(f'err_sq:         {err_sq}')\n","  print(f'loss_mean:      {loss_mean}')\n","  print(f'loss_sum:       {loss_sum}')\n","\n","  return yout, err_sq, loss_sum, loss_mean, w_mats, b_mats\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Manual calculation results of neural network output and prediction error "]},{"cell_type":"code","execution_count":284,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 3\n","----\n","layer: 0, neuron 0\n","w0:  0.9594607,   w0.grad:  0.0000000\n","w1: -0.2064133,   w1.grad:  0.0000000\n","b:  -0.6543420\n","\n","b:  -0.6543420\n","b_mat:  [-0.654341995999097]\n","\n","layer: 0, neuron 1\n","w0: -0.0930072,   w0.grad:  0.0000000\n","w1:  0.9062582,   w1.grad:  0.0000000\n","b:  -0.6312178\n","\n","b:  -0.6312178\n","b_mat:  [-0.654341995999097, -0.6312177860520167]\n","\n","layer: 0, neuron 2\n","w0:  0.7659390,   w0.grad:  0.0000000\n","w1:  0.0491724,   w1.grad:  0.0000000\n","b:  -0.8649635\n","\n","b:  -0.8649635\n","b_mat:  [-0.654341995999097, -0.6312177860520167, -0.8649635248197047]\n","\n","------\n","layer: 1, neuron_cnt: 3\n","----\n","layer: 1, neuron 0\n","w0:  0.6248858,   w0.grad:  0.0000000\n","w1:  0.2611649,   w1.grad:  0.0000000\n","w2:  0.1073639,   w2.grad:  0.0000000\n","b:   0.7851151\n","\n","b:   0.7851151\n","b_mat:  [0.7851150969174898]\n","\n","layer: 1, neuron 1\n","w0: -0.2392080,   w0.grad:  0.0000000\n","w1:  0.5818997,   w1.grad:  0.0000000\n","w2: -0.7804073,   w2.grad:  0.0000000\n","b:   0.8967502\n","\n","b:   0.8967502\n","b_mat:  [0.7851150969174898, 0.8967501641990212]\n","\n","layer: 1, neuron 2\n","w0:  0.9172012,   w0.grad:  0.0000000\n","w1: -0.8143631,   w1.grad:  0.0000000\n","w2:  0.3158594,   w2.grad:  0.0000000\n","b:  -0.5573518\n","\n","b:  -0.5573518\n","b_mat:  [0.7851150969174898, 0.8967501641990212, -0.5573518384397362]\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0:  0.6954354,   w0.grad:  0.0000000\n","w1: -0.5315587,   w1.grad:  0.0000000\n","w2: -0.5156266,   w2.grad:  0.0000000\n","b:  -0.6228903\n","\n","b:  -0.6228903\n","b_mat:  [-0.6228903440704217]\n","\n","------\n","layer: 0\n","w_mat(3, 2):\n","[[ 0.95946069 -0.20641326]\n"," [-0.09300721  0.90625824]\n"," [ 0.76593899  0.04917237]]\n","b_mat(1, 3):\n","[[-0.654342   -0.63121779 -0.86496352]]\n","\n","layer: 1\n","w_mat(3, 3):\n","[[ 0.62488576  0.26116493  0.10736389]\n"," [-0.23920801  0.58189966 -0.7804073 ]\n"," [ 0.91720121 -0.81436312  0.3158594 ]]\n","b_mat(1, 3):\n","[[ 0.7851151   0.89675016 -0.55735184]]\n","\n","layer: 2\n","w_mat(1, 3):\n","[[ 0.69543539 -0.53155873 -0.51562663]]\n","b_mat(1, 1):\n","[[-0.62289034]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 0\n","weights (3, 2):\n","[[ 0.95946069 -0.20641326]\n"," [-0.09300721  0.90625824]\n"," [ 0.76593899  0.04917237]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[ 1.29968161  3.08479534]\n"," [ 2.53276029 -1.18527988]\n"," [ 1.6793951   2.2486446 ]]\n","\n","bias (3, 1):\n","[[-0.654342  ]\n"," [-0.63121779]\n"," [-0.86496352]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[ 0.64533962  2.43045334]\n"," [ 1.90154251 -1.81649766]\n"," [ 0.81443157  1.38368108]]\n","\n","Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[ 0.56852427  0.98463208]\n"," [ 0.95636932 -0.94848806]\n"," [ 0.67202768  0.88177288]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 1\n","weights (3, 3):\n","[[ 0.62488576  0.26116493  0.10736389]\n"," [-0.23920801  0.58189966 -0.7804073 ]\n"," [ 0.91720121 -0.81436312  0.3158594 ]]\n","\n","input (3, 2):\n","[[ 0.56852427  0.98463208]\n"," [ 0.95636932 -0.94848806]\n"," [ 0.67202768  0.88177288]]\n","\n","weights_x_inputs (3, 2):\n","[[ 0.67718435  0.46224132]\n"," [-0.10393989 -1.47559875]\n"," [-0.04511449  1.95403569]]\n","\n","bias (3, 1):\n","[[ 0.7851151 ]\n"," [ 0.89675016]\n"," [-0.55735184]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[ 1.46229945  1.24735642]\n"," [ 0.79281028 -0.57884859]\n"," [-0.60246633  1.39668385]]\n","\n","Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[ 0.89809828  0.84754068]\n"," [ 0.65999811 -0.52182806]\n"," [-0.53880223  0.88463275]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 2\n","weights (1, 3):\n","[[ 0.69543539 -0.53155873 -0.51562663]]\n","\n","input (3, 2):\n","[[ 0.89809828  0.84754068]\n"," [ 0.65999811 -0.52182806]\n"," [-0.53880223  0.88463275]]\n","\n","weights_x_inputs (1, 2):\n","[[0.55156234 0.41065184]]\n","\n","bias (1, 1):\n","[[-0.62289034]]\n","\n","weights_x_inputs_+_bias (1, 2):\n","[[-0.071328  -0.2122385]]\n","\n","Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n","[[-0.07120729 -0.20910812]]\n","\n","-- Manual calculation results of neural network output and prediction error --\n","yout:           [-0.07120729 -0.20910812]\n","desired output: [1.0, -1.0]\n","err_sq:         [1.14748505 0.62550996]\n","loss_mean:      0.8864975059441333\n","loss_sum:       1.7729950118882667\n"]}],"source":["yout, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"]},{"cell_type":"code","execution_count":285,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- manual calculation results of neural network output and prediction error --\n","yout:           [-0.07120729 -0.20910812] <-- neural network output\n","desired output: [1.0, -1.0]\n","err_sq:         [1.14748505 0.62550996]\n","loss_mean:      0.8864975059441333\n","loss_sum:       1.7729950118882667 <-- sum(prediction_error)^2\n"]}],"source":["print(f'-- manual calculation results of neural network output and prediction error --')\n","print(f'yout:           {yout} <-- neural network output')   \n","print(f'desired output: {ys}')   \n","print(f'err_sq:         {err_sq}')\n","print(f'loss_mean:      {loss_mean}')\n","print(f'loss_sum:       {loss_sum} <-- sum(prediction_error)^2')\n"]},{"cell_type":"markdown","metadata":{},"source":["##### ---- End: Manual Calculation of Neural Network Output and Prediction Error ----"]},{"cell_type":"markdown","metadata":{},"source":["##### How Neural Network Learns<br>* calculate changes in prediction errors (loss) w.r.t. changes in each parameter (a.k.a. gradients)<br>* adjust parameters in direction of less loss with gradients<br>* repeat the process "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate gradient for parameter W0<br>* increase W0 by small amount, e.g. 0.00001<br>* recalculate output and loss<br>* calculate changes in loss w.r.t. changes in W0"]},{"cell_type":"code","execution_count":286,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss_sum_before:   1.7729950 <-- loss_sum before increase W1\n","W0_before:         0.9594607\n","W0_after:          0.9594707\n","W0_dif:            0.0000100 <-- increased W1 by a small amount\n"]}],"source":["# Increase W1 by h\n","h = .00001\n","loss_sum_before = loss_sum\n","print(f'loss_sum_before:  {loss_sum_before:10.7f} <-- loss_sum before increase W1')\n","W0_before = n.parameters()[0].data  # W1\n","print(f'W0_before:        {W0_before:10.7f}')\n","n.parameters()[0].data += h\n","W0_after = n.parameters()[0].data\n","print(f'W0_after:         {W0_after:10.7f}') \n","W0_dif = W0_after - W0_before\n","print(f'W0_dif:           {W0_dif:10.7f} <-- increased W1 by a small amount') "]},{"cell_type":"markdown","metadata":{},"source":["##### Recalculate output and Loss with small changes in W0"]},{"cell_type":"code","execution_count":287,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 3\n","----\n","layer: 0, neuron 0\n","w0:  0.9594707,   w0.grad:  0.0000000\n","w1: -0.2064133,   w1.grad:  0.0000000\n","b:  -0.6543420\n","\n","b:  -0.6543420\n","b_mat:  [-0.654341995999097]\n","\n","layer: 0, neuron 1\n","w0: -0.0930072,   w0.grad:  0.0000000\n","w1:  0.9062582,   w1.grad:  0.0000000\n","b:  -0.6312178\n","\n","b:  -0.6312178\n","b_mat:  [-0.654341995999097, -0.6312177860520167]\n","\n","layer: 0, neuron 2\n","w0:  0.7659390,   w0.grad:  0.0000000\n","w1:  0.0491724,   w1.grad:  0.0000000\n","b:  -0.8649635\n","\n","b:  -0.8649635\n","b_mat:  [-0.654341995999097, -0.6312177860520167, -0.8649635248197047]\n","\n","------\n","layer: 1, neuron_cnt: 3\n","----\n","layer: 1, neuron 0\n","w0:  0.6248858,   w0.grad:  0.0000000\n","w1:  0.2611649,   w1.grad:  0.0000000\n","w2:  0.1073639,   w2.grad:  0.0000000\n","b:   0.7851151\n","\n","b:   0.7851151\n","b_mat:  [0.7851150969174898]\n","\n","layer: 1, neuron 1\n","w0: -0.2392080,   w0.grad:  0.0000000\n","w1:  0.5818997,   w1.grad:  0.0000000\n","w2: -0.7804073,   w2.grad:  0.0000000\n","b:   0.8967502\n","\n","b:   0.8967502\n","b_mat:  [0.7851150969174898, 0.8967501641990212]\n","\n","layer: 1, neuron 2\n","w0:  0.9172012,   w0.grad:  0.0000000\n","w1: -0.8143631,   w1.grad:  0.0000000\n","w2:  0.3158594,   w2.grad:  0.0000000\n","b:  -0.5573518\n","\n","b:  -0.5573518\n","b_mat:  [0.7851150969174898, 0.8967501641990212, -0.5573518384397362]\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0:  0.6954354,   w0.grad:  0.0000000\n","w1: -0.5315587,   w1.grad:  0.0000000\n","w2: -0.5156266,   w2.grad:  0.0000000\n","b:  -0.6228903\n","\n","b:  -0.6228903\n","b_mat:  [-0.6228903440704217]\n","\n","------\n","layer: 0\n","w_mat(3, 2):\n","[[ 0.95947069 -0.20641326]\n"," [-0.09300721  0.90625824]\n"," [ 0.76593899  0.04917237]]\n","b_mat(1, 3):\n","[[-0.654342   -0.63121779 -0.86496352]]\n","\n","layer: 1\n","w_mat(3, 3):\n","[[ 0.62488576  0.26116493  0.10736389]\n"," [-0.23920801  0.58189966 -0.7804073 ]\n"," [ 0.91720121 -0.81436312  0.3158594 ]]\n","b_mat(1, 3):\n","[[ 0.7851151   0.89675016 -0.55735184]]\n","\n","layer: 2\n","w_mat(1, 3):\n","[[ 0.69543539 -0.53155873 -0.51562663]]\n","b_mat(1, 1):\n","[[-0.62289034]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 0\n","weights (3, 2):\n","[[ 0.95947069 -0.20641326]\n"," [-0.09300721  0.90625824]\n"," [ 0.76593899  0.04917237]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[ 1.29970161  3.08482534]\n"," [ 2.53276029 -1.18527988]\n"," [ 1.6793951   2.2486446 ]]\n","\n","bias (3, 1):\n","[[-0.654342  ]\n"," [-0.63121779]\n"," [-0.86496352]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[ 0.64535962  2.43048334]\n"," [ 1.90154251 -1.81649766]\n"," [ 0.81443157  1.38368108]]\n","\n","Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[ 0.5685378   0.984633  ]\n"," [ 0.95636932 -0.94848806]\n"," [ 0.67202768  0.88177288]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 1\n","weights (3, 3):\n","[[ 0.62488576  0.26116493  0.10736389]\n"," [-0.23920801  0.58189966 -0.7804073 ]\n"," [ 0.91720121 -0.81436312  0.3158594 ]]\n","\n","input (3, 2):\n","[[ 0.5685378   0.984633  ]\n"," [ 0.95636932 -0.94848806]\n"," [ 0.67202768  0.88177288]]\n","\n","weights_x_inputs (3, 2):\n","[[ 0.67719281  0.46224189]\n"," [-0.10394312 -1.47559897]\n"," [-0.04510208  1.95403653]]\n","\n","bias (3, 1):\n","[[ 0.7851151 ]\n"," [ 0.89675016]\n"," [-0.55735184]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[ 1.46230791  1.24735699]\n"," [ 0.79280704 -0.57884881]\n"," [-0.60245392  1.39668469]]\n","\n","Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[ 0.89809991  0.84754084]\n"," [ 0.65999628 -0.52182821]\n"," [-0.53879342  0.88463293]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 2\n","weights (1, 3):\n","[[ 0.69543539 -0.53155873 -0.51562663]]\n","\n","input (3, 2):\n","[[ 0.89809991  0.84754084]\n"," [ 0.65999628 -0.52182821]\n"," [-0.53879342  0.88463293]]\n","\n","weights_x_inputs (1, 2):\n","[[0.55155991 0.41065194]]\n","\n","bias (1, 1):\n","[[-0.62289034]]\n","\n","weights_x_inputs_+_bias (1, 2):\n","[[-0.07133044 -0.2122384 ]]\n","\n","Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n","[[-0.07120971 -0.20910802]]\n","\n","-- Manual calculation results of neural network output and prediction error --\n","yout:           [-0.07120971 -0.20910802]\n","desired output: [1.0, -1.0]\n","err_sq:         [1.14749024 0.62551012]\n","loss_mean:      0.886500177570239\n","loss_sum:       1.773000355140478\n"]}],"source":["yout, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"]},{"cell_type":"code","execution_count":288,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- recaluclate neural network output and loss with small change in W1 --\n","yout:             [-0.07120971 -0.20910802]\n","desired output:   [1.0, -1.0]\n","err_sq:           [1.14749024 0.62551012]\n","loss_sum_before:  1.7729950118882667\n","loss_sum_after:   1.773000355140478\n","\n","loss_sum_dif:     5.343252211353899e-06 <-- change in loss_sum\n","W0_dif:           9.99999999995449e-06 <-- change in W0\n","W0_grad:          0.5343252211378217 <-- changes in loss_sum w.r.t. changes in W0, manual calculation\n"]}],"source":["loss_sum_after = loss_sum\n","loss_sum_dif = loss_sum_after - loss_sum_before\n","W0_grad = loss_sum_dif / W0_dif\n","\n","# print(f'-- manual forward pass calculation --')\n","print(f'-- recaluclate neural network output and loss with small change in W1 --')\n","print(f'yout:             {yout}')   \n","print(f'desired output:   {ys}')   \n","print(f'err_sq:           {err_sq}')\n","print(f'loss_sum_before:  {loss_sum_before}')\n","print(f'loss_sum_after:   {loss_sum_after}\\n')\n","print(f'loss_sum_dif:     {loss_sum_dif} <-- change in loss_sum')\n","print(f'W0_dif:           {W0_dif} <-- change in W0')\n","print(f'W0_grad:          {W0_grad} <-- changes in loss_sum w.r.t. changes in W0, manual calculation')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate output and Loss with Micrograd<br>* change W1 back to initial value<br>* compare manual calculation vs Micrograd "]},{"cell_type":"code","execution_count":289,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- Calculate neural network loss and gradient using Micrograd --\n","ypred_data:         [-0.07120728576561222, -0.20910812192124742]\n","ys:                 [1.0, -1.0]\n","err_sq:             [1.1474850490773303, 0.6255099628109365]\n","loss_mean:          0.8864975059441333\n","loss_sum:           1.7729950118882667 <-- loss_sum, Micrograd calculation same as manual calc.\n","W0_before:          0.9594606936252317\n"]}],"source":["# change W1 back before Micrograd calculation\n","n.parameters()[0].data = W0_before\n","\n","ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","# loss.backward()\n","err_sq_ = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","err_sq = [l.data for l in err_sq_]\n","loss_sum = sum(err_sq)\n","loss_len = len(err_sq)\n","loss_mean = loss_sum / loss_len\n","\n","# print(f'-- Micrograd forward pass and backward pass --')\n","print(f'-- Calculate neural network loss and gradient using Micrograd --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'err_sq:             {err_sq}')\n","print(f'loss_mean:          {loss_mean}')\n","print(f'loss_sum:           {loss_sum} <-- loss_sum, Micrograd calculation same as manual calc.')\n","print(f'W0_before:          {n.parameters()[0].data}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate gradients and adjust parameters"]},{"cell_type":"code","execution_count":290,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== adjust parameters ===\n","  i  parameter before         gradient     learning rate   parameter adjusted\n","  0      0.9594606936     0.5343232027           0.05000         0.9327445335 <-- gradient same as manual calc. W0_grad  0.5343252211\n","  1     -0.2064132580     0.7730386363           0.05000        -0.2450651899\n","  2     -0.6543419960     0.2645755861           0.05000        -0.6675707753\n","  3     -0.0930072127    -0.0953319320           0.05000        -0.0882406161\n","  4      0.9062582394    -0.0740006730           0.05000         0.9099582731\n","  5     -0.6312177861    -0.0413934910           0.05000        -0.6291481115\n","  6      0.7659389917    -0.0206604730           0.05000         0.7669720154\n","  7      0.0491723717    -0.5631010676           0.05000         0.0773274250\n","  8     -0.8649635248    -0.0587039054           0.05000        -0.8620283295\n","  9      0.6248857609     0.1287428156           0.05000         0.6184486201\n"," 10      0.2611649262    -0.5552455917           0.05000         0.2889272058\n"," 11      0.1073638919     0.0685893128           0.05000         0.1039344263\n"," 12      0.7851150969     0.0095850994           0.05000         0.7846358419\n"," 13     -0.2392080070    -0.2125410246           0.05000        -0.2285809557\n"," 14      0.5818996603     1.1665524210           0.05000         0.5235720393\n"," 15     -0.7804072988    -0.0861683374           0.05000        -0.7760988819\n"," 16      0.8967501642     0.0543930532           0.05000         0.8940305115\n"," 17      0.9172012097     0.2764819799           0.05000         0.9033771107\n"," 18     -0.8143631213     0.9068237256           0.05000        -0.8597043076\n"," 19      0.3158594044     0.3746587021           0.05000         0.2971264693\n"," 20     -0.5573518384     0.6104320563           0.05000        -0.5878734413\n"," 21      0.6954353876    -0.6323371508           0.05000         0.7270522451\n"," 22     -0.5315587348    -2.1961466539           0.05000        -0.4217514021\n"," 23     -0.5156266281     2.4865964273           0.05000        -0.6399564495\n"," 24     -0.6228903441    -0.6189331538           0.05000        -0.5919436864\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","print('=== adjust parameters ===')\n","print(f'  i  parameter before         gradient     learning rate   parameter adjusted')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","\n","  if i == 0:  \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f} <-- gradient same as manual calc. W0_grad {W0_grad:13.10f}')\n","  else:\n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":291,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.21155274439245073), Value(data = -0.33589549583944733)]\n","step: 0, loss: 1.0626838673214096\n","-------\n","ypred: [Value(data = 0.3942585472399464), Value(data = -0.43450550048344044)]\n","step: 1, loss: 0.6867067365753444\n","-------\n","ypred: [Value(data = 0.4992562436769442), Value(data = -0.5111376904989755)]\n","step: 2, loss: 0.48973066714719937\n","-------\n","ypred: [Value(data = 0.5667151252790948), Value(data = -0.568107487191547)]\n","step: 3, loss: 0.3742669252819102\n","-------\n","ypred: [Value(data = 0.6142349285893889), Value(data = -0.6112061646411802)]\n","step: 4, loss: 0.299975336733455\n","-------\n","ypred: [Value(data = 0.649857986115634), Value(data = -0.644809269368122)]\n","step: 5, loss: 0.24875988501380691\n","-------\n","ypred: [Value(data = 0.6777576025715396), Value(data = -0.6717652238671461)]\n","step: 6, loss: 0.21157823096342654\n","-------\n","ypred: [Value(data = 0.7003262341184668), Value(data = -0.6939186956712589)]\n","step: 7, loss: 0.1834901308172034\n","-------\n","ypred: [Value(data = 0.7190414293282009), Value(data = -0.712495515283972)]\n","step: 8, loss: 0.16159654716576907\n","-------\n","ypred: [Value(data = 0.734869236917707), Value(data = -0.7283362358265636)]\n","step: 9, loss: 0.14409552229747946\n","-------\n","ypred: [Value(data = 0.7484703081786582), Value(data = -0.7420348664975713)]\n","step: 10, loss: 0.129813195970665\n","-------\n","ypred: [Value(data = 0.7603131525203469), Value(data = -0.7540227561157655)]\n","step: 11, loss: 0.11795458936361863\n","-------\n","ypred: [Value(data = 0.770740126262077), Value(data = -0.7646207496982375)]\n","step: 12, loss: 0.10796348117894818\n","-------\n","ypred: [Value(data = 0.7800077732055726), Value(data = -0.7740725882439475)]\n","step: 13, loss: 0.09943977523275967\n","-------\n","ypred: [Value(data = 0.7883124950026481), Value(data = -0.7825669076221997)]\n","step: 14, loss: 0.09208874943297692\n","-------\n","ypred: [Value(data = 0.7958074512412376), Value(data = -0.790252108644085)]\n","step: 15, loss: 0.08568877489685225\n","-------\n","ypred: [Value(data = 0.8026140153165523), Value(data = -0.7972466477287439)]\n","step: 16, loss: 0.08007014880668634\n","-------\n","ypred: [Value(data = 0.8088297393429587), Value(data = -0.8036463114199476)]\n","step: 17, loss: 0.07510083957867333\n","-------\n","ypred: [Value(data = 0.8145340164200446), Value(data = -0.809529458382397)]\n","step: 18, loss: 0.07067665828938331\n","-------\n","ypred: [Value(data = 0.8197921858568915), Value(data = -0.8149608628495864)]\n","step: 19, loss: 0.06671433855560673\n","-------\n","ypred: [Value(data = 0.8246585620853296), Value(data = -0.8199945772882056)]\n","step: 20, loss: 0.06314657205563601\n","-------\n","ypred: [Value(data = 0.8291787047983017), Value(data = -0.8246760952318162)]\n","step: 21, loss: 0.05991838647754895\n","-------\n","ypred: [Value(data = 0.8333911446391599), Value(data = -0.8290440067902842)]\n","step: 22, loss: 0.05698446229896974\n","-------\n","ypred: [Value(data = 0.8373287119469246), Value(data = -0.8331312810201342)]\n","step: 23, loss: 0.05430711733082805\n","-------\n","ypred: [Value(data = 0.8410195718780202), Value(data = -0.8369662701749891)]\n","step: 24, loss: 0.05185477358650261\n","-------\n","ypred: [Value(data = 0.8444880394439301), Value(data = -0.8405735041055504)]\n","step: 25, loss: 0.04960077746917557\n","-------\n","ypred: [Value(data = 0.8477552275784083), Value(data = -0.8439743245216736)]\n","step: 26, loss: 0.04752248213817027\n","-------\n","ypred: [Value(data = 0.8508395671134007), Value(data = -0.8471873957683611)]\n","step: 27, loss: 0.0456005267509732\n","-------\n","ypred: [Value(data = 0.8537572274767278), Value(data = -0.8502291194509628)]\n","step: 28, loss: 0.04381826517572752\n","-------\n","ypred: [Value(data = 0.8565224597095313), Value(data = -0.853113973515827)]\n","step: 29, loss: 0.04216130934411224\n","-------\n","ypred: [Value(data = 0.8591478781650763), Value(data = -0.8558547914756773)]\n","step: 30, loss: 0.040617161365920654\n","-------\n","ypred: [Value(data = 0.8616446934058779), Value(data = -0.858462993836954)]\n","step: 31, loss: 0.039174914976351644\n","-------\n","ypred: [Value(data = 0.864022905962865), Value(data = -0.8609487810761703)]\n","step: 32, loss: 0.03782501158698665\n","-------\n","ypred: [Value(data = 0.866291468481392), Value(data = -0.8633212954706807)]\n","step: 33, loss: 0.03655903967267554\n","-------\n","ypred: [Value(data = 0.8684584221604321), Value(data = -0.8655887575374811)]\n","step: 34, loss: 0.03536956880084114\n","-------\n","ypred: [Value(data = 0.8705310121558028), Value(data = -0.8677585816442959)]\n","step: 35, loss: 0.034250011542129255\n","-------\n","ypred: [Value(data = 0.8725157856673315), Value(data = -0.8698374744383609)]\n","step: 36, loss: 0.033194507964602125\n","-------\n","ypred: [Value(data = 0.8744186756922568), Value(data = -0.8718315190231857)]\n","step: 37, loss: 0.032197828530790576\n","-------\n","ypred: [Value(data = 0.8762450728508131), Value(data = -0.8737462472530398)]\n","step: 38, loss: 0.031255292076391106\n","-------\n","ypred: [Value(data = 0.877999887236486), Value(data = -0.8755867020726374)]\n","step: 39, loss: 0.0303626962154728\n","-------\n","ypred: [Value(data = 0.8796876018843401), Value(data = -0.8773574914783039)]\n","step: 40, loss: 0.029516258036835337\n","-------\n","ypred: [Value(data = 0.8813123191646633), Value(data = -0.8790628353963801)]\n","step: 41, loss: 0.0287125633644338\n","-------\n","ypred: [Value(data = 0.8828778011798467), Value(data = -0.8807066065492651)]\n","step: 42, loss: 0.02794852317745935\n","-------\n","ypred: [Value(data = 0.8843875050575599), Value(data = -0.8822923661974547)]\n","step: 43, loss: 0.02722133604220985\n","-------\n","ypred: [Value(data = 0.8858446138835204), Value(data = -0.8838233954981611)]\n","step: 44, loss: 0.026528455612979222\n","-------\n","ypred: [Value(data = 0.8872520638951867), Value(data = -0.8853027231005204)]\n","step: 45, loss: 0.025867562424050954\n","-------\n","ypred: [Value(data = 0.8886125684579067), Value(data = -0.8867331494985454)]\n","step: 46, loss: 0.02523653932806338\n","-------\n","ypred: [Value(data = 0.8899286392630195), Value(data = -0.8881172685816198)]\n","step: 47, loss: 0.024633450044127907\n","-------\n","ypred: [Value(data = 0.8912026051196732), Value(data = -0.8894574867550137)]\n","step: 48, loss: 0.024056520367263738\n","-------\n","ypred: [Value(data = 0.8924366286559743), Value(data = -0.8907560399470374)]\n","step: 49, loss: 0.023504121662946048\n","-------\n","ypred: [Value(data = 0.8936327211983314), Value(data = -0.892015008772864)]\n","step: 50, loss: 0.022974756329996546\n","-------\n","ypred: [Value(data = 0.8947927560588057), Value(data = -0.8932363320860774)]\n","step: 51, loss: 0.0224670449641363\n","-------\n","ypred: [Value(data = 0.8959184804275161), Value(data = -0.8944218191162809)]\n","step: 52, loss: 0.02197971499523265\n","-------\n","ypred: [Value(data = 0.8970115260395756), Value(data = -0.8955731603635111)]\n","step: 53, loss: 0.021511590605161994\n","-------\n","ypred: [Value(data = 0.8980734187627541), Value(data = -0.8966919373968937)]\n","step: 54, loss: 0.021061583761520213\n","-------\n","ypred: [Value(data = 0.8991055872323279), Value(data = -0.8977796316851887)]\n","step: 55, loss: 0.020628686226149065\n","-------\n","ypred: [Value(data = 0.9001093706428133), Value(data = -0.8988376325700548)]\n","step: 56, loss: 0.0202119624174061\n","-------\n","ypred: [Value(data = 0.9010860257919815), Value(data = -0.899867244478489)]\n","step: 57, loss: 0.019810543021955247\n","-------\n","ypred: [Value(data = 0.9020367334603322), Value(data = -0.9008696934586174)]\n","step: 58, loss: 0.01942361926611047\n","-------\n","ypred: [Value(data = 0.9029626041987221), Value(data = -0.9018461331124588)]\n","step: 59, loss: 0.019050437768871033\n","-------\n","ypred: [Value(data = 0.9038646835878329), Value(data = -0.9027976499902067)]\n","step: 60, loss: 0.018690295909093844\n","-------\n","ypred: [Value(data = 0.9047439570254016), Value(data = -0.9037252685027491)]\n","step: 61, loss: 0.01834253764804629\n","-------\n","ypred: [Value(data = 0.9056013540904171), Value(data = -0.9046299554023668)]\n","step: 62, loss: 0.01800654975611736\n","-------\n","ypred: [Value(data = 0.9064377525276816), Value(data = -0.905512623875683)]\n","step: 63, loss: 0.017681758398929513\n","-------\n","ypred: [Value(data = 0.9072539818910771), Value(data = -0.9063741372878377)]\n","step: 64, loss: 0.017367626043657325\n","-------\n","ypred: [Value(data = 0.9080508268794933), Value(data = -0.9072153126124076)]\n","step: 65, loss: 0.01706364865115815\n","-------\n","ypred: [Value(data = 0.9088290303955319), Value(data = -0.9080369235777248)]\n","step: 66, loss: 0.01676935312366808\n","-------\n","ypred: [Value(data = 0.9095892963537584), Value(data = -0.9088397035568444)]\n","step: 67, loss: 0.01648429498141253\n","-------\n","ypred: [Value(data = 0.9103322922623358), Value(data = -0.9096243482254437)]\n","step: 68, loss: 0.016208056244603024\n","-------\n","ypred: [Value(data = 0.9110586515992973), Value(data = -0.9103915180093236)]\n","step: 69, loss: 0.015940243500008548\n","-------\n","ypred: [Value(data = 0.9117689760024542), Value(data = -0.91114184034088)]\n","step: 70, loss: 0.015680486133661166\n","-------\n","ypred: [Value(data = 0.9124638372899349), Value(data = -0.9118759117418921)]\n","step: 71, loss: 0.015428434713325792\n","-------\n","ypred: [Value(data = 0.9131437793265943), Value(data = -0.9125942997481801)]\n","step: 72, loss: 0.015183759506178344\n","-------\n","ypred: [Value(data = 0.9138093197499705), Value(data = -0.9132975446901068)]\n","step: 73, loss: 0.014946149118726852\n","-------\n","ypred: [Value(data = 0.9144609515680879), Value(data = -0.9139861613414898)]\n","step: 74, loss: 0.014715309247409223\n","-------\n","ypred: [Value(data = 0.9150991446401837), Value(data = -0.9146606404482459)]\n","step: 75, loss: 0.014490961529532009\n","-------\n","ypred: [Value(data = 0.9157243470503411), Value(data = -0.915321450146983)]\n","step: 76, loss: 0.014272842485301241\n","-------\n","ypred: [Value(data = 0.9163369863830497), Value(data = -0.915969037282768)]\n","step: 77, loss: 0.014060702542654839\n","-------\n","ypred: [Value(data = 0.9169374709088449), Value(data = -0.9166038286344157)]\n","step: 78, loss: 0.013854305137456887\n","-------\n","ypred: [Value(data = 0.9175261906874113), Value(data = -0.9172262320548622)]\n","step: 79, loss: 0.013653425882364772\n","-------\n","ypred: [Value(data = 0.9181035185948409), Value(data = -0.9178366375334788)]\n","step: 80, loss: 0.013457851798350528\n","-------\n","ypred: [Value(data = 0.9186698112811231), Value(data = -0.9184354181865537)]\n","step: 81, loss: 0.013267380603450498\n","-------\n","ypred: [Value(data = 0.9192254100633883), Value(data = -0.9190229311816045)]\n","step: 82, loss: 0.013081820053846931\n","-------\n","ypred: [Value(data = 0.9197706417599264), Value(data = -0.919599518600673)]\n","step: 83, loss: 0.012900987332857591\n","-------\n","ypred: [Value(data = 0.9203058194695617), Value(data = -0.9201655082473003)]\n","step: 84, loss: 0.012724708483829976\n","-------\n","ypred: [Value(data = 0.920831243300553), Value(data = -0.9207212144014667)]\n","step: 85, loss: 0.012552817883314447\n","-------\n","ypred: [Value(data = 0.9213472010528344), Value(data = -0.9212669385264093)]\n","step: 86, loss: 0.012385157751227466\n","-------\n","ypred: [Value(data = 0.9218539688570768), Value(data = -0.9218029699308958)]\n","step: 87, loss: 0.0122215776950191\n","-------\n","ypred: [Value(data = 0.9223518117737587), Value(data = -0.9223295863902307)]\n","step: 88, loss: 0.012061934285130442\n","-------\n","ypred: [Value(data = 0.9228409843551646), Value(data = -0.9228470547289913)]\n","step: 89, loss: 0.011906090659271217\n","-------\n","ypred: [Value(data = 0.9233217311729881), Value(data = -0.9233556313682495)]\n","step: 90, loss: 0.011753916153267164\n","-------\n","ypred: [Value(data = 0.9237942873139947), Value(data = -0.9238555628398024)]\n","step: 91, loss: 0.011605285956425265\n","-------\n","ypred: [Value(data = 0.924258878846001), Value(data = -0.9243470862697357)]\n","step: 92, loss: 0.011460080789543563\n","-------\n","ypred: [Value(data = 0.9247157232562448), Value(data = -0.9248304298334531)]\n","step: 93, loss: 0.011318186603853732\n","-------\n","ypred: [Value(data = 0.9251650298640556), Value(data = -0.9253058131841393)]\n","step: 94, loss: 0.011179494299330392\n","-------\n","ypred: [Value(data = 0.925607000209583), Value(data = -0.9257734478564659)]\n","step: 95, loss: 0.011043899460933778\n","-------\n","ypred: [Value(data = 0.9260418284202067), Value(data = -0.9262335376472132)]\n","step: 96, loss: 0.01091130211147127\n","-------\n","ypred: [Value(data = 0.9264697015561217), Value(data = -0.9266862789743467)]\n","step: 97, loss: 0.010781606479873118\n","-------\n","ypred: [Value(data = 0.9268907999364829), Value(data = -0.9271318612159763)]\n","step: 98, loss: 0.010654720783775095\n","-------\n","ypred: [Value(data = 0.9273052974473847), Value(data = -0.9275704670305105)]\n","step: 99, loss: 0.010530557025391588\n","-------\n","ypred: [Value(data = 0.9277133618328569), Value(data = -0.9280022726592287)]\n","step: 100, loss: 0.01040903079974351\n","-------\n","ypred: [Value(data = 0.9281151549699737), Value(data = -0.928427448212399)]\n","step: 101, loss: 0.010290061114379735\n","-------\n","ypred: [Value(data = 0.9285108331290851), Value(data = -0.9288461579399834)]\n","step: 102, loss: 0.010173570219799307\n","-------\n","ypred: [Value(data = 0.9289005472201132), Value(data = -0.9292585604879064)]\n","step: 103, loss: 0.010059483449842546\n","-------\n","ypred: [Value(data = 0.9292844430257843), Value(data = -0.9296648091407831)]\n","step: 104, loss: 0.00994772907137602\n","-------\n","ypred: [Value(data = 0.9296626614226071), Value(data = -0.930065052051944)]\n","step: 105, loss: 0.009838238142648103\n","-------\n","ypred: [Value(data = 0.9300353385903489), Value(data = -0.9304594324615346)]\n","step: 106, loss: 0.009730944379738983\n","-------\n","ypred: [Value(data = 0.9304026062107089), Value(data = -0.9308480889034108)]\n","step: 107, loss: 0.009625784030572236\n","-------\n","ypred: [Value(data = 0.9307645916558415), Value(data = -0.931231155401505)]\n","step: 108, loss: 0.009522695755994327\n","-------\n","ypred: [Value(data = 0.9311214181673335), Value(data = -0.9316087616562897)]\n","step: 109, loss: 0.009421620517465524\n","-------\n","ypred: [Value(data = 0.931473205026203), Value(data = -0.9319810332219194)]\n","step: 110, loss: 0.009322501470938441\n","-------\n","ypred: [Value(data = 0.9318200677144435), Value(data = -0.9323480916745995)]\n","step: 111, loss: 0.009225283866531462\n","-------\n","ypred: [Value(data = 0.9321621180686069), Value(data = -0.9327100547726902)]\n","step: 112, loss: 0.009129914953631978\n","-------\n","ypred: [Value(data = 0.9324994644258842), Value(data = -0.9330670366090162)]\n","step: 113, loss: 0.009036343891091248\n","-------\n","ypred: [Value(data = 0.9328322117631104), Value(data = -0.9334191477558306)]\n","step: 114, loss: 0.00894452166219557\n","-------\n","ypred: [Value(data = 0.9331604618290981), Value(data = -0.933766495402845)]\n","step: 115, loss: 0.008854400994120815\n","-------\n","ypred: [Value(data = 0.9334843132706696), Value(data = -0.9341091834887152)]\n","step: 116, loss: 0.008765936281598222\n","-------\n","ypred: [Value(data = 0.9338038617527431), Value(data = -0.9344473128263441)]\n","step: 117, loss: 0.008679083514537141\n","-------\n","ypred: [Value(data = 0.9341192000727971), Value(data = -0.9347809812223425)]\n","step: 118, loss: 0.008593800209368568\n","-------\n","ypred: [Value(data = 0.9344304182700248), Value(data = -0.9351102835909679)]\n","step: 119, loss: 0.008510045343888502\n","-------\n","ypred: [Value(data = 0.9347376037294624), Value(data = -0.9354353120628389)]\n","step: 120, loss: 0.00842777929539568\n","-------\n","ypred: [Value(data = 0.9350408412813662), Value(data = -0.9357561560887053)]\n","step: 121, loss: 0.008346963781931449\n","-------\n","ypred: [Value(data = 0.9353402132960895), Value(data = -0.9360729025385377)]\n","step: 122, loss: 0.0082675618064425\n","-------\n","ypred: [Value(data = 0.9356357997746986), Value(data = -0.9363856357961835)]\n","step: 123, loss: 0.008189537603698501\n","-------\n","ypred: [Value(data = 0.9359276784355514), Value(data = -0.936694437849822)]\n","step: 124, loss: 0.008112856589808162\n","-------\n","ypred: [Value(data = 0.9362159247970506), Value(data = -0.9369993883784368)]\n","step: 125, loss: 0.008037485314186549\n","-------\n","ypred: [Value(data = 0.9365006122567675), Value(data = -0.9373005648345097)]\n","step: 126, loss: 0.007963391413836902\n","-------\n","ypred: [Value(data = 0.9367818121671231), Value(data = -0.9375980425231315)]\n","step: 127, loss: 0.00789054356981781\n","-------\n","ypred: [Value(data = 0.9370595939078034), Value(data = -0.9378918946777087)]\n","step: 128, loss: 0.007818911465775449\n","-------\n","ypred: [Value(data = 0.9373340249550716), Value(data = -0.9381821925324412)]\n","step: 129, loss: 0.0077484657484277655\n","-------\n","ypred: [Value(data = 0.9376051709481341), Value(data = -0.9384690053917293)]\n","step: 130, loss: 0.007679177989894607\n","-------\n","ypred: [Value(data = 0.937873095752708), Value(data = -0.9387524006966638)]\n","step: 131, loss: 0.007611020651774215\n","-------\n","ypred: [Value(data = 0.9381378615219248), Value(data = -0.9390324440887414)]\n","step: 132, loss: 0.007543967050872993\n","-------\n","ypred: [Value(data = 0.9383995287547054), Value(data = -0.9393091994709446)]\n","step: 133, loss: 0.007477991326499952\n","-------\n","ypred: [Value(data = 0.9386581563517266), Value(data = -0.9395827290663087)]\n","step: 134, loss: 0.007413068409244286\n","-------\n","ypred: [Value(data = 0.9389138016690939), Value(data = -0.9398530934741005)]\n","step: 135, loss: 0.007349173991158088\n","-------\n","ypred: [Value(data = 0.9391665205698378), Value(data = -0.9401203517237239)]\n","step: 136, loss: 0.0072862844972705\n","-------\n","ypred: [Value(data = 0.9394163674733262), Value(data = -0.9403845613264542)]\n","step: 137, loss: 0.007224377058366342\n","-------\n","ypred: [Value(data = 0.9396633954027018), Value(data = -0.9406457783251116)]\n","step: 138, loss: 0.007163429484962489\n","-------\n","ypred: [Value(data = 0.9399076560304307), Value(data = -0.9409040573417639)]\n","step: 139, loss: 0.007103420242422567\n","-------\n","ypred: [Value(data = 0.9401491997220525), Value(data = -0.941159451623552)]\n","step: 140, loss: 0.007044328427151886\n","-------\n","ypred: [Value(data = 0.9403880755782152), Value(data = -0.9414120130867273)]\n","step: 141, loss: 0.006986133743818393\n","-------\n","ypred: [Value(data = 0.9406243314750733), Value(data = -0.9416617923589775)]\n","step: 142, loss: 0.006928816483549026\n","-------\n","ypred: [Value(data = 0.9408580141031232), Value(data = -0.9419088388201239)]\n","step: 143, loss: 0.00687235750305272\n","-------\n","ypred: [Value(data = 0.9410891690045482), Value(data = -0.9421532006412604)]\n","step: 144, loss: 0.00681673820462496\n","-------\n","ypred: [Value(data = 0.9413178406091383), Value(data = -0.9423949248224062)]\n","step: 145, loss: 0.006761940516990741\n","-------\n","ypred: [Value(data = 0.9415440722688501), Value(data = -0.9426340572287355)]\n","step: 146, loss: 0.0067079468769454175\n","-------\n","ypred: [Value(data = 0.9417679062910665), Value(data = -0.9428706426254525)]\n","step: 147, loss: 0.006654740211754771\n","-------\n","ypred: [Value(data = 0.941989383970613), Value(data = -0.9431047247113633)]\n","step: 148, loss: 0.006602303922278719\n","-------\n","ypred: [Value(data = 0.942208545620586), Value(data = -0.9433363461512105)]\n","step: 149, loss: 0.006550621866783325\n","-------\n","ypred: [Value(data = 0.942425430602044), Value(data = -0.9435655486068166)]\n","step: 150, loss: 0.006499678345409631\n","-------\n","ypred: [Value(data = 0.9426400773526119), Value(data = -0.9437923727670929)]\n","step: 151, loss: 0.0064494580852677815\n","-------\n","ypred: [Value(data = 0.9428525234140436), Value(data = -0.944016858376958)]\n","step: 152, loss: 0.006399946226128012\n","-------\n","ypred: [Value(data = 0.9430628054587882), Value(data = -0.9442390442652168)]\n","step: 153, loss: 0.006351128306680257\n","-------\n","ypred: [Value(data = 0.9432709593156023), Value(data = -0.9444589683714396)]\n","step: 154, loss: 0.0063029902513367896\n","-------\n","ypred: [Value(data = 0.9434770199942478), Value(data = -0.9446766677718875)]\n","step: 155, loss: 0.0062555183575527685\n","-------\n","ypred: [Value(data = 0.9436810217093141), Value(data = -0.9448921787045191)]\n","step: 156, loss: 0.006208699283641402\n","-------\n","ypred: [Value(data = 0.943882997903201), Value(data = -0.9451055365931215)]\n","step: 157, loss: 0.0061625200370612745\n","-------\n","ypred: [Value(data = 0.9440829812682956), Value(data = -0.9453167760705966)]\n","step: 158, loss: 0.006116967963155057\n","-------\n","ypred: [Value(data = 0.9442810037683798), Value(data = -0.9455259310014443)]\n","step: 159, loss: 0.006072030734318714\n","-------\n","ypred: [Value(data = 0.9444770966592949), Value(data = -0.945733034503466)]\n","step: 160, loss: 0.006027696339583289\n","-------\n","ypred: [Value(data = 0.9446712905088983), Value(data = -0.9459381189687276)]\n","step: 161, loss: 0.005983953074590178\n","-------\n","ypred: [Value(data = 0.9448636152163369), Value(data = -0.9461412160838094)]\n","step: 162, loss: 0.005940789531943068\n","-------\n","ypred: [Value(data = 0.9450541000306667), Value(data = -0.9463423568493692)]\n","step: 163, loss: 0.00589819459192042\n","-------\n","ypred: [Value(data = 0.9452427735688451), Value(data = -0.946541571599049)]\n","step: 164, loss: 0.0058561574135323775\n","-------\n","ypred: [Value(data = 0.9454296638331179), Value(data = -0.9467388900177491)]\n","step: 165, loss: 0.005814667425907945\n","-------\n","ypred: [Value(data = 0.9456147982278283), Value(data = -0.9469343411592961)]\n","step: 166, loss: 0.0057737143199978\n","-------\n","ypred: [Value(data = 0.9457982035756684), Value(data = -0.9471279534635255)]\n","step: 167, loss: 0.005733288040579814\n","-------\n","ypred: [Value(data = 0.9459799061333943), Value(data = -0.9473197547728058)]\n","step: 168, loss: 0.005693378778554201\n","-------\n","ypred: [Value(data = 0.946159931607029), Value(data = -0.9475097723480226)]\n","step: 169, loss: 0.005653976963516204\n","-------\n","ypred: [Value(data = 0.946338305166567), Value(data = -0.9476980328840425)]\n","step: 170, loss: 0.005615073256595183\n","-------\n","ypred: [Value(data = 0.9465150514602061), Value(data = -0.9478845625246808)]\n","step: 171, loss: 0.005576658543548309\n","-------\n","ypred: [Value(data = 0.9466901946281191), Value(data = -0.9480693868771882)]\n","step: 172, loss: 0.005538723928098972\n","-------\n","ypred: [Value(data = 0.9468637583157861), Value(data = -0.9482525310262744)]\n","step: 173, loss: 0.00550126072550989\n","-------\n","ypred: [Value(data = 0.9470357656869016), Value(data = -0.9484340195476885)]\n","step: 174, loss: 0.005464260456380963\n","-------\n","ypred: [Value(data = 0.947206239435875), Value(data = -0.948613876521372)]\n","step: 175, loss: 0.0054277148406629665\n","-------\n","ypred: [Value(data = 0.9473752017999356), Value(data = -0.9487921255441971)]\n","step: 176, loss: 0.005391615791878769\n","-------\n","ypred: [Value(data = 0.9475426745708612), Value(data = -0.9489687897423107)]\n","step: 177, loss: 0.00535595541154307\n","-------\n","ypred: [Value(data = 0.9477086791063405), Value(data = -0.9491438917830936)]\n","step: 178, loss: 0.005320725983773372\n","-------\n","ypred: [Value(data = 0.9478732363409849), Value(data = -0.9493174538867528)]\n","step: 179, loss: 0.005285919970084249\n","-------\n","ypred: [Value(data = 0.9480363667970012), Value(data = -0.9494894978375572)]\n","step: 180, loss: 0.005251530004357937\n","-------\n","ypred: [Value(data = 0.9481980905945382), Value(data = -0.9496600449947323)]\n","step: 181, loss: 0.005217548887984046\n","-------\n","ypred: [Value(data = 0.9483584274617178), Value(data = -0.9498291163030228)]\n","step: 182, loss: 0.005183969585162271\n","-------\n","ypred: [Value(data = 0.9485173967443639), Value(data = -0.9499967323029379)]\n","step: 183, loss: 0.00515078521836129\n","-------\n","ypred: [Value(data = 0.9486750174154368), Value(data = -0.9501629131406901)]\n","step: 184, loss: 0.005117989063928113\n","-------\n","ypred: [Value(data = 0.9488313080841876), Value(data = -0.9503276785778353)]\n","step: 185, loss: 0.00508557454784217\n","-------\n","ypred: [Value(data = 0.9489862870050372), Value(data = -0.9504910480006286)]\n","step: 186, loss: 0.005053535241608504\n","-------\n","ypred: [Value(data = 0.9491399720861949), Value(data = -0.9506530404291045)]\n","step: 187, loss: 0.005021864858284627\n","-------\n","ypred: [Value(data = 0.9492923808980223), Value(data = -0.9508136745258888)]\n","step: 188, loss: 0.004990557248636455\n","-------\n","ypred: [Value(data = 0.949443530681151), Value(data = -0.9509729686047544)]\n","step: 189, loss: 0.004959606397418119\n","-------\n","ypred: [Value(data = 0.9495934383543658), Value(data = -0.9511309406389289)]\n","step: 190, loss: 0.00492900641977101\n","-------\n","ypred: [Value(data = 0.9497421205222577), Value(data = -0.951287608269161)]\n","step: 191, loss: 0.004898751557737979\n","-------\n","ypred: [Value(data = 0.9498895934826569), Value(data = -0.9514429888115554)]\n","step: 192, loss: 0.004868836176888115\n","-------\n","ypred: [Value(data = 0.9500358732338523), Value(data = -0.9515970992651852)]\n","step: 193, loss: 0.004839254763048011\n","-------\n","ypred: [Value(data = 0.9501809754816053), Value(data = -0.9517499563194851)]\n","step: 194, loss: 0.004810001919136007\n","-------\n","ypred: [Value(data = 0.9503249156459633), Value(data = -0.9519015763614378)]\n","step: 195, loss: 0.004781072362095258\n","-------\n","ypred: [Value(data = 0.9504677088678827), Value(data = -0.9520519754825558)]\n","step: 196, loss: 0.0047524609199222564\n","-------\n","ypred: [Value(data = 0.9506093700156629), Value(data = -0.9522011694856697)]\n","step: 197, loss: 0.004724162528787371\n","-------\n","ypred: [Value(data = 0.9507499136912034), Value(data = -0.9523491738915263)]\n","step: 198, loss: 0.004696172230243911\n","-------\n","ypred: [Value(data = 0.9508893542360843), Value(data = -0.9524960039452025)]\n","step: 199, loss: 0.004668485168523024\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":292,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":293,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 0.9306853413581848\n","Epoch 10 loss: 0.05523651838302612\n","Epoch 20 loss: 0.00045403599506244063\n","Epoch 30 loss: 3.204969516445999e-06\n","\n","Prediction:\n","tensor([[ 0.9999],\n","        [-0.9998]])\n","Loss: 3.766287903772536e-08\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":294,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":295,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[-0.3102,  0.5688, -0.5803],\n","        [ 0.0894,  0.2054, -0.5586],\n","        [-0.2724, -0.0827, -0.2614],\n","        [-0.2407,  0.1805, -0.2481]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 1.6663, -1.7896],\n","        [ 1.3536, -0.2165],\n","        [-0.5316, -0.8653],\n","        [ 0.3080, -1.0267]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.3755],\n","        [-0.2527],\n","        [ 0.4609],\n","        [ 0.1294]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 1.2908, -2.1652],\n","        [ 1.1008, -0.4693],\n","        [-0.0707, -0.4044],\n","        [ 0.4374, -0.8972]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.8593, -0.9740],\n","        [ 0.8008, -0.4376],\n","        [-0.0706, -0.3837],\n","        [ 0.4115, -0.7150]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[-0.2878, -0.5334, -0.1449, -0.2898],\n","        [-0.3842, -0.3894,  0.1235, -0.5061],\n","        [-0.0631,  0.4527, -0.2093, -0.0152],\n","        [-0.4022, -0.0861, -0.1301, -0.4002]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.8593, -0.9740],\n","        [ 0.8008, -0.4376],\n","        [-0.0706, -0.3837],\n","        [ 0.4115, -0.7150]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.7835,  0.7765],\n","        [-0.8590,  0.8591],\n","        [ 0.3168, -0.0455],\n","        [-0.5700,  0.7655]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[ 0.1490],\n","        [-0.0492],\n","        [ 0.4933],\n","        [-0.3061]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.6345,  0.9255],\n","        [-0.9082,  0.8099],\n","        [ 0.8101,  0.4478],\n","        [-0.8761,  0.4594]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.5611,  0.7285],\n","        [-0.7203,  0.6695],\n","        [ 0.6697,  0.4201],\n","        [-0.7045,  0.4296]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[-0.6791, -0.4342,  0.1803, -0.4193]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.5611,  0.7285],\n","        [-0.7203,  0.6695],\n","        [ 0.6697,  0.4201],\n","        [-0.7045,  0.4296]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 1.1099, -0.8898]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[-0.1100]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9999, -0.9998]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9999, -0.9998]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":296,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":296,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":297,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9999, -0.9998]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(2.2974e-08)"]},"execution_count":297,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","\n","# loss = torch.sum(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":298,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9999, -0.9998]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-9.3639e-05,  1.9282e-04]])\n","squared_difference: tensor([[8.7682e-09, 3.7180e-08]])\n"]},{"data":{"text/plain":["tensor(2.2974e-08)"]},"execution_count":298,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":299,"metadata":{},"outputs":[{"data":{"text/plain":["[0.9999063611030579, -0.9998071789741516]"]},"execution_count":299,"metadata":{},"output_type":"execute_result"}],"source":["# for item in output.item:\n","#   print(item)\n","# type(output)\n","output.tolist()[0]\n"]},{"cell_type":"code","execution_count":300,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.297409551488272e-08\n"]}],"source":["import numpy as np\n","\n","def mse_loss(y_true, y_pred):\n","  \"\"\"Calculates the mean squared error loss.\n","\n","  Args:\n","    y_true: The ground truth labels.\n","    y_pred: The predicted labels.\n","\n","  Returns:\n","    The mean squared error loss.\n","  \"\"\"\n","\n","  loss = np.mean((y_true - y_pred)**2)\n","  return loss\n","\n","def main():\n","  \"\"\"Main function.\"\"\"\n","\n","  # y_true = np.array([1, 2, 3, 4, 5])\n","  y_true = np.array([1.0, -1.0])\n","\n","  # y_pred = np.array([0, 1, 2, 3, 4])\n","  # y_pred = np.array([0.9997345209121704, -0.9980572462081909])\n","  y_pred = np.array(output.tolist()[0])  \n","\n","  loss = mse_loss(y_true, y_pred)\n","  print(loss)\n","\n","if __name__ == \"__main__\":\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":301,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":301,"metadata":{},"output_type":"execute_result"}],"source":["len(squared_difference)\n"]},{"cell_type":"code","execution_count":302,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(2.2974e-08)"]},"execution_count":302,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":303,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(4.5948e-08)"]},"execution_count":303,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":304,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- torch results matched backward pass results ----\n","x0.data.item()  = -3.000000\n","x0.grad.item()  =  1.000000\n","w0.data.item()  =  2.000000\n","w0.grad.item()  = -1.500000 <-- result matched micrograd\n","---\n","x1.data.item()  =  0.000000\n","x1.grad.item()  =  0.500000\n","w1.data.item()  =  1.000000\n","w1.grad.item()  =  0.000000\n","---\n","x2.data.item()  =  0.500000\n","x2.grad.item()  =  0.500000\n","w2.data.item()  =  1.000000\n","w2.grad.item()  =  0.250000\n","---\n","out.data.item() = -0.707107 <-- result matched micrograd\n"]}],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <-- result matched micrograd')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <-- result matched micrograd')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":305,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":306,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["parameters in MLP: 41\n","\n","i:  0,   0.9127026664\n","i:  1,   0.0431730362\n","i:  2,  -0.9241878631\n","i:  3,   0.7266427912\n","i:  4,   0.7759467967\n","---\n","i: 36,  -0.9195648217\n","i: 37,  -0.1360106546\n","i: 38,  -0.4362070821\n","i: 39,  -0.6157402864\n","i: 40,  -0.1249216480\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'parameters in MLP: {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":307,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 3)\n","xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":308,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","layer: 0, neuron 0\n","w0:  0.9127027,   w0.grad:  0.0000000\n","w1:  0.0431730,   w1.grad:  0.0000000\n","w2: -0.9241879,   w2.grad:  0.0000000\n","b:   0.7266428\n","\n","layer: 0, neuron 1\n","w0:  0.7759468,   w0.grad:  0.0000000\n","w1: -0.7816977,   w1.grad:  0.0000000\n","w2: -0.8746657,   w2.grad:  0.0000000\n","b:   0.2919712\n","\n","layer: 0, neuron 2\n","w0:  0.9783844,   w0.grad:  0.0000000\n","w1:  0.8445765,   w1.grad:  0.0000000\n","w2: -0.4438930,   w2.grad:  0.0000000\n","b:  -0.8634206\n","\n","layer: 0, neuron 3\n","w0: -0.3220315,   w0.grad:  0.0000000\n","w1:  0.5315550,   w1.grad:  0.0000000\n","w2:  0.0025346,   w2.grad:  0.0000000\n","b:  -0.3258758\n","\n","------\n","layer: 1, neuron_cnt: 4\n","----\n","layer: 1, neuron 0\n","w0: -0.9866332,   w0.grad:  0.0000000\n","w1:  0.0842178,   w1.grad:  0.0000000\n","w2:  0.1968115,   w2.grad:  0.0000000\n","w3: -0.5771002,   w3.grad:  0.0000000\n","b:   0.3780041\n","\n","layer: 1, neuron 1\n","w0:  0.9560823,   w0.grad:  0.0000000\n","w1: -0.7367079,   w1.grad:  0.0000000\n","w2:  0.9639842,   w2.grad:  0.0000000\n","w3: -0.3706112,   w3.grad:  0.0000000\n","b:   0.3755782\n","\n","layer: 1, neuron 2\n","w0: -0.0244008,   w0.grad:  0.0000000\n","w1:  0.5520212,   w1.grad:  0.0000000\n","w2:  0.3343159,   w2.grad:  0.0000000\n","w3: -0.1639824,   w3.grad:  0.0000000\n","b:   0.0338800\n","\n","layer: 1, neuron 3\n","w0:  0.2890883,   w0.grad:  0.0000000\n","w1:  0.6391971,   w1.grad:  0.0000000\n","w2:  0.1444065,   w2.grad:  0.0000000\n","w3: -0.5513208,   w3.grad:  0.0000000\n","b:  -0.5754913\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0: -0.9195648,   w0.grad:  0.0000000\n","w1: -0.1360107,   w1.grad:  0.0000000\n","w2: -0.4362071,   w2.grad:  0.0000000\n","w3: -0.6157403,   w3.grad:  0.0000000\n","b:  -0.1249216\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":309,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(4, 3):\n","[[ 0.91270267  0.04317304 -0.92418786]\n"," [ 0.7759468  -0.78169775 -0.87466573]\n"," [ 0.97838442  0.84457648 -0.44389302]\n"," [-0.3220315   0.53155503  0.00253462]]\n","b_mat(1, 4):\n","[[ 0.72664279  0.29197119 -0.86342056 -0.32587576]]\n","\n","i: 1\n","w_mat(4, 4):\n","[[-0.98663324  0.08421784  0.19681153 -0.57710017]\n"," [ 0.95608228 -0.73670794  0.96398419 -0.3706112 ]\n"," [-0.02440076  0.55202118  0.33431592 -0.16398243]\n"," [ 0.28908835  0.63919711  0.14440649 -0.55132077]]\n","b_mat(1, 4):\n","[[ 0.37800413  0.37557823  0.03388001 -0.57549126]]\n","\n","i: 2\n","w_mat(1, 4):\n","[[-0.91956482 -0.13601065 -0.43620708 -0.61574029]]\n","b_mat(1, 1):\n","[[-0.12492165]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":310,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[ 0.91270267  0.04317304 -0.92418786]\n"," [ 0.7759468  -0.78169775 -0.87466573]\n"," [ 0.97838442  0.84457648 -0.44389302]\n"," [-0.3220315   0.53155503  0.00253462]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","weights_x_inputs (4, 2):\n","[[ 2.8791123   2.23284103]\n"," [ 0.08146609  2.67220527]\n"," [ 4.9343913   1.86863029]\n"," [ 0.94806746 -1.49638223]]\n","\n","bias (4, 1):\n","[[ 0.72664279]\n"," [ 0.29197119]\n"," [-0.86342056]\n"," [-0.32587576]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[ 3.6057551   2.95948382]\n"," [ 0.37343728  2.96417645]\n"," [ 4.07097074  1.00520973]\n"," [ 0.6221917  -1.82225799]]\n","\n","output (4, 2):\n","[[ 0.99852501  0.99463846]\n"," [ 0.3569946   0.99468842]\n"," [ 0.99941803  0.76377344]\n"," [ 0.55265218 -0.94906308]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[-0.98663324  0.08421784  0.19681153 -0.57710017]\n"," [ 0.95608228 -0.73670794  0.96398419 -0.3706112 ]\n"," [-0.02440076  0.55202118  0.33431592 -0.16398243]\n"," [ 0.28908835  0.63919711  0.14440649 -0.55132077]]\n","\n","input (4, 2):\n","[[ 0.99852501  0.99463846]\n"," [ 0.3569946   0.99468842]\n"," [ 0.99941803  0.76377344]\n"," [ 0.55265218 -0.94906308]]\n","\n","weights_x_inputs (4, 2):\n","[[-1.07735132 -0.19954898]\n"," [ 1.45027539  1.30616028]\n"," [ 0.41619992  0.93579043]\n"," [ 0.35648569  1.55687238]]\n","\n","bias (4, 1):\n","[[ 0.37800413]\n"," [ 0.37557823]\n"," [ 0.03388001]\n"," [-0.57549126]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-0.6993472   0.17845514]\n"," [ 1.82585362  1.68173851]\n"," [ 0.45007993  0.96967044]\n"," [-0.21900558  0.98138112]]\n","\n","output (4, 2):\n","[[-0.60395325  0.17658459]\n"," [ 0.94941883  0.93308679]\n"," [ 0.4219647   0.74855943]\n"," [-0.21557005  0.75366316]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[-0.91956482 -0.13601065 -0.43620708 -0.61574029]]\n","\n","input (4, 2):\n","[[-0.60395325  0.17658459]\n"," [ 0.94941883  0.93308679]\n"," [ 0.4219647   0.74855943]\n"," [-0.21557005  0.75366316]]\n","\n","weights_x_inputs (1, 2):\n","[[ 0.37491426 -1.07987842]]\n","\n","bias (1, 1):\n","[[-0.12492165]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[ 0.24999262 -1.20480007]]\n","\n","output (1, 2):\n","[[ 0.24491172 -0.83511288]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [ 0.24491172 -0.83511288]\n","desired output:     [1.0, -1.0]\n","loss:               0.5973460727671431\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":311,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [0.24491172088933533, -0.8351128764371283]\n","ys:                 [1.0, -1.0]\n","loss_data:          0.5973460727671428\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":312,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0      0.9127026664    -0.0008400079           0.05000         0.9127446668\n","  1      0.0431730362    -0.0058963993           0.05000         0.0434678561\n","  2     -0.9241878631     0.0021059630           0.05000        -0.9242931612\n","  3      0.7266427912    -0.0008414937           0.05000         0.7266848659\n","  4      0.7759467967     1.5163666726           0.05000         0.7001284631\n","  5     -0.7816977453     2.2765151432           0.05000        -0.8955235025\n","  6     -0.8746657325    -0.7588979306           0.05000        -0.8367208360\n","  7      0.2919711852     0.7583619849           0.05000         0.2540530860\n","  8      0.9783844240    -0.0356737693           0.05000         0.9801681124\n","  9      0.8445764761     0.0139054088           0.05000         0.8438812057\n"," 10     -0.4438930213    -0.0066780473           0.05000        -0.4435591189\n"," 11     -0.8634205579    -0.0117081517           0.05000        -0.8628351503\n"," 12     -0.3220315026    -1.4081059927           0.05000        -0.2516262030\n"," 13      0.5315550276    -2.1502313527           0.05000         0.6390665952\n"," 14      0.0025346152     0.7178974922           0.05000        -0.0333602594\n"," 15     -0.3258757599    -0.7075141203           0.05000        -0.2905000539\n"," 16     -0.9866332382     0.7396038999           0.05000        -1.0236134331\n"," 17      0.0842178371     0.2076115512           0.05000         0.0738372595\n"," 18      0.1968115252     0.7608678166           0.05000         0.1587681344\n"," 19     -0.5771001698     0.5426553667           0.05000        -0.6042329381\n"," 20      0.3780041265     0.7403504082           0.05000         0.3409866061\n"," 21      0.9560822765     0.0172642464           0.05000         0.9552190642\n"," 22     -0.7367079394     0.0050504320           0.05000        -0.7369604610\n"," 23      0.9639841860     0.0176865331           0.05000         0.9630998594\n"," 24     -0.3706112003     0.0121877116           0.05000        -0.3712205859\n"," 25      0.3755782272     0.0172829158           0.05000         0.3747140814\n"," 26     -0.0244007576     0.4891938624           0.05000        -0.0488604508\n"," 27      0.5520211804     0.1626673760           0.05000         0.5438878116\n"," 28      0.3343159180     0.4940664642           0.05000         0.3096125948\n"," 29     -0.1639824304     0.2994506292           0.05000        -0.1789549618\n"," 30      0.0338800069     0.4898419999           0.05000         0.0093879069\n"," 31      0.2890883467     0.8058516392           0.05000         0.2487957647\n"," 32      0.6391971143     0.2711468671           0.05000         0.6256397710\n"," 33      0.1444064915     0.8127236557           0.05000         0.1037703087\n"," 34     -0.5513207712     0.4858154865           0.05000        -0.5756115455\n"," 35     -0.5754912618     0.8069387117           0.05000        -0.6158381974\n"," 36     -0.9195648217     0.8749886627           0.05000        -0.9633142548\n"," 37     -0.1360106546    -1.2546805454           0.05000        -0.0732766273\n"," 38     -0.4362070821    -0.5243231854           0.05000        -0.4099909229\n"," 39     -0.6157402864     0.3812263016           0.05000        -0.6348016015\n"," 40     -0.1249216480    -1.3198082941           0.05000        -0.0589312333\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":313,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.7816266627507924), Value(data = -0.7386891354585553)]\n","step: 0, loss: 0.11597028234875342\n","-------\n","ypred: [Value(data = 0.7960803659951045), Value(data = -0.7698792744294154)]\n","step: 1, loss: 0.09453876546982287\n","-------\n","ypred: [Value(data = 0.8091874817257491), Value(data = -0.7918458716310229)]\n","step: 2, loss: 0.07973755828720994\n","-------\n","ypred: [Value(data = 0.8205509242801284), Value(data = -0.8085215245416539)]\n","step: 3, loss: 0.06886597734056864\n","-------\n","ypred: [Value(data = 0.8303356790831811), Value(data = -0.8217812469457463)]\n","step: 4, loss: 0.06054790573237839\n","-------\n","ypred: [Value(data = 0.8387999566755487), Value(data = -0.8326659215838151)]\n","step: 5, loss: 0.05398614776719887\n","-------\n","ypred: [Value(data = 0.8461812059238688), Value(data = -0.8418132599619818)]\n","step: 6, loss: 0.048683266134890824\n","-------\n","ypred: [Value(data = 0.8526746140411545), Value(data = -0.8496416163327182)]\n","step: 7, loss: 0.044312412886960306\n","-------\n","ypred: [Value(data = 0.8584354752670439), Value(data = -0.8564395581801288)]\n","step: 8, loss: 0.04065011511838435\n","-------\n","ypred: [Value(data = 0.8635865167312371), Value(data = -0.8624139418878743)]\n","step: 9, loss: 0.03753856180435027\n","-------\n","ypred: [Value(data = 0.8682250575805087), Value(data = -0.8677176606100773)]\n","step: 10, loss: 0.03486325276413095\n","-------\n","ypred: [Value(data = 0.8724288529560458), Value(data = -0.872466587884557)]\n","step: 11, loss: 0.03253916876391762\n","-------\n","ypred: [Value(data = 0.8762605962882315), Value(data = -0.8767503997560662)]\n","step: 12, loss: 0.030501903991233523\n","-------\n","ypred: [Value(data = 0.8797713210067386), Value(data = -0.880639745950265)]\n","step: 13, loss: 0.028701805499281988\n","-------\n","ypred: [Value(data = 0.8830029608262738), Value(data = -0.8841911499410687)]\n","step: 14, loss: 0.02709999692739044\n","-------\n","ypred: [Value(data = 0.8859902830959703), Value(data = -0.8874504456456286)]\n","step: 15, loss: 0.025665617733904605\n","-------\n","ypred: [Value(data = 0.8887623585353568), Value(data = -0.8904552425741871)]\n","step: 16, loss: 0.024373866758096708\n","-------\n","ypred: [Value(data = 0.8913436878064234), Value(data = -0.8932367291169938)]\n","step: 17, loss: 0.023204590189146147\n","-------\n","ypred: [Value(data = 0.8937550727628132), Value(data = -0.895821014682548)]\n","step: 18, loss: 0.022141245545408993\n","-------\n","ypred: [Value(data = 0.8960142963460471), Value(data = -0.8982301441004439)]\n","step: 19, loss: 0.021170130134224127\n","-------\n","ypred: [Value(data = 0.8981366578810643), Value(data = -0.9004828749621728)]\n","step: 20, loss: 0.02027979864343386\n","-------\n","ypred: [Value(data = 0.9001353981514512), Value(data = -0.9025952807564144)]\n","step: 21, loss: 0.019460618033290925\n","-------\n","ypred: [Value(data = 0.9020220397274769), Value(data = -0.9045812241513922)]\n","step: 22, loss: 0.018704423483610982\n","-------\n","ypred: [Value(data = 0.9038066615748732), Value(data = -0.9064527322246442)]\n","step: 23, loss: 0.0180042496656051\n","-------\n","ypred: [Value(data = 0.9054981222773243), Value(data = -0.9082202967740065)]\n","step: 24, loss: 0.01735411881736299\n","-------\n","ypred: [Value(data = 0.9071042427596223), Value(data = -0.9098931167632504)]\n","step: 25, loss: 0.016748872119904418\n","-------\n","ypred: [Value(data = 0.9086319568489205), Value(data = -0.9114792956270419)]\n","step: 26, loss: 0.016184034411942168\n","-------\n","ypred: [Value(data = 0.910087436109963), Value(data = -0.9129860030365544)]\n","step: 27, loss: 0.0156557048128345\n","-------\n","ypred: [Value(data = 0.91147619396048), Value(data = -0.9144196084455085)]\n","step: 28, loss: 0.015160467654342643\n","-------\n","ypred: [Value(data = 0.9128031729895405), Value(data = -0.9157857920506567)]\n","step: 29, loss: 0.014695319461227239\n","-------\n","ypred: [Value(data = 0.914072818574386), Value(data = -0.9170896375419473)]\n","step: 30, loss: 0.014257608710676054\n","-------\n","ypred: [Value(data = 0.9152891412549027), Value(data = -0.9183357100683683)]\n","step: 31, loss: 0.013844985839369427\n","-------\n","ypred: [Value(data = 0.9164557698318831), Value(data = -0.9195281221230671)]\n","step: 32, loss: 0.013455361523423286\n","-------\n","ypred: [Value(data = 0.9175759967706881), Value(data = -0.920670589496827)]\n","step: 33, loss: 0.013086871679126547\n","-------\n","ypred: [Value(data = 0.9186528171899995), Value(data = -0.9217664790199026)]\n","step: 34, loss: 0.012737847956066984\n","-------\n","ypred: [Value(data = 0.9196889624768386), Value(data = -0.9228188494776801)]\n","step: 35, loss: 0.012406792743995634\n","-------\n","ypred: [Value(data = 0.9206869293794532), Value(data = -0.9238304868229477)]\n","step: 36, loss: 0.012092357908888997\n","-------\n","ypred: [Value(data = 0.9216490052781405), Value(data = -0.9248039345998993)]\n","step: 37, loss: 0.011793326625561073\n","-------\n","ypred: [Value(data = 0.922577290212292), Value(data = -0.925741520329803)]\n","step: 38, loss: 0.011508597793800732\n","-------\n","ypred: [Value(data = 0.9234737161435435), Value(data = -0.9266453784760484)]\n","step: 39, loss: 0.011237172619801138\n","-------\n","ypred: [Value(data = 0.9243400638550171), Value(data = -0.927517470499873)]\n","step: 40, loss: 0.010978143020199671\n","-------\n","ypred: [Value(data = 0.9251779778214474), Value(data = -0.9283596024319452)]\n","step: 41, loss: 0.010730681566596777\n","-------\n","ypred: [Value(data = 0.9259889793315518), Value(data = -0.9291734403149325)]\n","step: 42, loss: 0.010494032737207892\n","-------\n","ypred: [Value(data = 0.9267744781000119), Value(data = -0.9299605238149249)]\n","step: 43, loss: 0.01026750528180534\n","-------\n","ypred: [Value(data = 0.9275357825700568), Value(data = -0.9307222782525499)]\n","step: 44, loss: 0.010050465538251206\n","-------\n","ypred: [Value(data = 0.9282741090774472), Value(data = -0.9314600252658389)]\n","step: 45, loss: 0.009842331565193374\n","-------\n","ypred: [Value(data = 0.9289905900214944), Value(data = -0.9321749922847559)]\n","step: 46, loss: 0.009642567977068413\n","-------\n","ypred: [Value(data = 0.9296862811676946), Value(data = -0.9328683209705844)]\n","step: 47, loss: 0.009450681385336984\n","-------\n","ypred: [Value(data = 0.9303621681888933), Value(data = -0.9335410747510375)]\n","step: 48, loss: 0.009266216364599168\n","-------\n","ypred: [Value(data = 0.9310191725369981), Value(data = -0.9341942455632569)]\n","step: 49, loss: 0.009088751874469374\n","-------\n","ypred: [Value(data = 0.9316581567246807), Value(data = -0.9348287599011276)]\n","step: 50, loss: 0.008917898078293188\n","-------\n","ypred: [Value(data = 0.9322799290858329), Value(data = -0.9354454842500556)]\n","step: 51, loss: 0.008753293508329641\n","-------\n","ypred: [Value(data = 0.9328852480744683), Value(data = -0.9360452299811056)]\n","step: 52, loss: 0.008594602534195337\n","-------\n","ypred: [Value(data = 0.9334748261540112), Value(data = -0.936628757766846)]\n","step: 53, loss: 0.008441513097412116\n","-------\n","ypred: [Value(data = 0.9340493333222983), Value(data = -0.9371967815730995)]\n","step: 54, loss: 0.008293734680010285\n","-------\n","ypred: [Value(data = 0.9346094003119211), Value(data = -0.9377499722738467)]\n","step: 55, loss: 0.00815099647947344\n","-------\n","ypred: [Value(data = 0.9351556215006573), Value(data = -0.9382889609305588)]\n","step: 56, loss: 0.00801304576599611\n","-------\n","ypred: [Value(data = 0.9356885575625166), Value(data = -0.9388143417721174)]\n","step: 57, loss: 0.007879646401169\n","-------\n","ypred: [Value(data = 0.9362087378862747), Value(data = -0.9393266749070555)]\n","step: 58, loss: 0.007750577499896142\n","-------\n","ypred: [Value(data = 0.9367166627852173), Value(data = -0.9398264887960435)]\n","step: 59, loss: 0.007625632219652589\n","-------\n","ypred: [Value(data = 0.9372128055190588), Value(data = -0.940314282509238)]\n","step: 60, loss: 0.007504616663174583\n","-------\n","ypred: [Value(data = 0.9376976141466163), Value(data = -0.9407905277902459)]\n","step: 61, loss: 0.007387348882381548\n","-------\n","ypred: [Value(data = 0.9381715132257245), Value(data = -0.9412556709459501)]\n","step: 62, loss: 0.0072736579728072594\n","-------\n","ypred: [Value(data = 0.9386349053750591), Value(data = -0.9417101345792848)]\n","step: 63, loss: 0.007163383249093043\n","-------\n","ypred: [Value(data = 0.9390881727109321), Value(data = -0.9421543191801279)]\n","step: 64, loss: 0.007056373493207747\n","-------\n","ypred: [Value(data = 0.9395316781707126), Value(data = -0.9425886045878267)]\n","step: 65, loss: 0.006952486268023193\n","-------\n","ypred: [Value(data = 0.9399657667333025), Value(data = -0.9430133513374008)]\n","step: 66, loss: 0.0068515872897147705\n","-------\n","ypred: [Value(data = 0.9403907665459937), Value(data = -0.9434289019001912)]\n","step: 67, loss: 0.006753549853192405\n","-------\n","ypred: [Value(data = 0.9408069899660796), Value(data = -0.9438355818285832)]\n","step: 68, loss: 0.0066582543054095784\n","-------\n","ypred: [Value(data = 0.9412147345247371), Value(data = -0.9442337008134382)]\n","step: 69, loss: 0.006565587561962259\n","-------\n","ypred: [Value(data = 0.9416142838199449), Value(data = -0.9446235536619836)]\n","step: 70, loss: 0.006475442662885154\n","-------\n","ypred: [Value(data = 0.9420059083445292), Value(data = -0.9450054212031267)]\n","step: 71, loss: 0.0063877183639886505\n","-------\n","ypred: [Value(data = 0.9423898662548352), Value(data = -0.9453795711264684)]\n","step: 72, loss: 0.006302318760464295\n","-------\n","ypred: [Value(data = 0.9427664040849929), Value(data = -0.9457462587606685)]\n","step: 73, loss: 0.006219152939826653\n","-------\n","ypred: [Value(data = 0.9431357574112684), Value(data = -0.9461057277962747)]\n","step: 74, loss: 0.006138134661559355\n","-------\n","ypred: [Value(data = 0.9434981514705734), Value(data = -0.9464582109576306)]\n","step: 75, loss: 0.006059182061099858\n","-------\n","ypred: [Value(data = 0.9438538017368242), Value(data = -0.9468039306280415)]\n","step: 76, loss: 0.00598221737603407\n","-------\n","ypred: [Value(data = 0.9442029144585053), Value(data = -0.9471430994319934)]\n","step: 77, loss: 0.005907166692581011\n","-------\n","ypred: [Value(data = 0.9445456871604848), Value(data = -0.9474759207778605)]\n","step: 78, loss: 0.005833959710636409\n","-------\n","ypred: [Value(data = 0.9448823091128592), Value(data = -0.9478025893642283)]\n","step: 79, loss: 0.005762529525809773\n","-------\n","ypred: [Value(data = 0.9452129617693582), Value(data = -0.9481232916526805)]\n","step: 80, loss: 0.005692812427038655\n","-------\n","ypred: [Value(data = 0.9455378191776168), Value(data = -0.9484382063096355)]\n","step: 81, loss: 0.005624747708497677\n","-------\n","ypred: [Value(data = 0.9458570483634257), Value(data = -0.9487475046196008)]\n","step: 82, loss: 0.005558277494638272\n","-------\n","ypred: [Value(data = 0.9461708096908863), Value(data = -0.9490513508719989)]\n","step: 83, loss: 0.00549334657730295\n","-------\n","ypred: [Value(data = 0.946479257200239), Value(data = -0.9493499027235396)]\n","step: 84, loss: 0.005429902263953077\n","-------\n","ypred: [Value(data = 0.94678253892498), Value(data = -0.9496433115379422)]\n","step: 85, loss: 0.005367894236136012\n","-------\n","ypred: [Value(data = 0.9470807971897528), Value(data = -0.9499317227046578)]\n","step: 86, loss: 0.005307274417395359\n","-------\n","ypred: [Value(data = 0.9473741688903752), Value(data = -0.950215275938111)]\n","step: 87, loss: 0.00524799684989719\n","-------\n","ypred: [Value(data = 0.9476627857572564), Value(data = -0.950494105558848)]\n","step: 88, loss: 0.005190017579109335\n","-------\n","ypred: [Value(data = 0.9479467746033553), Value(data = -0.9507683407578701)]\n","step: 89, loss: 0.005133294545927092\n","-------\n","ypred: [Value(data = 0.9482262575577415), Value(data = -0.951038105845327)]\n","step: 90, loss: 0.005077787485690723\n","-------\n","ypred: [Value(data = 0.9485013522857352), Value(data = -0.9513035204846491)]\n","step: 91, loss: 0.005023457833586948\n","-------\n","ypred: [Value(data = 0.9487721721965314), Value(data = -0.9515646999131174)]\n","step: 92, loss: 0.004970268635968195\n","-------\n","ypred: [Value(data = 0.9490388266391375), Value(data = -0.9518217551497864)]\n","step: 93, loss: 0.004918184467163013\n","-------\n","ypred: [Value(data = 0.949301421087396), Value(data = -0.9520747931916093)]\n","step: 94, loss: 0.004867171351384556\n","-------\n","ypred: [Value(data = 0.9495600573148031), Value(data = -0.9523239171985445)]\n","step: 95, loss: 0.004817196689377193\n","-------\n","ypred: [Value(data = 0.9498148335597824), Value(data = -0.9525692266683715)]\n","step: 96, loss: 0.004768229189468667\n","-------\n","ypred: [Value(data = 0.950065844682022), Value(data = -0.9528108176018804)]\n","step: 97, loss: 0.004720238802722951\n","-------\n","ypred: [Value(data = 0.9503131823104439), Value(data = -0.9530487826590579)]\n","step: 98, loss: 0.004673196661911564\n","-------\n","ypred: [Value(data = 0.9505569349833254), Value(data = -0.9532832113068405)]\n","step: 99, loss: 0.004627075024044433\n","-------\n","ypred: [Value(data = 0.950797188281064), Value(data = -0.9535141899589705)]\n","step: 100, loss: 0.004581847216219745\n","-------\n","ypred: [Value(data = 0.9510340249520361), Value(data = -0.9537418021084454)]\n","step: 101, loss: 0.004537487584572045\n","-------\n","ypred: [Value(data = 0.9512675250319703), Value(data = -0.9539661284530212)]\n","step: 102, loss: 0.0044939714461133835\n","-------\n","ypred: [Value(data = 0.951497765957226), Value(data = -0.9541872470141949)]\n","step: 103, loss: 0.004451275043278419\n","-------\n","ypred: [Value(data = 0.9517248226723437), Value(data = -0.9544052332500622)]\n","step: 104, loss: 0.004409375500997893\n","-------\n","ypred: [Value(data = 0.9519487677322044), Value(data = -0.9546201601624207)]\n","step: 105, loss: 0.004368250786137994\n","-------\n","ypred: [Value(data = 0.9521696713991153), Value(data = -0.9548320983984588)]\n","step: 106, loss: 0.004327879669155109\n","-------\n","ypred: [Value(data = 0.9523876017351175), Value(data = -0.9550411163473544)]\n","step: 107, loss: 0.004288241687825906\n","-------\n","ypred: [Value(data = 0.9526026246897894), Value(data = -0.9552472802320789)]\n","step: 108, loss: 0.004249317112923034\n","-------\n","ypred: [Value(data = 0.9528148041838062), Value(data = -0.9554506541966841)]\n","step: 109, loss: 0.00421108691571598\n","-------\n","ypred: [Value(data = 0.9530242021884932), Value(data = -0.9556513003893361)]\n","step: 110, loss: 0.004173532737184472\n","-------\n","ypred: [Value(data = 0.953230878801599), Value(data = -0.9558492790413345)]\n","step: 111, loss: 0.004136636858840667\n","-------\n","ypred: [Value(data = 0.9534348923194997), Value(data = -0.9560446485423464)]\n","step: 112, loss: 0.004100382175062436\n","-------\n","ypred: [Value(data = 0.9536362993060302), Value(data = -0.9562374655120671)]\n","step: 113, loss: 0.0040647521668475285\n","-------\n","ypred: [Value(data = 0.9538351546581273), Value(data = -0.9564277848685044)]\n","step: 114, loss: 0.004029730876904357\n","-------\n","ypred: [Value(data = 0.954031511668458), Value(data = -0.956615659893077)]\n","step: 115, loss: 0.003995302886000284\n","-------\n","ypred: [Value(data = 0.9542254220851962), Value(data = -0.9568011422926952)]\n","step: 116, loss: 0.003961453290494409\n","-------\n","ypred: [Value(data = 0.9544169361690978), Value(data = -0.9569842822589936)]\n","step: 117, loss: 0.003928167680986036\n","-------\n","ypred: [Value(data = 0.9546061027480197), Value(data = -0.9571651285248631)]\n","step: 118, loss: 0.0038954321220148374\n","-------\n","ypred: [Value(data = 0.9547929692690148), Value(data = -0.9573437284184325)]\n","step: 119, loss: 0.003863233132752682\n","-------\n","ypred: [Value(data = 0.9549775818481318), Value(data = -0.9575201279146277)]\n","step: 120, loss: 0.0038315576686312677\n","-------\n","ypred: [Value(data = 0.9551599853180355), Value(data = -0.9576943716844426)]\n","step: 121, loss: 0.0038003931038528864\n","-------\n","ypred: [Value(data = 0.9553402232735624), Value(data = -0.9578665031420349)]\n","step: 122, loss: 0.0037697272147354122\n","-------\n","ypred: [Value(data = 0.955518338115314), Value(data = -0.9580365644897625)]\n","step: 123, loss: 0.0037395481638453944\n","-------\n","ypred: [Value(data = 0.9556943710913883), Value(data = -0.9582045967612666)]\n","step: 124, loss: 0.0037098444848759353\n","-------\n","ypred: [Value(data = 0.9558683623373436), Value(data = -0.9583706398626973)]\n","step: 125, loss: 0.0036806050682292356\n","-------\n","ypred: [Value(data = 0.9560403509144799), Value(data = -0.9585347326121793)]\n","step: 126, loss: 0.003651819147265537\n","-------\n","ypred: [Value(data = 0.9562103748465232), Value(data = -0.9586969127776018)]\n","step: 127, loss: 0.0036234762851830426\n","-------\n","ypred: [Value(data = 0.9563784711547896), Value(data = -0.9588572171128216)]\n","step: 128, loss: 0.0035955663624950276\n","-------\n","ypred: [Value(data = 0.9565446758919053), Value(data = -0.9590156813923514)]\n","step: 129, loss: 0.0035680795650728106\n","-------\n","ypred: [Value(data = 0.9567090241741474), Value(data = -0.9591723404446106)]\n","step: 130, loss: 0.0035410063727253317\n","-------\n","ypred: [Value(data = 0.9568715502124777), Value(data = -0.9593272281838098)]\n","step: 131, loss: 0.0035143375482867088\n","-------\n","ypred: [Value(data = 0.9570322873423267), Value(data = -0.9594803776405307)]\n","step: 132, loss: 0.0034880641271863794\n","-------\n","ypred: [Value(data = 0.9571912680521887), Value(data = -0.9596318209910689)]\n","step: 133, loss: 0.0034621774074766656\n","-------\n","ypred: [Value(data = 0.9573485240110845), Value(data = -0.9597815895855928)]\n","step: 134, loss: 0.003436668940294734\n","-------\n","ypred: [Value(data = 0.9575040860949426), Value(data = -0.9599297139751803)]\n","step: 135, loss: 0.003411530520736905\n","-------\n","ypred: [Value(data = 0.9576579844119503), Value(data = -0.9600762239377819)]\n","step: 136, loss: 0.003386754179124784\n","-------\n","ypred: [Value(data = 0.9578102483269202), Value(data = -0.9602211485031608)]\n","step: 137, loss: 0.00336233217264373\n","-------\n","ypred: [Value(data = 0.95796090648472), Value(data = -0.9603645159768568)]\n","step: 138, loss: 0.0033382569773352986\n","-------\n","ypred: [Value(data = 0.9581099868328041), Value(data = -0.9605063539632233)]\n","step: 139, loss: 0.0033145212804260547\n","-------\n","ypred: [Value(data = 0.9582575166428905), Value(data = -0.9606466893875717)]\n","step: 140, loss: 0.0032911179729768264\n","-------\n","ypred: [Value(data = 0.9584035225318188), Value(data = -0.9607855485174696)]\n","step: 141, loss: 0.0032680401428366386\n","-------\n","ypred: [Value(data = 0.9585480304816281), Value(data = -0.9609229569832302)]\n","step: 142, loss: 0.0032452810678865168\n","-------\n","ypred: [Value(data = 0.9586910658588857), Value(data = -0.9610589397976256)]\n","step: 143, loss: 0.0032228342095598638\n","-------\n","ypred: [Value(data = 0.9588326534333036), Value(data = -0.9611935213748616)]\n","step: 144, loss: 0.0032006932066258155\n","-------\n","ypred: [Value(data = 0.9589728173956689), Value(data = -0.9613267255488457)]\n","step: 145, loss: 0.0031788518692234366\n","-------\n","ypred: [Value(data = 0.9591115813751219), Value(data = -0.9614585755907783)]\n","step: 146, loss: 0.003157304173135031\n","-------\n","ypred: [Value(data = 0.959248968455808), Value(data = -0.9615890942260983)]\n","step: 147, loss: 0.0031360442542872893\n","-------\n","ypred: [Value(data = 0.9593850011929291), Value(data = -0.9617183036508075)]\n","step: 148, loss: 0.0031150664034701493\n","-------\n","ypred: [Value(data = 0.9595197016282219), Value(data = -0.9618462255472054)]\n","step: 149, loss: 0.003094365061262902\n","-------\n","ypred: [Value(data = 0.9596530913048857), Value(data = -0.961972881099055)]\n","step: 150, loss: 0.003073934813158496\n","-------\n","ypred: [Value(data = 0.9597851912819838), Value(data = -0.9620982910062058)]\n","step: 151, loss: 0.0030537703848768907\n","-------\n","ypred: [Value(data = 0.9599160221483386), Value(data = -0.9622224754986966)]\n","step: 152, loss: 0.0030338666378590643\n","-------\n","ypred: [Value(data = 0.9600456040359432), Value(data = -0.9623454543503601)]\n","step: 153, loss: 0.003014218564933453\n","-------\n","ypred: [Value(data = 0.9601739566329087), Value(data = -0.9624672468919493)]\n","step: 154, loss: 0.0029948212861473254\n","-------\n","ypred: [Value(data = 0.9603010991959647), Value(data = -0.9625878720238047)]\n","step: 155, loss: 0.002975670044755847\n","-------\n","ypred: [Value(data = 0.9604270505625335), Value(data = -0.9627073482280832)]\n","step: 156, loss: 0.0029567602033617247\n","-------\n","ypred: [Value(data = 0.9605518291623929), Value(data = -0.9628256935805651)]\n","step: 157, loss: 0.0029380872401990714\n","-------\n","ypred: [Value(data = 0.960675453028945), Value(data = -0.9629429257620572)]\n","step: 158, loss: 0.0029196467455551173\n","-------\n","ypred: [Value(data = 0.9607979398101076), Value(data = -0.963059062069406)]\n","step: 159, loss: 0.0029014344183239463\n","-------\n","ypred: [Value(data = 0.9609193067788409), Value(data = -0.9631741194261392)]\n","step: 160, loss: 0.0028834460626866146\n","-------\n","ypred: [Value(data = 0.9610395708433251), Value(data = -0.9632881143927505)]\n","step: 161, loss: 0.0028656775849120516\n","-------\n","ypred: [Value(data = 0.9611587485568038), Value(data = -0.9634010631766379)]\n","step: 162, loss: 0.0028481249902740403\n","-------\n","ypred: [Value(data = 0.9612768561271022), Value(data = -0.963512981641714)]\n","step: 163, loss: 0.002830784380079043\n","-------\n","ypred: [Value(data = 0.9613939094258381), Value(data = -0.9636238853176972)]\n","step: 164, loss: 0.002813651948800438\n","-------\n","ypred: [Value(data = 0.9615099239973318), Value(data = -0.9637337894090999)]\n","step: 165, loss: 0.002796723981314686\n","-------\n","ypred: [Value(data = 0.9616249150672312), Value(data = -0.9638427088039206)]\n","step: 166, loss: 0.0027799968502352935\n","-------\n","ypred: [Value(data = 0.9617388975508595), Value(data = -0.9639506580820569)]\n","step: 167, loss: 0.0027634670133403916\n","-------\n","ypred: [Value(data = 0.9618518860612973), Value(data = -0.9640576515234436)]\n","step: 168, loss: 0.0027471310110904574\n","-------\n","ypred: [Value(data = 0.96196389491721), Value(data = -0.9641637031159335)]\n","step: 169, loss: 0.0027309854642319988\n","-------\n","ypred: [Value(data = 0.9620749381504273), Value(data = -0.9642688265629242)]\n","step: 170, loss: 0.0027150270714843016\n","-------\n","ypred: [Value(data = 0.9621850295132867), Value(data = -0.9643730352907445)]\n","step: 171, loss: 0.0026992526073055397\n","-------\n","ypred: [Value(data = 0.9622941824857479), Value(data = -0.9644763424558063)]\n","step: 172, loss: 0.00268365891973523\n","-------\n","ypred: [Value(data = 0.9624024102822882), Value(data = -0.9645787609515355)]\n","step: 173, loss: 0.002668242928309852\n","-------\n","ypred: [Value(data = 0.9625097258585841), Value(data = -0.9646803034150827)]\n","step: 174, loss: 0.0026530016220491378\n","-------\n","ypred: [Value(data = 0.9626161419179897), Value(data = -0.9647809822338295)]\n","step: 175, loss: 0.002637932057509721\n","-------\n","ypred: [Value(data = 0.9627216709178182), Value(data = -0.9648808095516919)]\n","step: 176, loss: 0.0026230313569039756\n","-------\n","ypred: [Value(data = 0.9628263250754321), Value(data = -0.9649797972752321)]\n","step: 177, loss: 0.002608296706281291\n","-------\n","ypred: [Value(data = 0.9629301163741513), Value(data = -0.9650779570795825)]\n","step: 178, loss: 0.0025937253537694467\n","-------\n","ypred: [Value(data = 0.9630330565689843), Value(data = -0.9651753004141918)]\n","step: 179, loss: 0.002579314607873708\n","-------\n","ypred: [Value(data = 0.9631351571921881), Value(data = -0.9652718385083963)]\n","step: 180, loss: 0.002565061835831588\n","-------\n","ypred: [Value(data = 0.9632364295586643), Value(data = -0.9653675823768249)]\n","step: 181, loss: 0.0025509644620210657\n","-------\n","ypred: [Value(data = 0.9633368847711955), Value(data = -0.965462542824643)]\n","step: 182, loss: 0.0025370199664202114\n","-------\n","ypred: [Value(data = 0.9634365337255295), Value(data = -0.9655567304526408)]\n","step: 183, loss: 0.0025232258831163834\n","-------\n","ypred: [Value(data = 0.9635353871153142), Value(data = -0.9656501556621709)]\n","step: 184, loss: 0.002509579798863083\n","-------\n","ypred: [Value(data = 0.9636334554368903), Value(data = -0.9657428286599414)]\n","step: 185, loss: 0.002496079351682776\n","-------\n","ypred: [Value(data = 0.9637307489939451), Value(data = -0.96583475946267)]\n","step: 186, loss: 0.002482722229513838\n","-------\n","ypred: [Value(data = 0.9638272779020338), Value(data = -0.9659259579016007)]\n","step: 187, loss: 0.0024695061689001757\n","-------\n","ypred: [Value(data = 0.9639230520929711), Value(data = -0.9660164336268924)]\n","step: 188, loss: 0.0024564289537218854\n","-------\n","ypred: [Value(data = 0.9640180813190983), Value(data = -0.9661061961118796)]\n","step: 189, loss: 0.002443488413965387\n","-------\n","ypred: [Value(data = 0.9641123751574298), Value(data = -0.966195254657213)]\n","step: 190, loss: 0.0024306824245317435\n","-------\n","ypred: [Value(data = 0.9642059430136837), Value(data = -0.9662836183948798)]\n","step: 191, loss: 0.0024180089040817514\n","-------\n","ypred: [Value(data = 0.9642987941261991), Value(data = -0.9663712962921137)]\n","step: 192, loss: 0.0024054658139163196\n","-------\n","ypred: [Value(data = 0.9643909375697448), Value(data = -0.9664582971551913)]\n","step: 193, loss: 0.0023930511568912615\n","-------\n","ypred: [Value(data = 0.9644823822592228), Value(data = -0.9665446296331248)]\n","step: 194, loss: 0.0023807629763647623\n","-------\n","ypred: [Value(data = 0.9645731369532685), Value(data = -0.9666303022212502)]\n","step: 195, loss: 0.002368599355176966\n","-------\n","ypred: [Value(data = 0.9646632102577554), Value(data = -0.9667153232647172)]\n","step: 196, loss: 0.002356558414659881\n","-------\n","ypred: [Value(data = 0.9647526106291999), Value(data = -0.9667997009618816)]\n","step: 197, loss: 0.0023446383136772747\n","-------\n","ypred: [Value(data = 0.9648413463780781), Value(data = -0.9668834433676053)]\n","step: 198, loss: 0.0023328372476928874\n","-------\n","ypred: [Value(data = 0.9649294256720498), Value(data = -0.9669665583964666)]\n","step: 199, loss: 0.002321153447866333\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":314,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":315,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 0.9197540879249573\n","Epoch 10 loss: 0.05579092353582382\n","Epoch 20 loss: 0.0009356947848573327\n","Epoch 30 loss: 1.3890247828385327e-05\n","\n","Prediction:\n","tensor([[ 0.9999],\n","        [-0.9994]])\n","Loss: 3.168441935486044e-07\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":316,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":317,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[ 0.4240,  0.2327, -0.1169],\n","        [-0.5304,  0.3759,  0.2654],\n","        [ 0.3939, -0.6259, -0.0366],\n","        [ 0.0260,  0.4175,  0.0491]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 1.6630,  0.9810],\n","        [-0.1984, -1.8342],\n","        [-1.0531,  1.7894],\n","        [ 1.2555, -0.3150]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.4321],\n","        [ 0.5416],\n","        [ 0.0254],\n","        [-0.5523]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 1.2310,  0.5489],\n","        [ 0.3432, -1.2926],\n","        [-1.0277,  1.8148],\n","        [ 0.7032, -0.8673]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.8429,  0.4997],\n","        [ 0.3303, -0.8598],\n","        [-0.7730,  0.9483],\n","        [ 0.6064, -0.7000]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[ 0.1330, -0.0794, -0.3083,  0.3800],\n","        [-0.3049, -0.5459,  0.4768, -0.5602],\n","        [ 0.1886, -0.5617,  0.0236, -0.3349],\n","        [ 0.4158, -0.0063,  0.2357,  0.0346]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.8429,  0.4997],\n","        [ 0.3303, -0.8598],\n","        [-0.7730,  0.9483],\n","        [ 0.6064, -0.7000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 0.5547, -0.4237],\n","        [-1.1456,  1.1613],\n","        [-0.2480,  0.8341],\n","        [ 0.1872,  0.4125]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.1031],\n","        [ 0.0656],\n","        [-0.1438],\n","        [-0.1716]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 0.4516, -0.5268],\n","        [-1.0800,  1.2269],\n","        [-0.3918,  0.6902],\n","        [ 0.0156,  0.2409]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.4232, -0.4829],\n","        [-0.7932,  0.8417],\n","        [-0.3729,  0.5981],\n","        [ 0.0156,  0.2363]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[ 0.4287, -0.7230, -0.3609, -0.3547]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.4232, -0.4829],\n","        [-0.7932,  0.8417],\n","        [-0.3729,  0.5981],\n","        [ 0.0156,  0.2363]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 0.8840, -1.1153]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[0.1159]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9999, -0.9994]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9999, -0.9994]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":318,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":318,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":319,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9999, -0.9994]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(4.1621e-07)"]},"execution_count":319,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":320,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9999, -0.9994]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-6.5923e-05,  6.4176e-04]])\n","squared_difference: tensor([[4.3458e-09, 4.1186e-07]])\n"]},{"data":{"text/plain":["tensor(2.0810e-07)"]},"execution_count":320,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference) / 2\n","loss"]},{"cell_type":"code","execution_count":321,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":321,"metadata":{},"output_type":"execute_result"}],"source":["difference\n","len(squared_difference)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":322,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(2.0810e-07)"]},"execution_count":322,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":323,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(4.1621e-07)"]},"execution_count":323,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
