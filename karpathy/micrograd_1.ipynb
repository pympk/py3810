{"cells":[{"cell_type":"markdown","metadata":{},"source":["## [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":233,"metadata":{},"outputs":[],"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions"]},{"cell_type":"code","execution_count":234,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":235,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":236,"metadata":{},"outputs":[],"source":["import random\n","\n","class Neuron:\n","    \n","    def __init__(self, nin):\n","#### my add ##########################################        \n","        # random.seed(12345)  # WARNING: all neurons will have the same weights and bias\n","######################################################        \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["### Neuron in Neural Network\n","<!-- ### Simple Neural Network: Single Neuron with 3 Inputs -->\n","<img src=\"..\\karpathy\\img\\Nertual_Network_1_Neuron_3_Inputs.png\">"]},{"cell_type":"markdown","metadata":{},"source":["#### Activation Function: Tanh"]},{"cell_type":"code","execution_count":237,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXUAAADcCAYAAACPmTFaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsdUlEQVR4nO3deXxU1fn48c+dSTJZSAhLSAiELECBsMoWUZQAIVEoLVopWiyQIqg/qGBoLaFVDFqpimILVtAqCIaK8FXQgpiIIkXZMchOWcISyMKSTBaSTGbO74+QkZgACWQy2/N+veaVuWfOnfuczOThcu6552hKKYUQQgiXoLN3AEIIIRqOJHUhhHAhktSFEMKFSFIXQggXIkldCCFciCR1IYRwIZLUhRDChUhSF0IIFyJJXQghXIgkdeGUli5diqZp7Nq1y96h3FRsbCyxsbF2OfaECROIiIiwy7GFfUhSF7dN07Q6PTZt2mTvUGv1zDPPoGkaY8aMueX3OHjwIM8//zyZmZkNF1gdnTt3jueff56MjIxGP7ZwPB72DkA4v+XLl1fbXrZsGenp6TXKu3Tp0phh1YlSin//+99ERETw2WefUVhYiL+/f73f5+DBg6SkpBAbG1vjzDgtLa2Boq3duXPnSElJISIigl69elV77Z133sFisdj0+MKxSFIXt+3RRx+ttr1t2zbS09NrlDuiTZs2cfbsWb766isSEhL4+OOPGT9+fIMew8vLq0Hfrz48PT3tdmxhH9L9IhrFkiVLGDJkCK1atcJgMBAdHc1bb71Vo15ERAQ///nP2bJlC/3798fb25uoqCiWLVtW6/uWlZWRlJREUFAQfn5+PPDAA+Tl5dU5rtTUVKKjoxk8eDBxcXGkpqbWWi8rK4uJEycSGhqKwWAgMjKSJ598kvLycpYuXcro0aMBGDx4cI3upmv71HNycvDw8CAlJaXGMY4cOYKmaSxcuBCAS5cu8Yc//IHu3bvTpEkTAgICuP/++9m7d691n02bNtGvXz8AEhMTrcdeunQpUHufenFxMTNmzCAsLAyDwUCnTp2YN28eP52wVdM0pk6dypo1a+jWrRsGg4GuXbuyYcOGOv9+hR0oIRrYlClT1E+/Wv369VMTJkxQ8+fPVwsWLFDx8fEKUAsXLqxWLzw8XHXq1EkFBwerWbNmqYULF6revXsrTdPU/v37rfWWLFmiAHXHHXeoIUOGqAULFqgZM2YovV6vfv3rX9cpztLSUhUYGKheeOEFpZRSy5YtU3q9Xp0/f75avaysLBUaGqp8fX3V9OnT1aJFi9Szzz6runTpoi5fvqyOHz+unnrqKQWoWbNmqeXLl6vly5er7OxspZRSgwYNUoMGDbK+35AhQ1R0dHSNeFJSUpRer7fut3PnTtW+fXs1c+ZMtXjxYjVnzhzVpk0b1bRpU5WVlaWUUio7O1vNmTNHAWry5MnWYx8/flwppdT48eNVeHi49RgWi0UNGTJEaZqmHnvsMbVw4UI1cuRIBajp06dXiwdQPXv2VK1bt1YvvPCCeuONN1RUVJTy9fVVFy5cqNPvWDQ+SeqiwdWW1EtKSmrUS0hIUFFRUdXKwsPDFaA2b95sLcvNzVUGg0HNmDHDWlaV1OPi4pTFYrGWP/3000qv16v8/Pybxrl69WoFqP/9739KKaWMRqPy9vZW8+fPr1Zv3LhxSqfTqZ07d9Z4j6pjr1q1SgHq66+/rlHnp0l98eLFClD79u2rVi86OloNGTLEul1aWqrMZnO1OidPnlQGg0HNmTPHWrZz504FqCVLltQ49k+T+po1axSgXnzxxWr1HnroIaVpmjp27Ji1DFBeXl7Vyvbu3asAtWDBghrHEo5Bul9Eo/Dx8bE+Lygo4MKFCwwaNIgTJ05QUFBQrW50dDT33HOPdTsoKIhOnTpx4sSJGu87efJkNE2zbt9zzz2YzWZOnTp105hSU1Pp27cvHTp0AMDf358RI0ZU64KxWCysWbOGkSNH0rdv3xrvce2x6+rBBx/Ew8ODlStXWsv279/PwYMHq43AMRgM6HSVf6Jms5mLFy/SpEkTOnXqxJ49e+p9XID169ej1+t56qmnqpXPmDEDpRSff/55tfK4uDjat29v3e7RowcBAQG1fhbCMUhSF43i22+/JS4uDj8/PwIDAwkKCmLWrFkANZJ6u3btauzfrFkzLl++XKP8p3WbNWsGUGvda+Xn57N+/XoGDRrEsWPHrI+7776bXbt2cfToUQDy8vIwGo1069at7o29iZYtWzJ06FA++ugja9nKlSvx8PDgwQcftJZZLBbmz59Px44dMRgMtGzZkqCgIH744Ycav7O6OnXqFKGhoTVG+FSNTPrpP4b1+SyEY5CkLmzu+PHjDB06lAsXLvD666+zbt060tPTefrppwFqDLnT6/W1vo+qZeXF+tS91qpVqygrK+O1116jY8eO1kdSUhLAdS+YNpSHH36Yo0ePWseWf/TRRwwdOpSWLVta67z00kskJSVx77338sEHH/DFF1+Qnp5O165dG22Y4q3+foX9yJBGYXOfffYZZWVlfPrpp9XO/L7++mu7xZSamkq3bt2YPXt2jdcWL17MihUrSElJISgoiICAAPbv33/D96tvN8yoUaN4/PHHrV0wR48eJTk5uVqd1atXM3jwYN59991q5fn5+dWSf32OHR4ezpdfflljPP7hw4etrwvnJkld2FzV2d61Z3cFBQUsWbLELvGcOXOGzZs3k5KSwkMPPVTj9fLycsaOHcv27duJiYlh1KhRfPDBB+zatatGv7pSCk3T8PPzAyoTbl0EBgaSkJDARx99hFIKLy8vRo0aVa2OXq+vcUa8atUqsrKyrNcBgHode/jw4bz99tssXLiw2j8i8+fPR9M07r///jrFLxyXJHVhc/Hx8Xh5eTFy5Egef/xxioqKeOedd2jVqhXnz59v9HhWrFiBUopf/OIXtb4+fPhwPDw8SE1NJSYmhpdeeom0tDQGDRrE5MmT6dKlC+fPn2fVqlVs2bKFwMBAevXqhV6v5+WXX6agoACDwWAdl389Y8aM4dFHH+Wf//wnCQkJBAYGVnv95z//OXPmzCExMZG77rqLffv2kZqaSlRUVLV67du3JzAwkEWLFuHv74+fnx8xMTFERkbWOObIkSMZPHgwf/7zn8nMzKRnz56kpaWxdu1apk+fXu2iqHBO0qcubK5Tp06sXr0aTdP4wx/+wKJFi5g8eTLTpk2zSzypqam0a9eOnj171vp6YGAgAwcOZOXKlVRUVNCmTRu2b9/OQw89RGpqKk899RTLli0jNjYWX19fAEJCQli0aBG5ublMnDiRRx55hIMHD94wjl/84hf4+PhQWFhY67wzs2bNYsaMGXzxxRdMmzaNPXv2sG7dOsLCwqrV8/T05P3330ev1/PEE0/wyCOP8M0339R6TJ1Ox6effsr06dP5z3/+w/Tp0zl48CCvvvoqr7/+el1+fcLBaUqueAghhMuQM3UhhHAhktSFEMKFSFIXQggXIkldCCFciCR1IYRwIZLUhRDChbjczUcWi4Vz587h7+9/SzPoCSGEo1FKUVhYSGhoqHXmzutxuaR+7ty5GjdnCCGEKzhz5gxt27a9YR2XS+pVkxSdOXOGgIAAO0dzYyaTibS0NOLj4116LUl3aKc7tBHco52O2Eaj0UhYWFidFkV3uaRe1eUSEBDgFEnd19eXgIAAh/ny2II7tNMd2gju0U5HbmNdupRteqF08+bNjBw5ktDQUDRNY82aNTfdZ9OmTfTu3RuDwUCHDh2sC+gKIYS4OZsm9eLiYnr27Mmbb75Zp/onT55kxIgRDB48mIyMDKZPn85jjz3GF198YcswhRDCZdi0++X++++v1/zMixYtIjIyktdeew2oXGJry5YtzJ8/n4SEBFuFKYQQLsOh+tS3bt1KXFxctbKEhASmT59+3X3KysooKyuzbhuNRqCyX8xkMtkkzoZSFZ+jx3m73KGd7tBGaJh2KqUor7BQXG7mislMSbmZK1efl5rMlFcoyirMlJstlFVYKK+wUGFRVJgVJvOPzyssFswWVflQV39awKwUyqKwqKvPVeVzi1IohXVbUfmTq8+VAkXlsOgLF3X8O3snOk1D8eMCL5XPsT6Hui3t99Mqf4zvSJ/wZnX+ndXn9+1QST07O5vg4OBqZcHBwRiNRq5cuVJtRfoqc+fOJSUlpUZ5Wlqada5rR5eenm7vEBqFO7TTHdoI1dupFBRXQEE5GE0axnIwmsBYrlFogisVcMWscaUCSirgihnMytHvIdFBge0W1964ZRs5B+o+63lJSUmd6zpUUr8VycnJ1sWC4cehP/Hx8U4x+iU9PZ1hw4Y53FX2huQO7XSHNl4pN7P/7GX+s3knTVpHcTa/lMyLJZy6VEJxmfmW3tPLQ4evpx5fLz0+Xnq8PXV46XUYPHR4eegweOjx1Gt46nV46DU8dDo89RoeOg29rrJcp1Vu63Qaeo3KnzoNnaahaaC/+lPTNHQaleVUbmsaV5+DRuW22WzmwP79dO/eHQ+PyqUYq+pXPefqPrW5doTK9f7puqNdIK38DXX+PVX1QNSFQyX1kJAQcnJyqpXl5OQQEBBQ61k6gMFgwGCo+cvx9PR0mj8uZ4r1drhDO12ljRVmC0dzivjhbD57z+aTcaaAozmFmC0K0MPxUzX2aeHnRZC/gSB/A638vWkVYKCFnxeBvl4EeHvQ1MeTpr6eBHh70sTbA19PPR56x5upxGQy4ZOzj+G92zrMZ1mfOBwqqQ8YMID169dXK0tPT2fAgAF2ikgI92EsNfHNkTy+PJTD14dzMZZW1KgT1MSLZvpS+nZqR1SQPxEt/Iho6UdYcx8MV89qhX3ZNKkXFRVx7Ngx6/bJkyfJyMigefPmtGvXjuTkZLKysli2bBkATzzxBAsXLuSZZ57hd7/7HV999RUfffQR69ats2WYQritvMIyPt9/nvSDOWw7cRGT+cd+3iYGD3q0bUrPsEB6Xv3ZwkfP559/zvDh0Q5zFiuqs2lS37VrF4MHD7ZuV/V9jx8/nqVLl3L+/HlOnz5tfT0yMpJ169bx9NNP8/e//522bdvyr3/9S4YzCtHADp4z8u6Wk3y29xzlZou1vH2QH3HRwQzrEswd7Zqh11XvFXb10T2uwKZJPTY29obDfWq7WzQ2Npbvv//ehlEJ4Z4sFsXXR3J5d8tJvjt+0VreMyyQEd1DiOsSTFRQEztGKBqCQ/WpCyEanlKKLw5k88qGI5y4UAyAXqdxf7cQJg6M5I52dR8vLRyfJHUhXNi5/Cs8t/YAXx6qHFXm7+3Bb/q3Y9xdEbQJrH1EmXBuktSFcEFmi+L97zJ5Le0IxeVmPPUaTwxqzxOD2uNnkD97VyafrhAuZn9WAckf72NfVgEAfcObMffB7nQMvvlc3ML5SVIXwoWs2H6aZ9fux2xR+Ht7kHx/Fx7uF4ZO5+i35YuGIkldCBeglOLVL47wz03HAbivawhzftmVVgHedo5MNDZJ6kI4ufIKC8+s3suajHMATBvakelxHWXhdTclSV0IJ1ZwxcQTy3ez9cRFPHQaLz3YnV/3lYXX3ZkkdSGcVFb+FRKX7OBoThF+XnreerQP9/4syN5hCTuTpC6EE8ouKOWht77jfEEprfwNLEnsR9fQpvYOSzgASepCOJmS8goeW7aT8wWlRAX5sXxijNxIJKwcbzJjIcR1WSyKpJV72Z9lpLmfF+8n9peELqqRpC6EE5mXdoQNB7Lx0ut4+7d9CGvuHEs2isYjSV0IJ/F/u89ax6H/7Vfd6RvR3M4RCUckSV0IJ7Dj5CVmfvwDAFMHd+DB3m3tHJFwVJLUhXBwpy+W8PjyXZjMiuHdQ0ga9jN7hyQcmCR1IRyYyWzhydTdXC4x0b1NU14b3UvmcRE3JEldCAf29uYTHDhnJNDXk3+N74uPlyzuLG5MkroQDupYbhF/3/g/AGaPjCZYJucSdSBJXQgHZLEokj/+gfIKC7GdghjVq429QxJOQpK6EA4odfspdmZexs9Lz4ujusmMi6LOJKkL4WCy8q/wt88PA/Cn+zvTtpncYCTqTpK6EA5EKcWfP9lHcbmZvuHNeDQm3N4hCScjSV0IB7I24xybjuThpdfxt1/1kOGLot4aJam/+eabRERE4O3tTUxMDDt27Lhu3aVLl6JpWrWHt7dc9Reu70JRGSmfHQDgqaEd6NCqiZ0jEs7I5kl95cqVJCUlMXv2bPbs2UPPnj1JSEggNzf3uvsEBARw/vx56+PUqVO2DlMIu5v3xREul5joHOLP44Pa2zsc4aRsntRff/11Jk2aRGJiItHR0SxatAhfX1/ee++96+6jaRohISHWR3BwsK3DFMKuTuQVsWr3WQD++kA3PPXSMypujU0XySgvL2f37t0kJydby3Q6HXFxcWzduvW6+xUVFREeHo7FYqF379689NJLdO3atda6ZWVllJWVWbeNRiMAJpMJk8nUQC2xjar4HD3O2+UO7bzdNr6WdgSzRTG4U0t6hPo77O9KPkv7qE8sNk3qFy5cwGw21zjTDg4O5vDhw7Xu06lTJ9577z169OhBQUEB8+bN46677uLAgQO0bVtzZrq5c+eSkpJSozwtLQ1fX+cYCpaenm7vEBqFO7TzVtqYVQzr9lX+KfYzZLN+/fqGDqvByWfZuEpKSupc1+GWsxswYAADBgywbt9111106dKFxYsX88ILL9Son5ycTFJSknXbaDQSFhZGfHw8AQEBjRLzrTKZTKSnpzNs2DA8PT3tHY7NuEM7b6eNkz/YA1xgRPcQJo3uYZsAG4h8lvZR1QNRFzZN6i1btkSv15OTk1OtPCcnh5CQkDq9h6enJ3fccQfHjh2r9XWDwYDBYKh1P0f5QG7GmWK9He7Qzvq2cfepy3x95AJ6ncaM+E5O8/uRz7Jx1ScOm16N8fLyok+fPmzcuNFaZrFY2LhxY7Wz8Rsxm83s27eP1q1b2ypMIexCKcWrX1R2Qz7Uuy1RQTKEUdw+m3e/JCUlMX78ePr27Uv//v154403KC4uJjExEYBx48bRpk0b5s6dC8CcOXO488476dChA/n5+bz66qucOnWKxx57zNahCtGovj12kW0nLuGl1/FUXEd7hyNchM2T+pgxY8jLy+O5554jOzubXr16sWHDBuvF09OnT6PT/fgfhsuXLzNp0iSys7Np1qwZffr04bvvviM6OtrWoQrRaK49Sx97ZzvaBPrYOSLhKhrlQunUqVOZOnVqra9t2rSp2vb8+fOZP39+I0QlhP2kHcxh79kCfL30/L/YDvYOR7gQucNBiEZmtiheSzsCwO/ujiTIv+aFfiFulSR1IRrZ5/vPczSniABvDybdG2XvcISLkaQuRCNSSvHO5hMAJN4dSVMfxxgyJ1yHJHUhGtHOzMvsPVuAwUPHbwfIXOmi4UlSF6IRvfPfyrP0B3u3pWUT6UsXDU+SuhCN5EReEV8eqry7euLASDtHI1yVJHUhGsm7W06iFAzt3EoWwBA2I0ldiEZwqbic1VfnS5cRL8KWJKkL0Qg+2HaKsgoL3ds0JSayub3DES5MkroQNlZqMrNsayYAj90TiabJYtLCdiSpC2Fja77P4kJROaFNvRneXWYbFbYlSV0IG7JYFP/achKA3w2MlLVHhc3JN0wIG/rmaB7HcovwN3gwpl+YvcMRbkCSuhA2VHWz0cP9w/D3likBhO1JUhfCRg5nG/nu+EX0Oo0Jd8vNRqJxSFIXwkaWbz0FQELXYFkEQzQaSepC2ICx1MQn32cB8Ns7I+wbjHArktSFsIFP9mRRUm6mY6sm3BklNxuJxiNJXYgGppRi+bbKrpffDgiXm41Eo5KkLkQD23riIsdyi/Dz0vPAHW3sHY5wM5LUhWhgVRdIH+jdRoYxikYnSV2IBpRtLCXtYOWc6XKBVNiDJHUhGtDKnWcxWxT9I5vTKcTf3uEINyRJXYgGUmGBlbsq50wfJ+uPCjtplKT+5ptvEhERgbe3NzExMezYseOG9VetWkXnzp3x9vame/furF+/vjHCFOK27LukkVdUTpC/gfjoEHuHI9yUzZP6ypUrSUpKYvbs2ezZs4eePXuSkJBAbm5urfW/++47HnnkESZOnMj333/PqFGjGDVqFPv377d1qELclv9mV/45PdK/HV4e8p9gYR82/+a9/vrrTJo0icTERKKjo1m0aBG+vr689957tdb/+9//zn333ccf//hHunTpwgsvvEDv3r1ZuHChrUMV4pYdzSnkeKGGXqfxm/7t7B2OcGMetnzz8vJydu/eTXJysrVMp9MRFxfH1q1ba91n69atJCUlVStLSEhgzZo1tdYvKyujrKzMum00GgEwmUyYTKbbbIFtVcXn6HHeLndoZ9XNRkM7taSFr95l2+oOn6UjtrE+sdg0qV+4cAGz2UxwcHC18uDgYA4fPlzrPtnZ2bXWz87OrrX+3LlzSUlJqVGelpaGr6/vLUbeuNLT0+0dQqNw1XaWVsAne/SARkct2y2uAbnqZ3ktR2pjSUlJnevaNKk3huTk5Gpn9kajkbCwMOLj4wkICLBjZDdnMplIT09n2LBheHq67k0qrt7OD7afpsxymGAfxf/71VC8vLzsHZLNuPpnCY7ZxqoeiLqwaVJv2bIler2enJycauU5OTmEhNQ+OiAkJKRe9Q0GAwaDoUa5p6enw3wgN+NMsd4OV2ynUooVOyqHMQ4MtuDl5eVybayNK36WP+VIbaxPHDa9UOrl5UWfPn3YuHGjtcxisbBx40YGDBhQ6z4DBgyoVh8q/xt0vfpC2NO2E5f4X24Rvl56+gUpe4cjhO27X5KSkhg/fjx9+/alf//+vPHGGxQXF5OYmAjAuHHjaNOmDXPnzgVg2rRpDBo0iNdee40RI0bw4YcfsmvXLt5++21bhypEvS3flgnAL3u2xscj066xCAGNkNTHjBlDXl4ezz33HNnZ2fTq1YsNGzZYL4aePn0ane7H/zDcddddrFixgr/85S/MmjWLjh07smbNGrp162brUIWolxxjKV8cqOwqHNs/jON7Mu0bkBA00oXSqVOnMnXq1Fpf27RpU42y0aNHM3r0aBtHJcTtWbH9dOU8LxGV87wct3dAQiBzvwhxS0xmC//ecRqAR2WeF+FAJKkLcQvSDuSQW1hGyyYG7usq87wIxyFJXYhbUHWB9Df9w2SeF+FQ5NsoRD0dzSlk24lL6HUaj8TIPC/CsUhSF6KeqparG9YlmNZNfewcjRDVSVIXoh6Kyir4eE/lHaS/lQukwgFJUheiHj7Zc5bicjNRQX7c1b6FvcMRogZJ6kLUkcWieO/bTADG3RmOpmn2DUiIWkhSF6KOvjyUw8kLxQR4ezC6b5i9wxGiVpLUhaijf/33JABj7wzHz+D0s1YLFyVJXYg62Hsmnx2Zl/DUa0y4K8Le4QhxXZLUhaiDd/57AoCRPUMJDvC2czRCXJ8kdSFu4sylEj7fX7mc4mMDo+wcjRA3JkldiJtY8m0mZotiYIeWRIc69hKJQkhSF+IGCq6YWLmzcjbGx+6JtHM0QtycJHUhbuDDHacpLjfTKdifQT8Lsnc4QtyUJHUhrsNktrD0u0wAJt4TKTcbCacgSV2I61j3w3nOF5QS5G/gl71C7R2OEHUiSV2IWiileHtz5TDG8QPCMXjo7RyREHUjSV2IWmw5doGD5414e+oYGyOzMQrnIUldiJ9QSjEv7SgAD/drRzM/LztHJETdSVIX4ifSD+aw90w+Pp56pgzuYO9whKgXSepCXMNsUbx29Sz9dwMjCPI32DkiIepHkroQ1/hs7zmO5BQS4O3B5Hva2zscIerNpkn90qVLjB07loCAAAIDA5k4cSJFRUU33Cc2NhZN06o9nnjiCVuGKQRQOS799fTKs/THB7Wnqa+nnSMSov5sOin02LFjOX/+POnp6ZhMJhITE5k8eTIrVqy44X6TJk1izpw51m1fX19bhikEAKt2neX0pRJaNvEi8e4Ie4cjxC2xWVI/dOgQGzZsYOfOnfTt2xeABQsWMHz4cObNm0do6PVv5vD19SUkJMRWoQlRQ6nJzD82/g+AKYM74Osli2AI52Szb+7WrVsJDAy0JnSAuLg4dDod27dv54EHHrjuvqmpqXzwwQeEhIQwcuRInn322euerZeVlVFWVmbdNhqNAJhMJkwmUwO1xjaq4nP0OG+XM7Tz/W8zyTaW0rqpN6N7h9Y7VmdoY0Nwh3Y6YhvrE4vNknp2djatWrWqfjAPD5o3b052dvZ19/vNb35DeHg4oaGh/PDDD/zpT3/iyJEjfPzxx7XWnzt3LikpKTXK09LSnKbbJj093d4hNApHbWdpBfzjez2gEduymI1pG275vRy1jQ3NHdrpSG0sKSmpc916J/WZM2fy8ssv37DOoUOH6vu2VpMnT7Y+7969O61bt2bo0KEcP36c9u1rjkZITk4mKSnJum00GgkLCyM+Pp6AAMee+9pkMpGens6wYcPw9HTdi3KO3s4FXx+nuOI4kS18ee63d+Ghr//4AUdvY0Nxh3Y6YhureiDqot5JfcaMGUyYMOGGdaKioggJCSE3N7daeUVFBZcuXapXf3lMTAwAx44dqzWpGwwGDIaaY4k9PT0d5gO5GWeK9XY4YjtzjKW89+0pAGYkdMLH+/bGpTtiG23BHdrpSG2sTxz1TupBQUEEBd18XukBAwaQn5/P7t276dOnDwBfffUVFovFmqjrIiMjA4DWrVvXN1QhbkgpxbNr9lNUVkHPtk0Z3k2+Y8L52WycepcuXbjvvvuYNGkSO3bs4Ntvv2Xq1Kk8/PDD1pEvWVlZdO7cmR07dgBw/PhxXnjhBXbv3k1mZiaffvop48aN495776VHjx62ClW4qc/3Z5N2MAcPncbfftUDnU7mSxfOz6Y3H6WmptK5c2eGDh3K8OHDGThwIG+//bb1dZPJxJEjR6wXAby8vPjyyy+Jj4+nc+fOzJgxg1/96ld89tlntgxTuKH8knKeW3sAgCdj29OltWNffxGirmw6GLd58+Y3vNEoIiICpZR1OywsjG+++caWIQkBwIvrDnGhqIz2QX5MHSKTdgnXIXO/CLez+Wgeq3efRdPglYd6yAIYwqVIUhdupbisglmf7ANg/IAI+oQ3t3NEQjQsSerCrcxLO8LZy1doE+jDHxM62TscIRqcJHXhNvacvszS7zIBeOnB7vgZZH4X4XokqQu3cLm4nKSVGSgFD/Zuw6Cf3fxeCyGckSR14fLKKyw8mbqbzIsltAn04dkR0fYOSQibkaQuXFrVXaPbTlyiicGD9yb0k4WkhUuTpC5c2r/+e5KVu86g02DBI3fQKcTf3iEJYVOS1IXLSj+Yw0ufV84Y+pcR0Qzu3Oomewjh/CSpC5d04FwB0z78HqVgbEw7WZ5OuA1J6sLl5BpLmfT+LkrKzQzs0JLnf9EVTZPJuoR7kKQuXMqJvCIeWrSVcwWlRAX58ebY3njewqIXQjgruftCuIxdmZd4bNku8ktMtGvuy9IJ/Wnq4xiLHAjRWCSpC5fw+b7zTFuZQXmFhZ5hgbw7vi8tm9zeKkZCOCNJ6sLp/eu/J/jr+kMoBXFdgvnHI73w9ZKvtnBP8s0XTstktvDS+kMs+TYTgHEDwpk9sit6WcFIuDFJ6sIpZZzJZ+b//cDh7EIAku/vzOR7o2SUi3B7ktSFUyksNTHviyMs23YKpSDQ15OXHujO8O6yaLQQIEldOJEN+7N5/tMDZBtLAXjwjjb8eUQXWsgFUSGsJKkLh6aUYmfmZd7adIyvj+QBEN7Cl7+O6s7Aji3tHJ0QjkeSunBIJrOFdT+c590tJ9mXVQCAh07j8UFR/H5IR7w9ZV1RIWojSV04lAtFZXy06wzLvjtl7WYxeOh4sHcbJg6MokOrJnaOUAjHJkld2N2x3CK+PJTDlwdz2H36MkpVlgf5Gxh3Zzi/iWkn/eZC1JEkddHoco2l7D1bwI6TF9l4KJcTF4qrvd6zbVN+OyCCkT1bY/CQbhYh6sNmSf2vf/0r69atIyMjAy8vL/Lz82+6j1KK2bNn884775Cfn8/dd9/NW2+9RceOHW0VprAhi0Vx3ljK0fMFpGdprPt3BvuyjJwvKK1Wz0uvY0D7FsRFBxPXpRWtm/rYKWIhnJ/Nknp5eTmjR49mwIABvPvuu3Xa55VXXuEf//gH77//PpGRkTz77LMkJCRw8OBBvL29bRWquEWlJjO5xjLyikrJNZaRW1jG2cslnLxQwqmLxZy6VEJ5heVqbT2QC4BOg46t/OkZ1pTYTq24p2NL/L1l4i0hGoLNknpKSgoAS5curVN9pRRvvPEGf/nLX/jlL38JwLJlywgODmbNmjU8/PDDtgrVZSmlqLAoKswKk8VChVlRYbZQbrZQVmGhvKLyZ5nJTFmFhSsmM1fKzRSXV3Cl3EzJ1efGKxUYr5gouPowlpq4VFxOYWnFTWPw0GmENfOhKUUk9O1M7/DmdGvTFD+D9PwJYQsO85d18uRJsrOziYuLs5Y1bdqUmJgYtm7det2kXlZWRllZmXXbaDQCYDKZMJlMdT7+4s0nreOgf0pd+/zqVTxVa02sF/kU1ifWH9e+plTlexmNev554ju0a+pYlLI+V1efW9SP+5jVj9tmi8KiKh8VFoXFUvm62aIwW64XZcPx8tDRqokXQf4GgvwNtG7qTUQLX8Jb+BLe3JfQpt4oi5n09HSGxbTB09MTUPX6bJxBVXtcrV0/5Q7tdMQ21icWh0nq2dnZAAQHB1crDw4Otr5Wm7lz51r/V3CttLQ0fH1963z8b4/r2J1rj8UUNCgpasSjKTw08NCBp67yZ9W2lw689AqDDrz0WH/6eih89ODjUfnw1St8PSDAC3z0oGnlwDVtuAiFF2E/lY8q6enpjdZOe3GHNoJ7tNOR2lhSUlLnuvVK6jNnzuTll1++YZ1Dhw7RuXPn+rztbUlOTiYpKcm6bTQaCQsLIz4+noCAgDq/T/g5I1n5V677ukbNiaKunTuq2qta1etatdc07drnGuaKCvZ8/z19evfGw8MDTavsb9bQrO+t07Sr5ZURVD3X634s12mg1+nQ60Cv09BffV2v0/DQ6fDUa3jodXjqNHR2mMHQZDJVnqkPG3b1TN31uEMbwT3a6YhtrOqBqIt6JfUZM2YwYcKEG9aJioqqz1tahYSEAJCTk0Pr1j9OzpSTk0OvXr2uu5/BYMBgqDmG2dPTs14fSK/wFvQKr3u8DcFkMnHlpCK2c7DDfHlsqb6fiTNyhzaCe7TTkdpYnzjqldSDgoIICgqqd0B1ERkZSUhICBs3brQmcaPRyPbt23nyySdtckwhhHA1NutEPn36NBkZGZw+fRqz2UxGRgYZGRkUFf3Y99q5c2c++eQToLI7Yvr06bz44ot8+umn7Nu3j3HjxhEaGsqoUaNsFaYQQrgUm10ofe6553j//fet23fccQcAX3/9NbGxsQAcOXKEgoICa51nnnmG4uJiJk+eTH5+PgMHDmTDhg0yRl0IIerIZkl96dKlNx2jXjU8sIqmacyZM4c5c+bc8nGr3rM+FxbsxWQyUVJSgtFodJi+O1twh3a6QxvBPdrpiG2symc/zZm1cZghjQ2lsLByebOwsDA7RyKEEA2rsLCQpk2b3rCOpuqS+p2IxWLh3Llz+Pv7O/x6lVXDL8+cOVOv4ZfOxh3a6Q5tBPdopyO2USlFYWEhoaGh6HQ3vhTqcmfqOp2Otm3b2juMegkICHCYL48tuUM73aGN4B7tdLQ23uwMvYo9bqEUQghhI5LUhRDChUhStyODwcDs2bNrvSPWlbhDO92hjeAe7XT2NrrchVIhhHBncqYuhBAuRJK6EEK4EEnqQgjhQiSpCyGEC5Gk7mDKysro1asXmqaRkZFh73AaVGZmJhMnTiQyMhIfHx/at2/P7NmzKS8vt3dot+3NN98kIiICb29vYmJi2LFjh71DajBz586lX79++Pv706pVK0aNGsWRI0fsHZbN/e1vf7POHutMJKk7mGeeeYbQ0FB7h2EThw8fxmKxsHjxYg4cOMD8+fNZtGgRs2bNsndot2XlypUkJSUxe/Zs9uzZQ8+ePUlISCA3N9feoTWIb775hilTprBt2zbS09MxmUzEx8dTXFxs79BsZufOnSxevJgePXrYO5T6U8JhrF+/XnXu3FkdOHBAAer777+3d0g298orr6jIyEh7h3Fb+vfvr6ZMmWLdNpvNKjQ0VM2dO9eOUdlObm6uAtQ333xj71BsorCwUHXs2FGlp6erQYMGqWnTptk7pHqRM3UHkZOTw6RJk1i+fHm9Fsx2dgUFBTRv3tzeYdyy8vJydu/eTVxcnLVMp9MRFxfH1q1b7RiZ7VStgeDMn9uNTJkyhREjRlT7TJ2Jy03o5YyUUkyYMIEnnniCvn37kpmZae+QGsWxY8dYsGAB8+bNs3cot+zChQuYzWaCg4OrlQcHB3P48GE7RWU7FouF6dOnc/fdd9OtWzd7h9PgPvzwQ/bs2cPOnTvtHcotkzN1G5o5cyaapt3wcfjwYRYsWEBhYSHJycn2DvmW1LWd18rKyuK+++5j9OjRTJo0yU6Ri/qaMmUK+/fv58MPP7R3KA3uzJkzTJs2jdTUVKdebU2mCbChvLw8Ll68eMM6UVFR/PrXv+azzz6rNv+72WxGr9czduzYassCOqK6ttPLywuAc+fOERsby5133snSpUtvOj+0IysvL8fX15fVq1dXW0t3/Pjx5Ofns3btWvsF18CmTp3K2rVr2bx5M5GRkfYOp8GtWbOGBx54AL1eby0zm81omoZOp6OsrKzaa45KkroDOH36dLXl986dO0dCQgKrV68mJibG6eaHv5GsrCwGDx5Mnz59+OCDD5zij+RmYmJi6N+/PwsWLAAquyjatWvH1KlTmTlzpp2ju31KKX7/+9/zySefsGnTJjp27GjvkGyisLCQU6dOVStLTEykc+fO/OlPf3Ka7ibpU3cA7dq1q7bdpEkTANq3b+9yCT02Npbw8HDmzZtHXl6e9bWQkBA7RnZ7kpKSGD9+PH379qV///688cYbFBcXk5iYaO/QGsSUKVNYsWIFa9euxd/fn+zsbKBy0QYfHx87R9dw/P39ayRuPz8/WrRo4TQJHSSpi0aUnp7OsWPHOHbsWI1/rJz5P4xjxowhLy+P5557juzsbHr16sWGDRtqXDx1Vm+99RYAsbGx1cqXLFnChAkTGj8gcUPS/SKEEC7Eea9QCSGEqEGSuhBCuBBJ6kII4UIkqQshhAuRpC6EEC5EkroQQrgQSepCCOFCJKkLIYQLkaQuhBAuRJK6EEK4EEnqQgjhQiSpCyGEC/n/JdIvaw2X5VkAAAAASUVORK5CYII=","text/plain":["<Figure size 400x200 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["_num = np.arange(-5, 5, 0.2)\n","plt.figure(figsize=(4, 2))\n","plt.plot(_num, np.tanh(_num))\n","plt.title(\"Tanh Activation\")\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Output with Forward Pass"]},{"cell_type":"code","execution_count":238,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["neuron output: -0.7071067801767762\n"]}],"source":["# inputs\n","x0 = Value(-3.0, label='x0')\n","x1 = Value(0.0, label='x1')\n","x2 = Value(0.5, label='x2')\n","\n","# weights\n","w0 = Value(2.0, label='w0')\n","w1 = Value(1.0, label='w1')\n","w2 = Value(1.0, label='w2')\n","\n","# bias\n","b = Value(4.618626415, label='b')\n","\n","# forward pass\n","# x0*w0 + x1*w1 + x2*w2 + b\n","x0w0 = x0*w0; x0w0.label = 'x0*w0'\n","x1w1 = x1*w1; x1w1.label = 'x1*w1'\n","x2w2 = x2*w2; x2w2.label = 'x2*w2'\n","n_sum = x0w0 + x1w1 + x2w2; n_sum.label = 'x0w0 + x1w1 + x2w2'\n","n = n_sum + b; n.label = 'n'\n","out_0 = n.tanh(); out_0.label = 'out_0'\n","print(f'neuron output: {out_0.data}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Gradient with Backward Pass"]},{"cell_type":"code","execution_count":239,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w0.grad(i.e. d(output)/d(w0)): -1.500000004284097\n"]},{"data":{"image/svg+xml":["<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n","<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n"," \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n","<!-- Generated by graphviz version 2.46.0 (20210118.1747)\n"," -->\n","<!-- Pages: 1 -->\n","<svg width=\"1995pt\" height=\"265pt\"\n"," viewBox=\"0.00 0.00 1995.00 265.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n","<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 261)\">\n","<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-261 1991,-261 1991,4 -4,4\"/>\n","<!-- 2158520789504 -->\n","<g id=\"node1\" class=\"node\">\n","<title>2158520789504</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1451,-54.5 1451,-90.5 1643,-90.5 1643,-54.5 1451,-54.5\"/>\n","<text text-anchor=\"middle\" x=\"1462.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">n</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1474,-54.5 1474,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1517\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;0.8814</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1560,-54.5 1560,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1601.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515468560tanh -->\n","<g id=\"node10\" class=\"node\">\n","<title>2158515468560tanh</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"1706\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"1706\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">tanh</text>\n","</g>\n","<!-- 2158520789504&#45;&gt;2158515468560tanh -->\n","<g id=\"edge17\" class=\"edge\">\n","<title>2158520789504&#45;&gt;2158515468560tanh</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1643.4,-72.5C1652.32,-72.5 1660.93,-72.5 1668.75,-72.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1668.86,-76 1678.86,-72.5 1668.86,-69 1668.86,-76\"/>\n","</g>\n","<!-- 2158520789504+ -->\n","<g id=\"node2\" class=\"node\">\n","<title>2158520789504+</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"1388\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"1388\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n","</g>\n","<!-- 2158520789504+&#45;&gt;2158520789504 -->\n","<g id=\"edge1\" class=\"edge\">\n","<title>2158520789504+&#45;&gt;2158520789504</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1415.28,-72.5C1422.78,-72.5 1431.44,-72.5 1440.67,-72.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1440.87,-76 1450.87,-72.5 1440.87,-69 1440.87,-76\"/>\n","</g>\n","<!-- 2158520788496 -->\n","<g id=\"node3\" class=\"node\">\n","<title>2158520788496</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1018,-82.5 1018,-118.5 1325,-118.5 1325,-82.5 1018,-82.5\"/>\n","<text text-anchor=\"middle\" x=\"1087\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x0w0 + x1w1 + x2w2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1156,-82.5 1156,-118.5 \"/>\n","<text text-anchor=\"middle\" x=\"1199\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;5.5000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1242,-82.5 1242,-118.5 \"/>\n","<text text-anchor=\"middle\" x=\"1283.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158520788496&#45;&gt;2158520789504+ -->\n","<g id=\"edge12\" class=\"edge\">\n","<title>2158520788496&#45;&gt;2158520789504+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1310.47,-82.49C1325.29,-80.56 1339.25,-78.73 1351.09,-77.19\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1351.87,-80.62 1361.34,-75.85 1350.97,-73.67 1351.87,-80.62\"/>\n","</g>\n","<!-- 2158520788496+ -->\n","<g id=\"node4\" class=\"node\">\n","<title>2158520788496+</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"955\" cy=\"-100.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"955\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n","</g>\n","<!-- 2158520788496+&#45;&gt;2158520788496 -->\n","<g id=\"edge2\" class=\"edge\">\n","<title>2158520788496+&#45;&gt;2158520788496</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M982.3,-100.5C989.69,-100.5 998.31,-100.5 1007.71,-100.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1007.77,-104 1017.77,-100.5 1007.77,-97 1007.77,-104\"/>\n","</g>\n","<!-- 2158515731520 -->\n","<g id=\"node5\" class=\"node\">\n","<title>2158515731520</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"0,-110.5 0,-146.5 201,-146.5 201,-110.5 0,-110.5\"/>\n","<text text-anchor=\"middle\" x=\"16.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"33,-110.5 33,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"73.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 2.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"114,-110.5 114,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"157.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad &#45;1.5000</text>\n","</g>\n","<!-- 2158515731424* -->\n","<g id=\"node21\" class=\"node\">\n","<title>2158515731424*</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"264\" cy=\"-128.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"264\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n","</g>\n","<!-- 2158515731520&#45;&gt;2158515731424* -->\n","<g id=\"edge11\" class=\"edge\">\n","<title>2158515731520&#45;&gt;2158515731424*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M201.02,-128.5C210.04,-128.5 218.72,-128.5 226.59,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"226.77,-132 236.77,-128.5 226.77,-125 226.77,-132\"/>\n","</g>\n","<!-- 2158515729024 -->\n","<g id=\"node6\" class=\"node\">\n","<title>2158515729024</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"689,-110.5 689,-146.5 878,-146.5 878,-110.5 689,-110.5\"/>\n","<text text-anchor=\"middle\" x=\"699\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"709,-110.5 709,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"752\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"795,-110.5 795,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"836.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515729024&#45;&gt;2158520788496+ -->\n","<g id=\"edge19\" class=\"edge\">\n","<title>2158515729024&#45;&gt;2158520788496+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M878.17,-113.03C892.5,-110.66 906.49,-108.35 918.47,-106.37\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"919.14,-109.81 928.44,-104.72 918,-102.9 919.14,-109.81\"/>\n","</g>\n","<!-- 2158515729024+ -->\n","<g id=\"node7\" class=\"node\">\n","<title>2158515729024+</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"612\" cy=\"-128.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"612\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n","</g>\n","<!-- 2158515729024+&#45;&gt;2158515729024 -->\n","<g id=\"edge3\" class=\"edge\">\n","<title>2158515729024+&#45;&gt;2158515729024</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M639.11,-128.5C650.28,-128.5 664.11,-128.5 678.74,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"678.97,-132 688.97,-128.5 678.97,-125 678.97,-132\"/>\n","</g>\n","<!-- 2158515730896 -->\n","<g id=\"node8\" class=\"node\">\n","<title>2158515730896</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"341,-55.5 341,-91.5 535,-91.5 535,-55.5 341,-55.5\"/>\n","<text text-anchor=\"middle\" x=\"356\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"371,-55.5 371,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"411.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.5000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"452,-55.5 452,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"493.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515732336* -->\n","<g id=\"node15\" class=\"node\">\n","<title>2158515732336*</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"612\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"612\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n","</g>\n","<!-- 2158515730896&#45;&gt;2158515732336* -->\n","<g id=\"edge15\" class=\"edge\">\n","<title>2158515730896&#45;&gt;2158515732336*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M535.06,-73.5C549.08,-73.5 562.74,-73.5 574.52,-73.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"574.77,-77 584.77,-73.5 574.77,-70 574.77,-77\"/>\n","</g>\n","<!-- 2158515468560 -->\n","<g id=\"node9\" class=\"node\">\n","<title>2158515468560</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1769,-54.5 1769,-90.5 1987,-90.5 1987,-54.5 1769,-54.5\"/>\n","<text text-anchor=\"middle\" x=\"1793.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">out_0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1818,-54.5 1818,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1861\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;0.7071</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1904,-54.5 1904,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1945.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 1.0000</text>\n","</g>\n","<!-- 2158515468560tanh&#45;&gt;2158515468560 -->\n","<g id=\"edge4\" class=\"edge\">\n","<title>2158515468560tanh&#45;&gt;2158515468560</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1733.19,-72.5C1740.65,-72.5 1749.28,-72.5 1758.56,-72.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1758.84,-76 1768.84,-72.5 1758.84,-69 1758.84,-76\"/>\n","</g>\n","<!-- 2158515732240 -->\n","<g id=\"node11\" class=\"node\">\n","<title>2158515732240</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"329.5,-165.5 329.5,-201.5 546.5,-201.5 546.5,-165.5 329.5,-165.5\"/>\n","<text text-anchor=\"middle\" x=\"356\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x1*w1</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"382.5,-165.5 382.5,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"423\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"463.5,-165.5 463.5,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"505\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515732240&#45;&gt;2158515729024+ -->\n","<g id=\"edge20\" class=\"edge\">\n","<title>2158515732240&#45;&gt;2158515729024+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M520,-165.42C529.85,-162.69 539.71,-159.71 549,-156.5 559.58,-152.85 570.81,-148.07 580.78,-143.49\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"582.32,-146.63 589.87,-139.21 579.33,-140.3 582.32,-146.63\"/>\n","</g>\n","<!-- 2158515732240* -->\n","<g id=\"node12\" class=\"node\">\n","<title>2158515732240*</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"264\" cy=\"-183.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"264\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n","</g>\n","<!-- 2158515732240*&#45;&gt;2158515732240 -->\n","<g id=\"edge5\" class=\"edge\">\n","<title>2158515732240*&#45;&gt;2158515732240</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M291.12,-183.5C299.31,-183.5 308.94,-183.5 319.28,-183.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"319.39,-187 329.39,-183.5 319.39,-180 319.39,-187\"/>\n","</g>\n","<!-- 2158515730224 -->\n","<g id=\"node13\" class=\"node\">\n","<title>2158515730224</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1077.5,-27.5 1077.5,-63.5 1265.5,-63.5 1265.5,-27.5 1077.5,-27.5\"/>\n","<text text-anchor=\"middle\" x=\"1089.5\" y=\"-41.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1101.5,-27.5 1101.5,-63.5 \"/>\n","<text text-anchor=\"middle\" x=\"1142\" y=\"-41.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 4.6186</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1182.5,-27.5 1182.5,-63.5 \"/>\n","<text text-anchor=\"middle\" x=\"1224\" y=\"-41.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515730224&#45;&gt;2158520789504+ -->\n","<g id=\"edge8\" class=\"edge\">\n","<title>2158515730224&#45;&gt;2158520789504+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1265.65,-57.22C1295.85,-61.03 1327.56,-65.02 1350.99,-67.97\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1350.78,-71.47 1361.14,-69.25 1351.66,-64.52 1350.78,-71.47\"/>\n","</g>\n","<!-- 2158515732336 -->\n","<g id=\"node14\" class=\"node\">\n","<title>2158515732336</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"675,-55.5 675,-91.5 892,-91.5 892,-55.5 675,-55.5\"/>\n","<text text-anchor=\"middle\" x=\"701.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x2*w2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"728,-55.5 728,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"768.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.5000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"809,-55.5 809,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"850.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515732336&#45;&gt;2158520788496+ -->\n","<g id=\"edge9\" class=\"edge\">\n","<title>2158515732336&#45;&gt;2158520788496+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M892.33,-90.68C901.68,-92.17 910.63,-93.59 918.66,-94.87\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"918.23,-98.35 928.66,-96.46 919.33,-91.43 918.23,-98.35\"/>\n","</g>\n","<!-- 2158515732336*&#45;&gt;2158515732336 -->\n","<g id=\"edge6\" class=\"edge\">\n","<title>2158515732336*&#45;&gt;2158515732336</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M639.11,-73.5C646.55,-73.5 655.16,-73.5 664.4,-73.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"664.66,-77 674.66,-73.5 664.66,-70 664.66,-77\"/>\n","</g>\n","<!-- 2158515731856 -->\n","<g id=\"node16\" class=\"node\">\n","<title>2158515731856</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"339.5,-0.5 339.5,-36.5 536.5,-36.5 536.5,-0.5 339.5,-0.5\"/>\n","<text text-anchor=\"middle\" x=\"356\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"372.5,-0.5 372.5,-36.5 \"/>\n","<text text-anchor=\"middle\" x=\"413\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 1.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"453.5,-0.5 453.5,-36.5 \"/>\n","<text text-anchor=\"middle\" x=\"495\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.2500</text>\n","</g>\n","<!-- 2158515731856&#45;&gt;2158515732336* -->\n","<g id=\"edge14\" class=\"edge\">\n","<title>2158515731856&#45;&gt;2158515732336*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M520,-36.58C529.85,-39.31 539.71,-42.29 549,-45.5 559.58,-49.15 570.81,-53.93 580.78,-58.51\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"579.33,-61.7 589.87,-62.79 582.32,-55.37 579.33,-61.7\"/>\n","</g>\n","<!-- 2158515732384 -->\n","<g id=\"node17\" class=\"node\">\n","<title>2158515732384</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"3.5,-220.5 3.5,-256.5 197.5,-256.5 197.5,-220.5 3.5,-220.5\"/>\n","<text text-anchor=\"middle\" x=\"18.5\" y=\"-234.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x1</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"33.5,-220.5 33.5,-256.5 \"/>\n","<text text-anchor=\"middle\" x=\"74\" y=\"-234.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"114.5,-220.5 114.5,-256.5 \"/>\n","<text text-anchor=\"middle\" x=\"156\" y=\"-234.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515732384&#45;&gt;2158515732240* -->\n","<g id=\"edge10\" class=\"edge\">\n","<title>2158515732384&#45;&gt;2158515732240*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M173.4,-220.46C182.75,-217.68 192.16,-214.67 201,-211.5 211.53,-207.72 222.76,-202.9 232.72,-198.34\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"234.25,-201.48 241.83,-194.07 231.29,-195.14 234.25,-201.48\"/>\n","</g>\n","<!-- 2158515731376 -->\n","<g id=\"node18\" class=\"node\">\n","<title>2158515731376</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1,-55.5 1,-91.5 200,-91.5 200,-55.5 1,-55.5\"/>\n","<text text-anchor=\"middle\" x=\"16\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"31,-55.5 31,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"74\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"117,-55.5 117,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"158.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 1.0000</text>\n","</g>\n","<!-- 2158515731376&#45;&gt;2158515731424* -->\n","<g id=\"edge13\" class=\"edge\">\n","<title>2158515731376&#45;&gt;2158515731424*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M170.4,-91.57C180.73,-94.66 191.2,-98 201,-101.5 211.28,-105.17 222.28,-109.73 232.11,-114.06\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"230.93,-117.37 241.48,-118.27 233.79,-110.98 230.93,-117.37\"/>\n","</g>\n","<!-- 2158515729360 -->\n","<g id=\"node19\" class=\"node\">\n","<title>2158515729360</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"2,-165.5 2,-201.5 199,-201.5 199,-165.5 2,-165.5\"/>\n","<text text-anchor=\"middle\" x=\"18.5\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w1</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"35,-165.5 35,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"75.5\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 1.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"116,-165.5 116,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"157.5\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.0000</text>\n","</g>\n","<!-- 2158515729360&#45;&gt;2158515732240* -->\n","<g id=\"edge16\" class=\"edge\">\n","<title>2158515729360&#45;&gt;2158515732240*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M199.17,-183.5C208.89,-183.5 218.26,-183.5 226.69,-183.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"226.83,-187 236.83,-183.5 226.83,-180 226.83,-187\"/>\n","</g>\n","<!-- 2158515731424 -->\n","<g id=\"node20\" class=\"node\">\n","<title>2158515731424</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"327,-110.5 327,-146.5 549,-146.5 549,-110.5 327,-110.5\"/>\n","<text text-anchor=\"middle\" x=\"353.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x0*w0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"380,-110.5 380,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"423\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"466,-110.5 466,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"507.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2158515731424&#45;&gt;2158515729024+ -->\n","<g id=\"edge18\" class=\"edge\">\n","<title>2158515731424&#45;&gt;2158515729024+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M549.39,-128.5C558.39,-128.5 566.99,-128.5 574.77,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"574.82,-132 584.82,-128.5 574.82,-125 574.82,-132\"/>\n","</g>\n","<!-- 2158515731424*&#45;&gt;2158515731424 -->\n","<g id=\"edge7\" class=\"edge\">\n","<title>2158515731424*&#45;&gt;2158515731424</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M291.12,-128.5C298.66,-128.5 307.41,-128.5 316.81,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"316.82,-132 326.81,-128.5 316.81,-125 316.82,-132\"/>\n","</g>\n","</g>\n","</svg>\n"],"text/plain":["<graphviz.graphs.Digraph at 0x1f6913752b0>"]},"execution_count":239,"metadata":{},"output_type":"execute_result"}],"source":["# backward pass to calculate gradient\n","out_0.backward()\n","  \n","out_0_grad = w0.grad  # store w0.grad, further calculation with w0 will reset w0.grad to zero\n","print(f'w0.grad(i.e. d(output)/d(w0)): {w0.grad}')\n","draw_dot(out_0)"]},{"cell_type":"markdown","metadata":{},"source":["##### Check Backward Pass Gradient Calculation: d(output) / d(x0)"]},{"cell_type":"code","execution_count":240,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- w0.grad from backward pass is same as d(out)/d(w0) calculation ----\n","out_1: -0.7071082802, out_0: -0.7071067802, d(out): -0.0000015000, d(w0): 0.0000010000, d(out)/d(w0): -1.4999968224\n","d(out) / d(w0):             -1.4999968224\n","w0.grad from backward pass: -1.5000000043\n"]}],"source":["h = 0.000001\n","w0 += h  # increment x0 by h\n","\n","# x0*w0 + x1*w1 + x2*w2 + b\n","x0w0 = x0*w0; x0w0.label = 'x0*w0'\n","x1w1 = x1*w1; x1w1.label = 'x1*w1'\n","x2w2 = x2*w2; x2w2.label = 'x2*w2'\n","n_sum = x0w0 + x1w1 + x2w2; n_sum.label = 'x0w0 + x1w1 + x2w2'\n","n = n_sum + b; n.label = 'n'\n","out_1 = n.tanh(); out_1.label = 'out_1'\n","out_grad = (out_1 - out_0) / h \n","\n","print(f'---- w0.grad from backward pass is same as d(out)/d(w0) calculation ----')\n","print(f'out_1: {out_1.data:<12.10f}, out_0: {out_0.data:<12.10f}, d(out): {out_1.data-out_0.data:<12.10f}, d(w0): {h:<12.10f}, d(out)/d(w0): {(out_1.data-out_0.data)/h:<12.10f}')\n","print(f'd(out) / d(w0):             {out_grad.data:<12.10f}')\n","print(f'w0.grad from backward pass: {out_0_grad:<12.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":241,"metadata":{},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":242,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- torch results matched backward pass results ----\n","x0.data.item()  = -3.000000\n","x0.grad.item()  =  1.000000\n","w0.data.item()  =  2.000000\n","w0.grad.item()  = -1.500000 <--\n","---\n","x1.data.item()  =  0.000000\n","x1.grad.item()  =  0.500000\n","w1.data.item()  =  1.000000\n","w1.grad.item()  =  0.000000\n","---\n","x2.data.item()  =  0.500000\n","x2.grad.item()  =  0.500000\n","w2.data.item()  =  1.000000\n","w2.grad.item()  =  0.250000\n","---\n","out.data.item() = -0.707107 <--\n"]}],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <--')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <--')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":243,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":244,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["len(n.parameters()): 41\n","i:  0,  -0.0952690735\n","i:  1,   0.3679387872\n","i:  2,   0.9316894304\n","i:  3,   0.4807952497\n","i:  4,  -0.1278204536\n","i:  5,   0.7633232397\n","i:  6,   0.1105173324\n","i:  7,   0.5484646896\n","i:  8,   0.3070391659\n","i:  9,   0.1673682275\n","i: 10,   0.0757392934\n","i: 11,   0.2194152612\n","i: 12,  -0.7117225967\n","i: 13,  -0.6519255363\n","i: 14,   0.7322786608\n","i: 15,   0.7699393888\n","i: 16,   0.5167810851\n","i: 17,   0.2847722259\n","i: 18,  -0.7121525570\n","i: 19,  -0.5845793008\n","i: 20,  -0.9292181697\n","i: 21,   0.6099217977\n","i: 22,  -0.2605157380\n","i: 23,   0.1771300226\n","i: 24,  -0.4989551381\n","i: 25,  -0.5952030994\n","i: 26,   0.4611598036\n","i: 27,   0.9232751882\n","i: 28,  -0.7898806706\n","i: 29,  -0.5116289431\n","i: 30,  -0.6633367809\n","i: 31,  -0.5473370018\n","i: 32,  -0.1372456003\n","i: 33,  -0.6384022579\n","i: 34,  -0.9340299413\n","i: 35,  -0.7743634475\n","i: 36,   0.5358277663\n","i: 37,   0.8758098765\n","i: 38,   0.9368674906\n","i: 39,   0.2332465746\n","i: 40,  -0.1207504561\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'len(n.parameters()): {len(n.parameters())}')\n","# n.parameters()\n","for i, v in enumerate(n.parameters()):\n","  print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":245,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":246,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","neuron 0\n","w0: -0.0952691,   w0.grad:  0.0000000\n","w1:  0.3679388,   w1.grad:  0.0000000\n","w2:  0.9316894,   w2.grad:  0.0000000\n","b:   0.4807952\n","b_mat:  [0.48079524973212573]\n","neuron 1\n","w0: -0.1278205,   w0.grad:  0.0000000\n","w1:  0.7633232,   w1.grad:  0.0000000\n","w2:  0.1105173,   w2.grad:  0.0000000\n","b:   0.5484647\n","b_mat:  [0.48079524973212573, 0.5484646895629972]\n","neuron 2\n","w0:  0.3070392,   w0.grad:  0.0000000\n","w1:  0.1673682,   w1.grad:  0.0000000\n","w2:  0.0757393,   w2.grad:  0.0000000\n","b:   0.2194153\n","b_mat:  [0.48079524973212573, 0.5484646895629972, 0.2194152611754807]\n","neuron 3\n","w0: -0.7117226,   w0.grad:  0.0000000\n","w1: -0.6519255,   w1.grad:  0.0000000\n","w2:  0.7322787,   w2.grad:  0.0000000\n","b:   0.7699394\n","b_mat:  [0.48079524973212573, 0.5484646895629972, 0.2194152611754807, 0.7699393887704105]\n","----\n","layer: 1, neuron_cnt: 4\n","----\n","neuron 0\n","w0:  0.5167811,   w0.grad:  0.0000000\n","w1:  0.2847722,   w1.grad:  0.0000000\n","w2: -0.7121526,   w2.grad:  0.0000000\n","w3: -0.5845793,   w3.grad:  0.0000000\n","b:  -0.9292182\n","b_mat:  [-0.929218169688568]\n","neuron 1\n","w0:  0.6099218,   w0.grad:  0.0000000\n","w1: -0.2605157,   w1.grad:  0.0000000\n","w2:  0.1771300,   w2.grad:  0.0000000\n","w3: -0.4989551,   w3.grad:  0.0000000\n","b:  -0.5952031\n","b_mat:  [-0.929218169688568, -0.5952030993801443]\n","neuron 2\n","w0:  0.4611598,   w0.grad:  0.0000000\n","w1:  0.9232752,   w1.grad:  0.0000000\n","w2: -0.7898807,   w2.grad:  0.0000000\n","w3: -0.5116289,   w3.grad:  0.0000000\n","b:  -0.6633368\n","b_mat:  [-0.929218169688568, -0.5952030993801443, -0.6633367808735688]\n","neuron 3\n","w0: -0.5473370,   w0.grad:  0.0000000\n","w1: -0.1372456,   w1.grad:  0.0000000\n","w2: -0.6384023,   w2.grad:  0.0000000\n","w3: -0.9340299,   w3.grad:  0.0000000\n","b:  -0.7743634\n","b_mat:  [-0.929218169688568, -0.5952030993801443, -0.6633367808735688, -0.7743634475225218]\n","----\n","layer: 2, neuron_cnt: 1\n","----\n","neuron 0\n","w0:  0.5358278,   w0.grad:  0.0000000\n","w1:  0.8758099,   w1.grad:  0.0000000\n","w2:  0.9368675,   w2.grad:  0.0000000\n","w3:  0.2332466,   w3.grad:  0.0000000\n","b:  -0.1207505\n","b_mat:  [-0.12075045606828083]\n","----\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}')\n","        print(f'b_mat:  {b_mat}')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('----')"]},{"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":247,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat:\n","[[-0.09526907  0.36793879  0.93168943]\n"," [-0.12782045  0.76332324  0.11051733]\n"," [ 0.30703917  0.16736823  0.07573929]\n"," [-0.7117226  -0.65192554  0.73227866]]\n","b_mat:\n","[[0.48079525 0.54846469 0.21941526 0.76993939]]\n","\n","i: 1\n","w_mat:\n","[[ 0.51678109  0.28477223 -0.71215256 -0.5845793 ]\n"," [ 0.6099218  -0.26051574  0.17713002 -0.49895514]\n"," [ 0.4611598   0.92327519 -0.78988067 -0.51162894]\n"," [-0.547337   -0.1372456  -0.63840226 -0.93402994]]\n","b_mat:\n","[[-0.92921817 -0.5952031  -0.66333678 -0.77436345]]\n","\n","i: 2\n","w_mat:\n","[[0.53582777 0.87580988 0.93686749 0.23324657]]\n","b_mat:\n","[[-0.12075046]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat:\\n{w_n_b[0]}')\n","  print(f'b_mat:\\n{w_n_b[1]}\\n')  \n","    "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":248,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[-0.09526907  0.36793879  0.93168943]\n"," [-0.12782045  0.76332324  0.11051733]\n"," [ 0.30703917  0.16736823  0.07573929]\n"," [-0.7117226  -0.65192554  0.73227866]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","bias (4, 1):\n","[[0.48079525]\n"," [0.54846469]\n"," [0.21941526]\n"," [0.76993939]]\n","\n","output (4, 2):\n","[[ 0.43202527  0.28479623]\n"," [ 0.98585653 -0.49530172]\n"," [ 0.85102523  0.76618942]\n"," [-0.9974994  -0.33385763]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[ 0.51678109  0.28477223 -0.71215256 -0.5845793 ]\n"," [ 0.6099218  -0.26051574  0.17713002 -0.49895514]\n"," [ 0.4611598   0.92327519 -0.78988067 -0.51162894]\n"," [-0.547337   -0.1372456  -0.63840226 -0.93402994]]\n","\n","input (4, 2):\n","[[ 0.43202527  0.28479623]\n"," [ 0.98585653 -0.49530172]\n"," [ 0.85102523  0.76618942]\n"," [-0.9974994  -0.33385763]]\n","\n","bias (4, 1):\n","[[-0.92921817]\n"," [-0.5952031 ]\n"," [-0.66333678]\n"," [-0.77436345]]\n","\n","output (4, 2):\n","[[-0.42037993 -0.85476135]\n"," [ 0.05984536  0.00982903]\n"," [ 0.27683763 -0.89036556]\n"," [-0.63973998 -0.77771782]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[0.53582777 0.87580988 0.93686749 0.23324657]]\n","\n","input (4, 2):\n","[[-0.42037993 -0.85476135]\n"," [ 0.05984536  0.00982903]\n"," [ 0.27683763 -0.89036556]\n"," [-0.63973998 -0.77771782]]\n","\n","bias (1, 1):\n","[[-0.12075046]]\n","\n","output (1, 2):\n","[[-0.18141505 -0.91948765]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [-0.18141505 -0.91948765]\n","desired output:     [1.0, -1.0]\n","loss:               1.4022237690647879\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","  output = np.tanh(np.matmul(weights, input) + bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')    \n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":249,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [-0.18141505462832805, -0.9194876545967469]\n","ys:                 [1.0, -1.0]\n","loss_data:          1.4022237690647879\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":250,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0     -0.0952690735    -3.9840132700           0.05000         0.1039315900\n","  1      0.3679387872    -6.0574932664           0.05000         0.6708134505\n","  2      0.9316894304     2.0216333119           0.05000         0.8306077648\n","  3      0.4807952497    -1.9994133042           0.05000         0.5807659149\n","  4     -0.1278204536    -0.0881870322           0.05000        -0.1234111020\n","  5      0.7633232397    -0.1301716135           0.05000         0.7698318204\n","  6      0.1105173324     0.0433266307           0.05000         0.1083510009\n","  7      0.5484646896    -0.0439017948           0.05000         0.5506597793\n","  8      0.3070391659     1.1681575710           0.05000         0.2486312873\n","  9      0.1673682275     1.7612730881           0.05000         0.0793045731\n"," 10      0.0757392934    -0.5873648697           0.05000         0.1051075369\n"," 11      0.2194152612     0.5849003066           0.05000         0.1901702458\n"," 12     -0.7117225967    -0.0180146016           0.05000        -0.7108218666\n"," 13     -0.6519255363     0.0589287303           0.05000        -0.6548719728\n"," 14      0.7322786608    -0.0222474747           0.05000         0.7333910345\n"," 15      0.7699393888    -0.0011936069           0.05000         0.7699990691\n"," 16      0.5167810851    -0.4344698947           0.05000         0.5385045798\n"," 17      0.2847722259    -0.9955485330           0.05000         0.3345496525\n"," 18     -0.7121525570    -0.8551037533           0.05000        -0.6693973693\n"," 19     -0.5845793008     1.0043065241           0.05000        -0.6347946270\n"," 20     -0.9292181697    -1.0044344112           0.05000        -0.8789964491\n"," 21      0.6099217977    -0.8553020026           0.05000         0.6526868978\n"," 22     -0.2605157380    -1.9767060150           0.05000        -0.1616804373\n"," 23      0.1771300226    -1.6803456694           0.05000         0.2611473061\n"," 24     -0.4989551381     1.9818537597           0.05000        -0.5980478261\n"," 25     -0.5952030994    -1.9723232940           0.05000        -0.4965869347\n"," 26      0.4611598036    -0.8526233081           0.05000         0.5037909690\n"," 27      0.9232751882    -1.9511701297           0.05000         1.0208336947\n"," 28     -0.7898806706    -1.6785491019           0.05000        -0.7059532155\n"," 29     -0.5116289431     1.9701786566           0.05000        -0.6101378759\n"," 30     -0.6633367809    -1.9719029887           0.05000        -0.5647416314\n"," 31     -0.5473370018    -0.1353703732           0.05000        -0.5405684832\n"," 32     -0.1372456003    -0.3115339543           0.05000        -0.1216689026\n"," 33     -0.6384022579    -0.2661887929           0.05000        -0.6250928183\n"," 34     -0.9340299413     0.3132979425           0.05000        -0.9496948384\n"," 35     -0.7743634475    -0.3125573748           0.05000        -0.7587355788\n"," 36      0.5358277663     0.9393250149           0.05000         0.4888615155\n"," 37      0.8758098765    -0.1365060052           0.05000         0.8826351767\n"," 38      0.9368674906    -0.6547491503           0.05000         0.9696049481\n"," 39      0.2332465746     1.4424944543           0.05000         0.1611218519\n"," 40     -0.1207504561    -2.2601808596           0.05000        -0.0077414131\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":251,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.8541020624483188), Value(data = -0.7623896006323063)]\n","step: 0, loss: 0.07774491006950916\n","-------\n","ypred: [Value(data = 0.8544159381585993), Value(data = -0.8163606661868883)]\n","step: 1, loss: 0.054918123985564254\n","-------\n","ypred: [Value(data = 0.8576769341825883), Value(data = -0.843750813721096)]\n","step: 2, loss: 0.044669663276486964\n","-------\n","ypred: [Value(data = 0.8617611617511163), Value(data = -0.8613940975288775)]\n","step: 3, loss: 0.03832157260023535\n","-------\n","ypred: [Value(data = 0.866027292769352), Value(data = -0.874052685559009)]\n","step: 4, loss: 0.033811412297606806\n","-------\n","ypred: [Value(data = 0.870222016374644), Value(data = -0.8837349359142657)]\n","step: 5, loss: 0.030359890160723074\n","-------\n","ypred: [Value(data = 0.8742393670844095), Value(data = -0.8914645856451232)]\n","step: 6, loss: 0.027595672960514694\n","-------\n","ypred: [Value(data = 0.8780382776437373), Value(data = -0.897828206336281)]\n","step: 7, loss: 0.02531373714056767\n","-------\n","ypred: [Value(data = 0.8816082894144105), Value(data = -0.9031902069584674)]\n","step: 8, loss: 0.023388733164126357\n","-------\n","ypred: [Value(data = 0.884953681390452), Value(data = -0.90779093613724)]\n","step: 9, loss: 0.021738166884056165\n","-------\n","ypred: [Value(data = 0.8880856426301632), Value(data = -0.9117964210951092)]\n","step: 10, loss: 0.020304694717134846\n","-------\n","ypred: [Value(data = 0.8910182690027497), Value(data = -0.9153256603206285)]\n","step: 11, loss: 0.01904676149129462\n","-------\n","ypred: [Value(data = 0.8937664855389397), Value(data = -0.918466555791943)]\n","step: 12, loss: 0.017933262119176653\n","-------\n","ypred: [Value(data = 0.8963449769285761), Value(data = -0.921285690205816)]\n","step: 13, loss: 0.016940306374312205\n","-------\n","ypred: [Value(data = 0.8987676592824815), Value(data = -0.9238345801665725)]\n","step: 14, loss: 0.016049157985550032\n","-------\n","ypred: [Value(data = 0.9010474462701359), Value(data = -0.9261538171031166)]\n","step: 15, loss: 0.015244866618101597\n","-------\n","ypred: [Value(data = 0.9031961753360919), Value(data = -0.9282758921512793)]\n","step: 16, loss: 0.014515328116255593\n","-------\n","ypred: [Value(data = 0.9052246194790158), Value(data = -0.9302271734306569)]\n","step: 17, loss: 0.01385062008037299\n","-------\n","ypred: [Value(data = 0.9071425427473795), Value(data = -0.9320293214702918)]\n","step: 18, loss: 0.013242520507211177\n","-------\n","ypred: [Value(data = 0.9089587758644819), Value(data = -0.9337003226863014)]\n","step: 19, loss: 0.012684151703994196\n","-------\n","ypred: [Value(data = 0.9106812987721848), Value(data = -0.9352552573555494)]\n","step: 20, loss: 0.012169712089119853\n","-------\n","ypred: [Value(data = 0.9123173228473478), Value(data = -0.9367068793281373)]\n","step: 21, loss: 0.011694270997039206\n","-------\n","ypred: [Value(data = 0.9138733689930251), Value(data = -0.938066059852515)]\n","step: 22, loss: 0.01125360951080387\n","-------\n","ypred: [Value(data = 0.915355339797969), Value(data = -0.9393421317306955)]\n","step: 23, loss: 0.01084409548369359\n","-------\n","ypred: [Value(data = 0.916768585097247), Value(data = -0.9405431592979385)]\n","step: 24, loss: 0.010462584332984526\n","-------\n","ypred: [Value(data = 0.9181179609040739), Value(data = -0.9416761524650298)]\n","step: 25, loss: 0.010106339517789217\n","-------\n","ypred: [Value(data = 0.9194078820239863), Value(data = -0.942747238063827)]\n","step: 26, loss: 0.009772968229179817\n","-------\n","ypred: [Value(data = 0.9206423688292461), Value(data = -0.9437617982390918)]\n","step: 27, loss: 0.009460368962334029\n","-------\n","ypred: [Value(data = 0.9218250887368098), Value(data = -0.9447245831452864)]\n","step: 28, loss: 0.009166688459470019\n","-------\n","ypred: [Value(data = 0.9229593929410352), Value(data = -0.945639803418288)]\n","step: 29, loss: 0.008890286108416183\n","-------\n","ypred: [Value(data = 0.9240483489287908), Value(data = -0.9465112065878184)]\n","step: 30, loss: 0.00862970432113375\n","-------\n","ypred: [Value(data = 0.9250947692653945), Value(data = -0.9473421406344426)]\n","step: 31, loss: 0.00838364374436732\n","-------\n","ypred: [Value(data = 0.926101237094098), Value(data = -0.9481356071777265)]\n","step: 32, loss: 0.008150942401845818\n","-------\n","ypred: [Value(data = 0.9270701287449262), Value(data = -0.9488943062419556)]\n","step: 33, loss: 0.007930558055772655\n","-------\n","ypred: [Value(data = 0.9280036338035457), Value(data = -0.9496206741351242)]\n","step: 34, loss: 0.007721553220093297\n","-------\n","ypred: [Value(data = 0.928903772948956), Value(data = -0.9503169156619254)]\n","step: 35, loss: 0.007523082370237832\n","-------\n","ypred: [Value(data = 0.929772413830834), Value(data = -0.9509850316478996)]\n","step: 36, loss: 0.0073343809817050425\n","-------\n","ypred: [Value(data = 0.9306112852234493), Value(data = -0.9516268425620809)]\n","step: 37, loss: 0.007154756098855223\n","-------\n","ypred: [Value(data = 0.9314219896631047), Value(data = -0.9522440088764736)]\n","step: 38, loss: 0.006983578189957641\n","-------\n","ypred: [Value(data = 0.9322060147497429), Value(data = -0.9528380486829175)]\n","step: 39, loss: 0.006820274088146939\n","-------\n","ypred: [Value(data = 0.9329647432703686), Value(data = -0.9534103529942)]\n","step: 40, loss: 0.006664320852932644\n","-------\n","ypred: [Value(data = 0.9336994622819171), Value(data = -0.9539621990812812)]\n","step: 41, loss: 0.006515240415138512\n","-------\n","ypred: [Value(data = 0.9344113712737908), Value(data = -0.954494762138115)]\n","step: 42, loss: 0.006372594891051244\n","-------\n","ypred: [Value(data = 0.9351015895151701), Value(data = -0.955009125516663)]\n","step: 43, loss: 0.006235982470232871\n","-------\n","ypred: [Value(data = 0.9357711626790913), Value(data = -0.9555062897349285)]\n","step: 44, loss: 0.006105033796747884\n","-------\n","ypred: [Value(data = 0.9364210688238964), Value(data = -0.9559871804282928)]\n","step: 45, loss: 0.005979408776147371\n","-------\n","ypred: [Value(data = 0.9370522238027773), Value(data = -0.9564526553877112)]\n","step: 46, loss: 0.005858793750957076\n","-------\n","ypred: [Value(data = 0.9376654861635535), Value(data = -0.956903510806249)]\n","step: 47, loss: 0.005742898996053235\n","-------\n","ypred: [Value(data = 0.9382616615933514), Value(data = -0.9573404868371469)]\n","step: 48, loss: 0.005631456492505493\n","-------\n","ypred: [Value(data = 0.9388415069563647), Value(data = -0.9577642725513753)]\n","step: 49, loss: 0.005524217944482896\n","-------\n","ypred: [Value(data = 0.9394057339672024), Value(data = -0.9581755103699044)]\n","step: 50, loss: 0.00542095300887142\n","-------\n","ypred: [Value(data = 0.9399550125374054), Value(data = -0.9585748000352428)]\n","step: 51, loss: 0.0053214477115032625\n","-------\n","ypred: [Value(data = 0.9404899738283855), Value(data = -0.9589627021778033)]\n","step: 52, loss: 0.00522550302749391\n","-------\n","ypred: [Value(data = 0.9410112130402718), Value(data = -0.9593397415250637)]\n","step: 53, loss: 0.005132933606228826\n","-------\n","ypred: [Value(data = 0.9415192919628456), Value(data = -0.9597064097950444)]\n","step: 54, loss: 0.005043566624131793\n","-------\n","ypred: [Value(data = 0.9420147413118366), Value(data = -0.960063168310158)]\n","step: 55, loss: 0.004957240750555994\n","-------\n","ypred: [Value(data = 0.9424980628713121), Value(data = -0.960410450362807)]\n","step: 56, loss: 0.004873805214027341\n","-------\n","ypred: [Value(data = 0.9429697314606428), Value(data = -0.9607486633601171)]\n","step: 57, loss: 0.004793118957688611\n","-------\n","ypred: [Value(data = 0.9434301967425557), Value(data = -0.9610781907717562)]\n","step: 58, loss: 0.004715049874185765\n","-------\n","ypred: [Value(data = 0.9438798848870433), Value(data = -0.9613993939018526)]\n","step: 59, loss: 0.004639474111435843\n","-------\n","ypred: [Value(data = 0.9443192001043507), Value(data = -0.961712613503467)]\n","step: 60, loss: 0.004566275441754237\n","-------\n","ypred: [Value(data = 0.9447485260589048), Value(data = -0.9620181712518816)]\n","step: 61, loss: 0.004495344687714915\n","-------\n","ypred: [Value(data = 0.94516822717484), Value(data = -0.9623163710910624)]\n","step: 62, loss: 0.004426579198896479\n","-------\n","ypred: [Value(data = 0.9455786498426992), Value(data = -0.9626075004659846)]\n","step: 63, loss: 0.004359882374344879\n","-------\n","ypred: [Value(data = 0.9459801235359387), Value(data = -0.9628918314520781)]\n","step: 64, loss: 0.004295163226173418\n","-------\n","ypred: [Value(data = 0.9463729618450158), Value(data = -0.9631696217917797)]\n","step: 65, loss: 0.004232335980236677\n","-------\n","ypred: [Value(data = 0.9467574634360826), Value(data = -0.9634411158470816)]\n","step: 66, loss: 0.004171319710266593\n","-------\n","ypred: [Value(data = 0.9471339129406324), Value(data = -0.963706545476)]\n","step: 67, loss: 0.004112038002254288\n","-------\n","ypred: [Value(data = 0.9475025817818464), Value(data = -0.9639661308400285)]\n","step: 68, loss: 0.00405441864620967\n","-------\n","ypred: [Value(data = 0.9478637289428437), Value(data = -0.9642200811489099)]\n","step: 69, loss: 0.003998393352735872\n","-------\n","ypred: [Value(data = 0.9482176016815576), Value(data = -0.9644685953483834)]\n","step: 70, loss: 0.003943897492126755\n","-------\n","ypred: [Value(data = 0.9485644361965289), Value(data = -0.9647118627559952)]\n","step: 71, loss: 0.00389086985393267\n","-------\n","ypred: [Value(data = 0.9489044582475119), Value(data = -0.9649500636495365)]\n","step: 72, loss: 0.003839252425151796\n","-------\n","ypred: [Value(data = 0.949237883734445), Value(data = -0.9651833698122224)]\n","step: 73, loss: 0.0037889901853901935\n","-------\n","ypred: [Value(data = 0.949564919238018), Value(data = -0.9654119450383111)]\n","step: 74, loss: 0.003740030917500461\n","-------\n","ypred: [Value(data = 0.9498857625247877), Value(data = -0.9656359456025135)]\n","step: 75, loss: 0.003692325032355378\n","-------\n","ypred: [Value(data = 0.9502006030195332), Value(data = -0.9658555206962078)]\n","step: 76, loss: 0.0036458254065452134\n","-------\n","ypred: [Value(data = 0.9505096222473138), Value(data = -0.9660708128331992)]\n","step: 77, loss: 0.0036004872319033753\n","-------\n","ypred: [Value(data = 0.9508129942474823), Value(data = -0.9662819582275)]\n","step: 78, loss: 0.0035562678758702636\n","-------\n","ypred: [Value(data = 0.9511108859617143), Value(data = -0.9664890871453788)]\n","step: 79, loss: 0.0035131267517985137\n","-------\n","ypred: [Value(data = 0.9514034575979471), Value(data = -0.9666923242337232)]\n","step: 80, loss: 0.003471025198385949\n","-------\n","ypred: [Value(data = 0.9516908629719614), Value(data = -0.9668917888265712)]\n","step: 81, loss: 0.0034299263674981663\n","-------\n","ypred: [Value(data = 0.9519732498281983), Value(data = -0.9670875952315061)]\n","step: 82, loss: 0.003389795119709833\n","-------\n","ypred: [Value(data = 0.9522507601412799), Value(data = -0.9672798529974553)]\n","step: 83, loss: 0.0033505979269537215\n","-------\n","ypred: [Value(data = 0.9525235303995754), Value(data = -0.9674686671653022)]\n","step: 84, loss: 0.003312302781721929\n","-------\n","ypred: [Value(data = 0.9527916918720584), Value(data = -0.9676541385025975)]\n","step: 85, loss: 0.003274879112311825\n","-------\n","ypred: [Value(data = 0.9530553708595965), Value(data = -0.967836363723546)]\n","step: 86, loss: 0.0032382977036540537\n","-------\n","ypred: [Value(data = 0.9533146889317279), Value(data = -0.9680154356953468)]\n","step: 87, loss: 0.0032025306232998296\n","-------\n","ypred: [Value(data = 0.9535697631498988), Value(data = -0.9681914436318741)]\n","step: 88, loss: 0.003167551152180732\n","-------\n","ypred: [Value(data = 0.953820706278062), Value(data = -0.9683644732756068)]\n","step: 89, loss: 0.003133333719786814\n","-------\n","ypred: [Value(data = 0.9540676269814663), Value(data = -0.9685346070686348)]\n","step: 90, loss: 0.0030998538434389294\n","-------\n","ypred: [Value(data = 0.9543106300144084), Value(data = -0.9687019243135119)]\n","step: 91, loss: 0.0030670880713574164\n","-------\n","ypred: [Value(data = 0.9545498163976579), Value(data = -0.9688665013246548)]\n","step: 92, loss: 0.0030350139292543285\n","-------\n","ypred: [Value(data = 0.9547852835862174), Value(data = -0.969028411570941)]\n","step: 93, loss: 0.0030036098701978035\n","-------\n","ypred: [Value(data = 0.9550171256280279), Value(data = -0.9691877258100974)]\n","step: 94, loss: 0.002972855227518366\n","-------\n","ypred: [Value(data = 0.9552454333141903), Value(data = -0.9693445122154375)]\n","step: 95, loss: 0.002942730170544047\n","-------\n","ypred: [Value(data = 0.9554702943212283), Value(data = -0.9694988364954503)]\n","step: 96, loss: 0.002913215662969304\n","-------\n","ypred: [Value(data = 0.955691793345885), Value(data = -0.9696507620067155)]\n","step: 97, loss: 0.0028842934236767786\n","-------\n","ypred: [Value(data = 0.9559100122329074), Value(data = -0.9698003498605775)]\n","step: 98, loss: 0.002855945889845897\n","-------\n","ypred: [Value(data = 0.9561250300962448), Value(data = -0.9699476590239824)]\n","step: 99, loss: 0.002828156182194249\n","-------\n","ypred: [Value(data = 0.9563369234340557), Value(data = -0.970092746414851)]\n","step: 100, loss: 0.002800908072209919\n","-------\n","ypred: [Value(data = 0.9565457662378909), Value(data = -0.9702356669923321)]\n","step: 101, loss: 0.0027741859512433716\n","-------\n","ypred: [Value(data = 0.9567516300963961), Value(data = -0.9703764738422588)]\n","step: 102, loss: 0.002747974801337332\n","-------\n","ypred: [Value(data = 0.9569545842938558), Value(data = -0.9705152182581058)]\n","step: 103, loss: 0.0027222601676818994\n","-------\n","ypred: [Value(data = 0.9571546959038754), Value(data = -0.9706519498177227)]\n","step: 104, loss: 0.002697028132590859\n","-------\n","ypred: [Value(data = 0.9573520298784816), Value(data = -0.9707867164561019)]\n","step: 105, loss: 0.002672265290902116\n","-------\n","ypred: [Value(data = 0.9575466491329024), Value(data = -0.9709195645344207)]\n","step: 106, loss: 0.002647958726712613\n","-------\n","ypred: [Value(data = 0.957738614626271), Value(data = -0.9710505389055806)]\n","step: 107, loss: 0.002624095991364142\n","-------\n","ypred: [Value(data = 0.9579279854384805), Value(data = -0.97117968297645)]\n","step: 108, loss: 0.0026006650826026403\n","-------\n","ypred: [Value(data = 0.9581148188434044), Value(data = -0.9713070387670067)]\n","step: 109, loss: 0.002577654424838882\n","-------\n","ypred: [Value(data = 0.9582991703786845), Value(data = -0.9714326469665584)]\n","step: 110, loss: 0.0025550528504432687\n","-------\n","ypred: [Value(data = 0.9584810939122698), Value(data = -0.9715565469872095)]\n","step: 111, loss: 0.0025328495820125837\n","-------\n","ypred: [Value(data = 0.958660641705887), Value(data = -0.9716787770147366)]\n","step: 112, loss: 0.0025110342155500575\n","-------\n","ypred: [Value(data = 0.9588378644756056), Value(data = -0.9717993740570121)]\n","step: 113, loss: 0.0024895967045049333\n","-------\n","ypred: [Value(data = 0.9590128114496511), Value(data = -0.971918373990121)]\n","step: 114, loss: 0.002468527344620561\n","-------\n","ypred: [Value(data = 0.9591855304236167), Value(data = -0.9720358116022939)]\n","step: 115, loss: 0.002447816759543922\n","-------\n","ypred: [Value(data = 0.9593560678132036), Value(data = -0.9721517206357824)]\n","step: 116, loss: 0.0024274558871524143\n","-------\n","ypred: [Value(data = 0.9595244687046255), Value(data = -0.9722661338267836)]\n","step: 117, loss: 0.0024074359665567173\n","-------\n","ypred: [Value(data = 0.9596907769027927), Value(data = -0.9723790829435268)]\n","step: 118, loss: 0.002387748525741002\n","-------\n","ypred: [Value(data = 0.9598550349773933), Value(data = -0.9724905988226142)]\n","step: 119, loss: 0.0023683853698046663\n","-------\n","ypred: [Value(data = 0.9600172843069762), Value(data = -0.9726007114037148)]\n","step: 120, loss: 0.002349338569771694\n","-------\n","ypred: [Value(data = 0.9601775651211392), Value(data = -0.9727094497626968)]\n","step: 121, loss: 0.002330600451935882\n","-------\n","ypred: [Value(data = 0.9603359165409152), Value(data = -0.9728168421432789)]\n","step: 122, loss: 0.0023121635877126614\n","-------\n","ypred: [Value(data = 0.9604923766174478), Value(data = -0.9729229159872825)]\n","step: 123, loss: 0.0022940207839693455\n","-------\n","ypred: [Value(data = 0.9606469823690398), Value(data = -0.9730276979635516)]\n","step: 124, loss: 0.0022761650738080644\n","-------\n","ypred: [Value(data = 0.9607997698166555), Value(data = -0.9731312139956132)]\n","step: 125, loss: 0.002258589707776726\n","-------\n","ypred: [Value(data = 0.9609507740179509), Value(data = -0.9732334892881406)]\n","step: 126, loss: 0.002241288145485222\n","-------\n","ypred: [Value(data = 0.9611000290999044), Value(data = -0.9733345483522814)]\n","step: 127, loss: 0.0022242540476051006\n","-------\n","ypred: [Value(data = 0.961247568290114), Value(data = -0.9734344150299042)]\n","step: 128, loss: 0.0022074812682327523\n","-------\n","ypred: [Value(data = 0.9613934239468254), Value(data = -0.9735331125168244)]\n","step: 129, loss: 0.0021909638475966346\n","-------\n","ypred: [Value(data = 0.9615376275877506), Value(data = -0.9736306633850489)]\n","step: 130, loss: 0.0021746960050911626\n","-------\n","ypred: [Value(data = 0.9616802099177346), Value(data = -0.9737270896041013)]\n","step: 131, loss: 0.0021586721326198083\n","-------\n","ypred: [Value(data = 0.9618212008553227), Value(data = -0.9738224125614607)]\n","step: 132, loss: 0.002142886788231977\n","-------\n","ypred: [Value(data = 0.961960629558281), Value(data = -0.9739166530821632)]\n","step: 133, loss: 0.0021273346900385504\n","-------\n","ypred: [Value(data = 0.9620985244481166), Value(data = -0.9740098314476056)]\n","step: 134, loss: 0.0021120107103918873\n","-------\n","ypred: [Value(data = 0.962234913233645), Value(data = -0.9741019674135875)]\n","step: 135, loss: 0.0020969098703172103\n","-------\n","ypred: [Value(data = 0.9623698229336466), Value(data = -0.9741930802276294)]\n","step: 136, loss: 0.0020820273341826826\n","-------\n","ypred: [Value(data = 0.9625032798986544), Value(data = -0.9742831886456008)]\n","step: 137, loss: 0.002067358404596413\n","-------\n","ypred: [Value(data = 0.9626353098319101), Value(data = -0.9743723109476898)]\n","step: 138, loss: 0.002052898517519249\n","-------\n","ypred: [Value(data = 0.9627659378095289), Value(data = -0.9744604649537444)]\n","step: 139, loss: 0.002038643237582788\n","-------\n","ypred: [Value(data = 0.9628951882999044), Value(data = -0.9745476680380167)]\n","step: 140, loss: 0.002024588253602547\n","-------\n","ypred: [Value(data = 0.963023085182389), Value(data = -0.9746339371433334)]\n","step: 141, loss: 0.002010729374277224\n","-------\n","ypred: [Value(data = 0.9631496517652816), Value(data = -0.9747192887947232)]\n","step: 142, loss: 0.0019970625240646224\n","-------\n","ypred: [Value(data = 0.9632749108031512), Value(data = -0.9748037391125219)]\n","step: 143, loss: 0.0019835837392263573\n","-------\n","ypred: [Value(data = 0.9633988845135264), Value(data = -0.9748873038249798)]\n","step: 144, loss: 0.0019702891640330524\n","-------\n","ypred: [Value(data = 0.9635215945929777), Value(data = -0.9749699982803941)]\n","step: 145, loss: 0.001957175047122548\n","-------\n","ypred: [Value(data = 0.9636430622326171), Value(data = -0.9750518374587877)]\n","step: 146, loss: 0.001944237738004105\n","-------\n","ypred: [Value(data = 0.9637633081330419), Value(data = -0.9751328359831521)]\n","step: 147, loss: 0.0019314736837016803\n","-------\n","ypred: [Value(data = 0.9638823525187443), Value(data = -0.9752130081302747)]\n","step: 148, loss: 0.0019188794255300841\n","-------\n","ypred: [Value(data = 0.9640002151520096), Value(data = -0.9752923678411703)]\n","step: 149, loss: 0.001906451595997631\n","-------\n","ypred: [Value(data = 0.9641169153463245), Value(data = -0.9753709287311302)]\n","step: 150, loss: 0.0018941869158299119\n","-------\n","ypred: [Value(data = 0.9642324719793153), Value(data = -0.9754487040994079)]\n","step: 151, loss: 0.0018820821911088989\n","-------\n","ypred: [Value(data = 0.9643469035052353), Value(data = -0.9755257069385561)]\n","step: 152, loss: 0.001870134310522441\n","-------\n","ypred: [Value(data = 0.9644602279670208), Value(data = -0.9756019499434299)]\n","step: 153, loss: 0.0018583402427190328\n","-------\n","ypred: [Value(data = 0.9645724630079308), Value(data = -0.9756774455198686)]\n","step: 154, loss: 0.0018466970337633962\n","-------\n","ypred: [Value(data = 0.9646836258827898), Value(data = -0.9757522057930735)]\n","step: 155, loss: 0.001835201804688213\n","-------\n","ypred: [Value(data = 0.964793733468848), Value(data = -0.9758262426156884)]\n","step: 156, loss: 0.001823851749138073\n","-------\n","ypred: [Value(data = 0.9649028022762747), Value(data = -0.9758995675756007)]\n","step: 157, loss: 0.0018126441311013073\n","-------\n","ypred: [Value(data = 0.9650108484582984), Value(data = -0.975972192003471)]\n","step: 158, loss: 0.001801576282726222\n","-------\n","ypred: [Value(data = 0.9651178878210099), Value(data = -0.9760441269800036)]\n","step: 159, loss: 0.0017906456022178364\n","-------\n","ypred: [Value(data = 0.9652239358328383), Value(data = -0.9761153833429685)]\n","step: 160, loss: 0.0017798495518118942\n","-------\n","ypred: [Value(data = 0.9653290076337151), Value(data = -0.9761859716939822)]\n","step: 161, loss: 0.001769185655822801\n","-------\n","ypred: [Value(data = 0.9654331180439378), Value(data = -0.9762559024050634)]\n","step: 162, loss: 0.0017586514987622086\n","-------\n","ypred: [Value(data = 0.9655362815727435), Value(data = -0.9763251856249638)]\n","step: 163, loss: 0.001748244723525637\n","-------\n","ypred: [Value(data = 0.9656385124266057), Value(data = -0.9763938312852906)]\n","step: 164, loss: 0.0017379630296438576\n","-------\n","ypred: [Value(data = 0.9657398245172618), Value(data = -0.976461849106423)]\n","step: 165, loss: 0.0017278041715968156\n","-------\n","ypred: [Value(data = 0.9658402314694853), Value(data = -0.9765292486032362)]\n","step: 166, loss: 0.0017177659571870338\n","-------\n","ypred: [Value(data = 0.965939746628608), Value(data = -0.976596039090635)]\n","step: 167, loss: 0.0017078462459705038\n","-------\n","ypred: [Value(data = 0.9660383830678055), Value(data = -0.9766622296889083)]\n","step: 168, loss: 0.0016980429477423927\n","-------\n","ypred: [Value(data = 0.9661361535951517), Value(data = -0.9767278293289087)]\n","step: 169, loss: 0.0016883540210755606\n","-------\n","ypred: [Value(data = 0.966233070760453), Value(data = -0.9767928467570662)]\n","step: 170, loss: 0.0016787774719095884\n","-------\n","ypred: [Value(data = 0.9663291468618684), Value(data = -0.9768572905402408)]\n","step: 171, loss: 0.00166931135218845\n","-------\n","ypred: [Value(data = 0.9664243939523262), Value(data = -0.976921169070419)]\n","step: 172, loss: 0.0016599537585447754\n","-------\n","ypred: [Value(data = 0.9665188238457401), Value(data = -0.9769844905692641)]\n","step: 173, loss: 0.0016507028310288792\n","-------\n","ypred: [Value(data = 0.9666124481230369), Value(data = -0.9770472630925197)]\n","step: 174, loss: 0.0016415567518809108\n","-------\n","ypred: [Value(data = 0.9667052781379992), Value(data = -0.9771094945342781)]\n","step: 175, loss: 0.001632513744344237\n","-------\n","ypred: [Value(data = 0.9667973250229311), Value(data = -0.9771711926311143)]\n","step: 176, loss: 0.001623572071518565\n","-------\n","ypred: [Value(data = 0.9668885996941532), Value(data = -0.9772323649660911)]\n","step: 177, loss: 0.001614730035251307\n","-------\n","ypred: [Value(data = 0.9669791128573315), Value(data = -0.977293018972643)]\n","step: 178, loss: 0.0016059859750656006\n","-------\n","ypred: [Value(data = 0.9670688750126492), Value(data = -0.9773531619383379)]\n","step: 179, loss: 0.0015973382671236715\n","-------\n","ypred: [Value(data = 0.9671578964598221), Value(data = -0.977412801008527)]\n","step: 180, loss: 0.001588785323224165\n","-------\n","ypred: [Value(data = 0.967246187302969), Value(data = -0.9774719431898821)]\n","step: 181, loss: 0.0015803255898320927\n","-------\n","ypred: [Value(data = 0.9673337574553361), Value(data = -0.9775305953538274)]\n","step: 182, loss: 0.0015719575471402534\n","-------\n","ypred: [Value(data = 0.9674206166438849), Value(data = -0.977588764239867)]\n","step: 183, loss: 0.0015636797081609755\n","-------\n","ypred: [Value(data = 0.9675067744137462), Value(data = -0.9776464564588155)]\n","step: 184, loss: 0.0015554906178468072\n","-------\n","ypred: [Value(data = 0.9675922401325449), Value(data = -0.9777036784959289)]\n","step: 185, loss: 0.0015473888522395407\n","-------\n","ypred: [Value(data = 0.9676770229946007), Value(data = -0.9777604367139472)]\n","step: 186, loss: 0.0015393730176459213\n","-------\n","ypred: [Value(data = 0.9677611320250081), Value(data = -0.9778167373560442)]\n","step: 187, loss: 0.0015314417498396794\n","-------\n","ypred: [Value(data = 0.967844576083601), Value(data = -0.9778725865486948)]\n","step: 188, loss: 0.001523593713288329\n","-------\n","ypred: [Value(data = 0.9679273638688027), Value(data = -0.977927990304454)]\n","step: 189, loss: 0.0015158276004044575\n","-------\n","ypred: [Value(data = 0.9680095039213703), Value(data = -0.9779829545246606)]\n","step: 190, loss: 0.0015081421308199869\n","-------\n","ypred: [Value(data = 0.9680910046280312), Value(data = -0.9780374850020599)]\n","step: 191, loss: 0.0015005360506830695\n","-------\n","ypred: [Value(data = 0.9681718742250199), Value(data = -0.9780915874233528)]\n","step: 192, loss: 0.0014930081319765444\n","-------\n","ypred: [Value(data = 0.9682521208015153), Value(data = -0.9781452673716712)]\n","step: 193, loss: 0.001485557171857315\n","-------\n","ypred: [Value(data = 0.9683317523029835), Value(data = -0.9781985303289847)]\n","step: 194, loss: 0.0014781819920157882\n","-------\n","ypred: [Value(data = 0.9684107765344286), Value(data = -0.9782513816784388)]\n","step: 195, loss: 0.00147088143805475\n","-------\n","ypred: [Value(data = 0.9684892011635541), Value(data = -0.9783038267066266)]\n","step: 196, loss: 0.0014636543788870491\n","-------\n","ypred: [Value(data = 0.9685670337238383), Value(data = -0.9783558706057989)]\n","step: 197, loss: 0.0014564997061512373\n","-------\n","ypred: [Value(data = 0.9686442816175266), Value(data = -0.9784075184760115)]\n","step: 198, loss: 0.0014494163336447668\n","-------\n","ypred: [Value(data = 0.9687209521185418), Value(data = -0.9784587753272143)]\n","step: 199, loss: 0.0014424031967739872\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":252,"metadata":{},"outputs":[{"data":{"text/plain":["Text(0, 0.5, 'Loss')"]},"execution_count":252,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","# Create a list of iterations\n","iterations = range(len(losses))\n","\n","# Plot the loss as a function of iteration\n","plt.plot(iterations, losses)\n","\n","# Add a title to the plot\n","plt.title('Loss vs. Iteration')\n","\n","# Add labels to the x-axis and y-axis\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# TODO build same model with pyTorch "]},{"cell_type":"code","execution_count":253,"metadata":{},"outputs":[],"source":["# # create neural network and initialize weights and biases\n","# n = MLP(3, [4, 4, 1])\n","\n","# # inputs\n","# xs = [\n","#   [2.0, 3.0, -1.0],\n","#   [3.0, -1.0, 0.5],\n","#   [5.0, -3.0, 1.5]  \n","# ]\n","\n","# # desired targets\n","# ys = [1.0, -1.0, -.5]\n","\n","# # learning rate (i.e. step size)\n","# learning_rate = 0.05"]},{"cell_type":"code","execution_count":261,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 0.896253228187561\n","Epoch 100 loss: 5.684341886080802e-14\n","Epoch 200 loss: 2.3092638912203256e-14\n","Epoch 300 loss: 1.4210854715202004e-14\n","Epoch 400 loss: 3.552713678800501e-15\n","Epoch 500 loss: 3.552713678800501e-15\n","Epoch 600 loss: 3.552713678800501e-15\n","Epoch 700 loss: 3.552713678800501e-15\n","Epoch 800 loss: 3.552713678800501e-15\n","Epoch 900 loss: 3.552713678800501e-15\n","Prediction: tensor([[ 1.0000],\n","        [-1.0000]], grad_fn=<AddmmBackward0>)\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        self.fc3 = nn.Linear(4, 1)\n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","model = MLP()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","\n","# # inputs\n","# xs = [\n","#   [2.0, 3.0, -1.0],\n","#   [3.0, -1.0, 0.5],\n","#   [5.0, -3.0, 1.5]  \n","# ]\n","\n","# # desired targets\n","# ys = [1.0, -1.0, -.5]\n","\n","# add a dimension to the target tensor\n","ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","for epoch in range(1000):\n","    # forward pass\n","    outputs = model(torch.tensor(xs))\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, ys)\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 100 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","print(f\"Prediction: {model(torch.tensor(xs))}\")\n"]},{"cell_type":"code","execution_count":262,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inputs.shape: torch.Size([2, 3])\n","inputs:\n","tensor([[ 2.0000,  3.0000, -1.0000],\n","        [ 3.0000, -1.0000,  0.5000]])\n","\n","torch.Size([4, 3])\n","Parameter containing:\n","tensor([[ 0.4922, -0.2512, -0.4227],\n","        [ 0.3134, -0.5421,  0.4623],\n","        [-0.1160, -0.4052, -0.0508],\n","        [-0.2223, -0.2276,  0.3273]], requires_grad=True)\n","---\n","torch.Size([4])\n","Parameter containing:\n","tensor([ 0.0839, -0.2882,  0.3065, -0.2138], requires_grad=True)\n","---\n","torch.Size([4, 4])\n","Parameter containing:\n","tensor([[ 0.0130,  0.6891,  0.3890,  0.4239],\n","        [-0.4375,  0.4491, -0.1910,  0.1280],\n","        [ 0.3163,  0.3782,  0.5352, -0.4319],\n","        [-0.3815,  0.2908,  0.4849, -0.2074]], requires_grad=True)\n","---\n","torch.Size([4])\n","Parameter containing:\n","tensor([ 0.3154, -0.4014, -0.1113,  0.3830], requires_grad=True)\n","---\n","torch.Size([1, 4])\n","Parameter containing:\n","tensor([[-0.9101, -0.2871, -0.3493, -0.2715]], requires_grad=True)\n","---\n","torch.Size([1])\n","Parameter containing:\n","tensor([-0.1149], requires_grad=True)\n","---\n","target.shape: torch.Size([2, 1])\n","target:\n","tensor([[ 1.],\n","        [-1.]])\n","\n","prediction.shape: torch.Size([2, 1])\n","prediction:\n","tensor([[ 1.0000],\n","        [-1.0000]], grad_fn=<AddmmBackward0>)\n","\n"]}],"source":["print(f'inputs.shape: {torch.tensor(xs).shape}')\n","print(f'inputs:\\n{torch.tensor(xs)}\\n')\n","\n","for p in model.parameters():\n","  print(p.shape)\n","  print(p)\n","  print('---')\n","\n","print(f'target.shape: {ys.shape}')\n","print(f'target:\\n{ys}\\n')\n","print(f'prediction.shape: {model(torch.tensor(xs)).shape}')\n","print(f'prediction:\\n{model(torch.tensor(xs))}\\n')\n"]},{"cell_type":"code","execution_count":268,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input.shape: torch.Size([2, 3])\n","input:\n","tensor([[ 2.0000,  3.0000, -1.0000],\n","        [ 3.0000, -1.0000,  0.5000]])\n","\n","input_T.shape: torch.Size([3, 2])\n","input_T:\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n"]}],"source":["input = torch.tensor([[ 2.0000,  3.0000, -1.0000],\n","        [ 3.0000, -1.0000,  0.5000]])\n","\n","# input = torch.tensor([[ 2.0000,  3.0000, -1.0000],\n","#         [ 3.0000, -1.0000,  0.5000],\n","#         [ 5.0000, -3.0000,  1.5000]])\n","print(f'input.shape: {input.shape}')\n","print(f'input:\\n{input}\\n')\n","\n","input_T = torch.transpose(input, 0, 1)\n","print(f'input_T.shape: {input_T.shape}')\n","print(f'input_T:\\n{input_T}')"]},{"cell_type":"code","execution_count":271,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w0.shape: torch.Size([4, 3])\n","w0:\n","tensor([[-0.0953,  0.3679,  0.9317],\n","        [-0.1278,  0.7633,  0.1105],\n","        [ 0.3070,  0.1674,  0.0757],\n","        [-0.7117, -0.6519,  0.7323]], requires_grad=True)\n"]}],"source":["# w0 = torch.tensor([\n","#         [ 0.3721, -0.4228,  0.6085],\n","#         [ 0.3603, -0.7032,  0.2727],\n","#         [ 0.1643,  0.0272,  0.5176],\n","#         [ 0.3444,  0.1957,  0.2714]], requires_grad=True)\n","\n","w0 = torch.tensor(\n","[[-0.09526907, 0.36793879, 0.93168943],\n"," [-0.12782045, 0.76332324, 0.11051733],\n"," [ 0.30703917, 0.16736823, 0.07573929],\n"," [-0.7117226, -0.65192554, 0.73227866]],\n"," requires_grad=True)\n","print(f'w0.shape: {w0.shape}')\n","print(f'w0:\\n{w0}')"]},{"cell_type":"code","execution_count":273,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["output_.shape: torch.Size([4, 2])\n","output_:\n","tensor([[-0.0184, -0.1879],\n","        [ 1.9238, -1.0915],\n","        [ 1.0404,  0.7916],\n","        [-4.1115, -1.1171]], grad_fn=<MmBackward0>)\n"]}],"source":["output_= torch.matmul(w0, input_T)\n","print(f'output_.shape: {output_.shape}')\n","print(f'output_:\\n{output_}')"]},{"cell_type":"code","execution_count":281,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["b0.shape: torch.Size([4])\n","b0:\n","tensor([0.4808, 0.5485, 0.2194, 0.7699], requires_grad=True)\n","\n","b0_T.shape: torch.Size([4, 1])\n","b0_T:\n","tensor([[0.4808],\n","        [0.5485],\n","        [0.2194],\n","        [0.7699]], grad_fn=<UnsqueezeBackward0>)\n","\n"]}],"source":["b0 = torch.tensor([ 0.48079525,  0.54846469, 0.21941526,  0.76993939], requires_grad=True)\n","print(f'b0.shape: {b0.shape}')\n","print(f'b0:\\n{b0}\\n')\n","# b0_T = torch.transpose(b0, 0, 1)\n","# torch.unsqueeze(b0, 1)\n","\n","b0_T = torch.unsqueeze(b0, 1)\n","print(f'b0_T.shape: {b0_T.shape}')\n","print(f'b0_T:\\n{b0_T}\\n')\n"]},{"cell_type":"code","execution_count":285,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["output_0.shape: torch.Size([4, 2])\n","output_0:\n","tensor([[ 0.4320,  0.2848],\n","        [ 0.9859, -0.4953],\n","        [ 0.8510,  0.7662],\n","        [-0.9975, -0.3339]], grad_fn=<TanhBackward0>)\n","\n"]}],"source":["output_0 = torch.tanh(output_ + b0_T)\n","print(f'output_0.shape: {output_0.shape}')\n","print(f'output_0:\\n{output_0}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5],\n","  [5.0, -3.0, 1.5]  \n","]\n","\n","print(f'torch.tensor(xs).shape: {torch.tensor(xs).shape}')\n","# desired targets\n","ys = [1.0, -1.0, -.5]\n","print(f'torch.tensor(ys).shape: {torch.tensor(ys).shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ys = [1.0, -1.0, -.5]\n","ys = torch.unsqueeze(torch.tensor(ys), 1)\n","print(f'ys.shape: {ys.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class MLP(nn.Module):\n","#     def __init__(self):\n","#         super(MLP, self).__init__()\n","#         self.fc1 = nn.Linear(3, 4)\n","#         self.fc2 = nn.Linear(4, 4)\n","#         self.fc3 = nn.Linear(4, 1)\n","\n","#     def forward(self, x):\n","#         x = torch.tanh(self.fc1(x))\n","#         x = torch.tanh(self.fc2(x))\n","#         x = self.fc3(x)\n","#         return x\n","\n","# model = MLP()\n","\n","for p in model.parameters():\n","  print(p)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
