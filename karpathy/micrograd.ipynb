{"cells":[{"cell_type":"markdown","metadata":{},"source":["## [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","\n","class Neuron:\n","    \n","    def __init__(self, nin):\n","#### my add ##########################################        \n","        # random.seed(12345)  # WARNING: all neurons will have the same weights and bias\n","######################################################        \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["### Neuron in Neural Network\n","<!-- ### Simple Neural Network: Single Neuron with 3 Inputs -->\n","<img src=\"..\\karpathy\\img\\Nertual_Network_1_Neuron_3_Inputs.png\">"]},{"cell_type":"markdown","metadata":{},"source":["#### Activation Function: Tanh"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_num = np.arange(-5, 5, 0.2)\n","plt.figure(figsize=(4, 2))\n","plt.plot(_num, np.tanh(_num))\n","plt.title(\"Tanh Activation\")\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Output with Forward Pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# inputs\n","x0 = Value(-3.0, label='x0')\n","x1 = Value(0.0, label='x1')\n","x2 = Value(0.5, label='x2')\n","\n","# weights\n","w0 = Value(2.0, label='w0')\n","w1 = Value(1.0, label='w1')\n","w2 = Value(1.0, label='w2')\n","\n","# bias\n","b = Value(4.618626415, label='b')\n","\n","# forward pass\n","# x0*w0 + x1*w1 + x2*w2 + b\n","x0w0 = x0*w0; x0w0.label = 'x0*w0'\n","x1w1 = x1*w1; x1w1.label = 'x1*w1'\n","x2w2 = x2*w2; x2w2.label = 'x2*w2'\n","n_sum = x0w0 + x1w1 + x2w2; n_sum.label = 'x0w0 + x1w1 + x2w2'\n","n = n_sum + b; n.label = 'n'\n","out_0 = n.tanh(); out_0.label = 'out_0'\n","print(f'neuron output: {out_0.data}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Gradient with Backward Pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# backward pass to calculate gradient\n","out_0.backward()\n","  \n","out_0_grad = w0.grad  # store w0.grad, further calculation with w0 will reset w0.grad to zero\n","print(f'w0.grad(i.e. d(output)/d(w0)): {w0.grad}')\n","draw_dot(out_0)"]},{"cell_type":"markdown","metadata":{},"source":["##### Check Backward Pass Gradient Calculation: d(output) / d(x0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["h = 0.000001\n","w0 += h  # increment x0 by h\n","\n","# x0*w0 + x1*w1 + x2*w2 + b\n","x0w0 = x0*w0; x0w0.label = 'x0*w0'\n","x1w1 = x1*w1; x1w1.label = 'x1*w1'\n","x2w2 = x2*w2; x2w2.label = 'x2*w2'\n","n_sum = x0w0 + x1w1 + x2w2; n_sum.label = 'x0w0 + x1w1 + x2w2'\n","n = n_sum + b; n.label = 'n'\n","out_1 = n.tanh(); out_1.label = 'out_1'\n","out_grad = (out_1 - out_0) / h \n","\n","print(f'---- w0.grad from backward pass is same as d(out)/d(w0) calculation ----')\n","print(f'out_1: {out_1.data:<12.10f}, out_0: {out_0.data:<12.10f}, d(out): {out_1.data-out_0.data:<12.10f}, d(w0): {h:<12.10f}, d(out)/d(w0): {(out_1.data-out_0.data)/h:<12.10f}')\n","print(f'd(out) / d(w0):             {out_grad.data:<12.10f}')\n","print(f'w0.grad from backward pass: {out_0_grad:<12.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <--')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <--')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'len(n.parameters()): {len(n.parameters())}')\n","# n.parameters()\n","for i, v in enumerate(n.parameters()):\n","  print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}')\n","        print(f'b_mat:  {b_mat}')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('----')"]},{"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat:\\n{w_n_b[0]}')\n","  print(f'b_mat:\\n{w_n_b[1]}\\n')  \n","    "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","  output = np.tanh(np.matmul(weights, input) + bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')    \n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')"]},{"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Create a list of iterations\n","iterations = range(len(losses))\n","\n","# Plot the loss as a function of iteration\n","plt.plot(iterations, losses)\n","\n","# Add a title to the plot\n","plt.title('Loss vs. Iteration')\n","\n","# Add labels to the x-axis and y-axis\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# TODO build same model with pyTorch "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5],\n","  [5.0, -3.0, 1.5]  \n","]\n","\n","# desired targets\n","ys = [1.0, -1.0, -.5]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
