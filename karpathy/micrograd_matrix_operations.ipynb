{"cells":[{"cell_type":"markdown","metadata":{},"source":["### [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import math, random, torch\n","import numpy as np\n","# import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def plot_losses(losses):\n","  # import matplotlib.pyplot as plt\n","  \n","  # Create a list of iterations\n","  iterations = range(len(losses))\n","\n","  # Plot the loss as a function of iteration\n","  plt.plot(iterations, losses)\n","\n","  # Add a title to the plot\n","  plt.title('Loss vs. Iteration')\n","\n","  # Add labels to the x-axis and y-axis\n","  plt.xlabel('Iteration')\n","  plt.ylabel('Loss')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions<br>* limited to neural network with one output, e.g. MLP(2, [3, 1])<br>* neural network with multiple outputs, e.g.  MLP(2, [3, 3]), will produce errors in backward pass "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class Neuron:\n","    \n","    def __init__(self, nin):\n","        # random numbers evenly distributed between -1 and 1    \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]  \n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["### Hidden Layer Matrix Operations<br>* Hidden layer with two inputs (X1, X2), and three neurons (B1, B2, B3)<br>* Two sets of inputs (X1, X2) are shown in different shades of gray<br>* Two sets of neuron outputs (Y1, Y2, Y3) are shown in corresponding shades of gray<br>* Multiple sets of inputs are process with one matrix operation \n","\n","<img src=\"..\\karpathy\\img\\Hidden Layer Matrix Operations_900x506.png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"markdown","metadata":{},"source":["### Parameters in MLP(3, [4, 4, 1])<br>* layer 1: 4 neurons * (3 inputs + 1 bias) = 16<br>* layer 2: 4 neurons * (4 neurons + 1 bias) = 20<br>* layer 2: 1 output * (4 neurons + 1 bias) = 5<br>* Total  41 parameters "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in MLP(3, [4, 4, 1]): 41\n","\n","i:  0,   0.4316316089\n","i:  1,  -0.8140262935\n","i:  2,   0.3963930075\n","i:  3,   0.8966758209\n","i:  4,  -0.0436834798\n","---\n","i: 36,  -0.9100380790\n","i: 37,  -0.7438412678\n","i: 38,  -0.9752773510\n","i: 39,   0.1945947910\n","i: 40,   0.8890396661\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'Number of parameters in MLP(3, [4, 4, 1]): {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### ---- Start: Manual Calculation of Output and Loss with Matrix Multiplication ----"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 3)\n","xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","layer: 0, neuron 0\n","w0:  0.4316316,   w0.grad:  0.0000000\n","w1: -0.8140263,   w1.grad:  0.0000000\n","w2:  0.3963930,   w2.grad:  0.0000000\n","b:   0.8966758\n","\n","layer: 0, neuron 1\n","w0: -0.0436835,   w0.grad:  0.0000000\n","w1: -0.6876896,   w1.grad:  0.0000000\n","w2: -0.7444220,   w2.grad:  0.0000000\n","b:   0.8962330\n","\n","layer: 0, neuron 2\n","w0:  0.9440205,   w0.grad:  0.0000000\n","w1:  0.5950776,   w1.grad:  0.0000000\n","w2:  0.6462173,   w2.grad:  0.0000000\n","b:  -0.8774179\n","\n","layer: 0, neuron 3\n","w0: -0.5754718,   w0.grad:  0.0000000\n","w1: -0.1487363,   w1.grad:  0.0000000\n","w2:  0.8932791,   w2.grad:  0.0000000\n","b:   0.4414870\n","\n","------\n","layer: 1, neuron_cnt: 4\n","----\n","layer: 1, neuron 0\n","w0: -0.0418546,   w0.grad:  0.0000000\n","w1:  0.5411707,   w1.grad:  0.0000000\n","w2: -0.4707173,   w2.grad:  0.0000000\n","w3: -0.0748059,   w3.grad:  0.0000000\n","b:   0.5618427\n","\n","layer: 1, neuron 1\n","w0:  0.3077562,   w0.grad:  0.0000000\n","w1: -0.7182101,   w1.grad:  0.0000000\n","w2:  0.8967297,   w2.grad:  0.0000000\n","w3:  0.9767225,   w3.grad:  0.0000000\n","b:  -0.1191923\n","\n","layer: 1, neuron 2\n","w0:  0.5922629,   w0.grad:  0.0000000\n","w1:  0.0397459,   w1.grad:  0.0000000\n","w2:  0.4803761,   w2.grad:  0.0000000\n","w3: -0.9897075,   w3.grad:  0.0000000\n","b:  -0.7314759\n","\n","layer: 1, neuron 3\n","w0:  0.6869982,   w0.grad:  0.0000000\n","w1:  0.4241768,   w1.grad:  0.0000000\n","w2:  0.9790051,   w2.grad:  0.0000000\n","w3: -0.7445639,   w3.grad:  0.0000000\n","b:   0.5928141\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0: -0.9100381,   w0.grad:  0.0000000\n","w1: -0.7438413,   w1.grad:  0.0000000\n","w2: -0.9752774,   w2.grad:  0.0000000\n","w3:  0.1945948,   w3.grad:  0.0000000\n","b:   0.8890397\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(4, 3):\n","[[ 0.43163161 -0.81402629  0.39639301]\n"," [-0.04368348 -0.68768962 -0.74442205]\n"," [ 0.94402051  0.59507757  0.64621726]\n"," [-0.57547183 -0.14873632  0.8932791 ]]\n","b_mat(1, 4):\n","[[ 0.89667582  0.89623301 -0.87741785  0.44148701]]\n","\n","i: 1\n","w_mat(4, 4):\n","[[-0.04185463  0.54117065 -0.47071735 -0.07480588]\n"," [ 0.30775616 -0.71821012  0.89672968  0.9767225 ]\n"," [ 0.59226286  0.0397459   0.48037612 -0.98970752]\n"," [ 0.68699816  0.42417684  0.97900513 -0.74456389]]\n","b_mat(1, 4):\n","[[ 0.56184271 -0.11919233 -0.73147589  0.59281412]]\n","\n","i: 2\n","w_mat(1, 4):\n","[[-0.91003808 -0.74384127 -0.97527735  0.19459479]]\n","b_mat(1, 1):\n","[[0.88903967]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[ 0.43163161 -0.81402629  0.39639301]\n"," [-0.04368348 -0.68768962 -0.74442205]\n"," [ 0.94402051  0.59507757  0.64621726]\n"," [-0.57547183 -0.14873632  0.8932791 ]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","weights_x_inputs (4, 2):\n","[[-1.97520867  2.30711762]\n"," [-1.40601377  0.18442816]\n"," [ 3.02705648  2.56009258]\n"," [-2.49043172 -1.13103963]]\n","\n","bias (4, 1):\n","[[ 0.89667582]\n"," [ 0.89623301]\n"," [-0.87741785]\n"," [ 0.44148701]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-1.07853285  3.20379345]\n"," [-0.50978076  1.08066116]\n"," [ 2.14963863  1.68267473]\n"," [-2.04894471 -0.68955261]]\n","\n","output (4, 2):\n","[[-0.79265439  0.99670743]\n"," [-0.46977436  0.79344415]\n"," [ 0.97320707  0.93320779]\n"," [-0.96732724 -0.59769451]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[-0.04185463  0.54117065 -0.47071735 -0.07480588]\n"," [ 0.30775616 -0.71821012  0.89672968  0.9767225 ]\n"," [ 0.59226286  0.0397459   0.48037612 -0.98970752]\n"," [ 0.68699816  0.42417684  0.97900513 -0.74456389]]\n","\n","input (4, 2):\n","[[-0.79265439  0.99670743]\n"," [-0.46977436  0.79344415]\n"," [ 0.97320707  0.93320779]\n"," [-0.96732724 -0.59769451]]\n","\n","weights_x_inputs (4, 2):\n","[[-0.60679552 -0.00689416]\n"," [ 0.02134581 -0.01006333]\n"," [ 0.93674511  1.66168244]\n"," [ 0.92919213  2.37993377]]\n","\n","bias (4, 1):\n","[[ 0.56184271]\n"," [-0.11919233]\n"," [-0.73147589]\n"," [ 0.59281412]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-0.04495281  0.55494856]\n"," [-0.09784652 -0.12925565]\n"," [ 0.20526922  0.93020655]\n"," [ 1.52200625  2.97274789]]\n","\n","output (4, 2):\n","[[-0.04492256  0.50421986]\n"," [-0.09753545 -0.12854061]\n"," [ 0.20243396  0.73069018]\n"," [ 0.90904665  0.99477846]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[-0.91003808 -0.74384127 -0.97527735  0.19459479]]\n","\n","input (4, 2):\n","[[-0.04492256  0.50421986]\n"," [-0.09753545 -0.12854061]\n"," [ 0.20243396  0.73069018]\n"," [ 0.90904665  0.99477846]]\n","\n","weights_x_inputs (1, 2):\n","[[ 0.09289862 -0.88229234]]\n","\n","bias (1, 1):\n","[[0.88903967]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[0.98193829 0.00674733]]\n","\n","output (1, 2):\n","[[0.75390375 0.00674722]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [0.75390375 0.00674722]\n","desired output:     [1.0, -1.0]\n","loss:               1.0741033348714268\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"cell_type":"markdown","metadata":{},"source":["### ---- End: Manual Calculation of Output and Loss with Matrix Multiplication ----"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [0.7539037494483588, 0.0067472226611060004]\n","ys:                 [1.0, -1.0]\n","loss_data:          1.0741033348714268\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0      0.4316316089     0.0951784408           0.05000         0.4268726869\n","  1     -0.8140262935     0.1765889067           0.05000        -0.8228557389\n","  2      0.3963930075    -0.0598878551           0.05000         0.3993874002\n","  3      0.8966758209     0.0506638791           0.05000         0.8941426270\n","  4     -0.0436834798     0.3104178396           0.05000        -0.0592043718\n","  5     -0.6876896182    -0.1124676407           0.05000        -0.6820662362\n","  6     -0.7444220477     0.0550072257           0.05000        -0.7471724090\n","  7      0.8962330068     0.1026548834           0.05000         0.8911002627\n","  8      0.9440205079    -0.4166525329           0.05000         0.9648531345\n","  9      0.5950775748     0.1656277234           0.05000         0.5867961886\n"," 10      0.6462172565    -0.0791670146           0.05000         0.6501756073\n"," 11     -0.8774178512    -0.1364529462           0.05000        -0.8705952039\n"," 12     -0.5754718325    -0.8429615451           0.05000        -0.5333237552\n"," 13     -0.1487363172     0.2685271726           0.05000        -0.1621626758\n"," 14      0.8932791036    -0.1359626785           0.05000         0.9000772376\n"," 15      0.4414870146    -0.2821199098           0.05000         0.4555930101\n"," 16     -0.0418546255    -1.5148779955           0.05000         0.0338892743\n"," 17      0.5411706525    -1.1748335539           0.05000         0.5999123302\n"," 18     -0.4707173482    -1.0873997138           0.05000        -0.4163473625\n"," 19     -0.0748058837     0.6300753151           0.05000        -0.1063096494\n"," 20      0.5618427113    -1.1734977923           0.05000         0.6205176009\n"," 21      0.3077561616    -1.5921248538           0.05000         0.3873624043\n"," 22     -0.7182101217    -1.2421992260           0.05000        -0.6561001604\n"," 23      0.8967296821    -1.2222000479           0.05000         0.9578396845\n"," 24      0.9767225045     0.7289405770           0.05000         0.9402754756\n"," 25     -0.1191923260    -1.3163850795           0.05000        -0.0533730720\n"," 26      0.5922628645    -1.0697185413           0.05000         0.6457487915\n"," 27      0.0397458983    -0.8195290177           0.05000         0.0807223492\n"," 28      0.4803761169    -0.6607219238           0.05000         0.5134122131\n"," 29     -0.9897075181     0.3548183929           0.05000        -1.0074484378\n"," 30     -0.7314758888    -0.7165283683           0.05000        -0.6956494704\n"," 31      0.6869981596     0.0097572442           0.05000         0.6865102974\n"," 32      0.4241768389     0.0066100690           0.05000         0.4238463355\n"," 33      0.9790051331    -0.0031775000           0.05000         0.9791640081\n"," 34     -0.7445638857     0.0045044875           0.05000        -0.7447891100\n"," 35      0.5928141234    -0.0030972517           0.05000         0.5929689860\n"," 36     -0.9100380790     1.0247412217           0.05000        -0.9612751401\n"," 37     -0.7438412678    -0.2380831333           0.05000        -0.7319371111\n"," 38     -0.9752773510     1.4281676360           0.05000        -1.0466857328\n"," 39      0.1945947910     1.8097676429           0.05000         0.1041064088\n"," 40      0.8890396661     1.8009581567           0.05000         0.7989917583\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.5965277199403797), Value(data = -0.6091126248711118)]\n","step: 0, loss: 0.31558282081166084\n","-------\n","ypred: [Value(data = 0.7349101759258675), Value(data = -0.6502405004102401)]\n","step: 1, loss: 0.1926043223809338\n","-------\n","ypred: [Value(data = 0.7762463558920084), Value(data = -0.7006304277899231)]\n","step: 2, loss: 0.1396878340168502\n","-------\n","ypred: [Value(data = 0.8040239152731201), Value(data = -0.7349750163186035)]\n","step: 3, loss: 0.10864486776020169\n","-------\n","ypred: [Value(data = 0.824118012320584), Value(data = -0.7602003396200975)]\n","step: 4, loss: 0.08843835070837883\n","-------\n","ypred: [Value(data = 0.8394365464252886), Value(data = -0.7796823700266584)]\n","step: 5, loss: 0.07432048070090876\n","-------\n","ypred: [Value(data = 0.8515710643638137), Value(data = -0.7952841324631486)]\n","step: 6, loss: 0.06393973535545686\n","-------\n","ypred: [Value(data = 0.8614671202133406), Value(data = -0.8081253311332653)]\n","step: 7, loss: 0.056007247334704094\n","-------\n","ypred: [Value(data = 0.8697234533129306), Value(data = -0.8189231336750982)]\n","step: 8, loss: 0.04976081013475456\n","-------\n","ypred: [Value(data = 0.8767387561682288), Value(data = -0.8281601569180415)]\n","step: 9, loss: 0.04472226590138748\n","-------\n","ypred: [Value(data = 0.8827894961437719), Value(data = -0.8361742726584839)]\n","step: 10, loss: 0.04057717115320765\n","-------\n","ypred: [Value(data = 0.8880738764947739), Value(data = -0.8432097953405157)]\n","step: 11, loss: 0.03711062540007012\n","-------\n","ypred: [Value(data = 0.8927379990729415), Value(data = -0.8494482160805733)]\n","step: 12, loss: 0.03417097648419805\n","-------\n","ypred: [Value(data = 0.8968921279297415), Value(data = -0.8550274690628704)]\n","step: 13, loss: 0.03164826800917379\n","-------\n","ypred: [Value(data = 0.9006211791954329), Value(data = -0.8600544561927164)]\n","step: 14, loss: 0.029460905256022595\n","-------\n","ypred: [Value(data = 0.9039917042660375), Value(data = -0.8646134459549707)]\n","step: 15, loss: 0.027547111865927648\n","-------\n","ypred: [Value(data = 0.9070566676259031), Value(data = -0.8687718588843961)]\n","step: 16, loss: 0.025859288053458697\n","-------\n","ypred: [Value(data = 0.9098587950077921), Value(data = -0.8725843474177903)]\n","step: 17, loss: 0.024360185360397597\n","-------\n","ypred: [Value(data = 0.912432969754522), Value(data = -0.876095733042666)]\n","step: 18, loss: 0.02302025215624675\n","-------\n","ypred: [Value(data = 0.9148079804262028), Value(data = -0.8793431599540625)]\n","step: 19, loss: 0.02181575324893318\n","-------\n","ypred: [Value(data = 0.9170078168397967), Value(data = -0.8823577003331985)]\n","step: 20, loss: 0.020727413136590257\n","-------\n","ypred: [Value(data = 0.9190526458292098), Value(data = -0.8851655686595513)]\n","step: 21, loss: 0.019739420768535576\n","-------\n","ypred: [Value(data = 0.920959555961342), Value(data = -0.8877890526092375)]\n","step: 22, loss: 0.0188386885081607\n","-------\n","ypred: [Value(data = 0.9227431330020888), Value(data = -0.8902472353870505)]\n","step: 23, loss: 0.01801429283851844\n","-------\n","ypred: [Value(data = 0.9244159096540498), Value(data = -0.8925565624539937)]\n","step: 24, loss: 0.017257046985127304\n","-------\n","ypred: [Value(data = 0.9259887206952925), Value(data = -0.8947312906992503)]\n","step: 25, loss: 0.01655917062216518\n","-------\n","ypred: [Value(data = 0.9274709861016481), Value(data = -0.8967838477743326)]\n","step: 26, loss: 0.015914031937339458\n","-------\n","ypred: [Value(data = 0.9288709387489528), Value(data = -0.8987251220406565)]\n","step: 27, loss: 0.015315944260135154\n","-------\n","ypred: [Value(data = 0.9301958090393218), Value(data = -0.9005646983990241)]\n","step: 28, loss: 0.014760004280151878\n","-------\n","ypred: [Value(data = 0.9314519757350636), Value(data = -0.902311051525059)]\n","step: 29, loss: 0.014241962284766\n","-------\n","ypred: [Value(data = 0.9326450900543), Value(data = -0.9039717052968191)]\n","step: 30, loss: 0.013758117277394316\n","-------\n","ypred: [Value(data = 0.9337801784397908), Value(data = -0.9055533651767705)]\n","step: 31, loss: 0.01330523159689841\n","-------\n","ypred: [Value(data = 0.9348617281895648), Value(data = -0.9070620287983652)]\n","step: 32, loss: 0.01288046094552604\n","-------\n","ypred: [Value(data = 0.9358937592185894), Value(data = -0.9085030788671702)]\n","step: 33, loss: 0.012481296683911468\n","-------\n","ypred: [Value(data = 0.9368798845231353), Value(data = -0.9098813616186683)]\n","step: 34, loss: 0.012105517961517967\n","-------\n","ypred: [Value(data = 0.9378233613856847), Value(data = -0.9112012534088091)]\n","step: 35, loss: 0.011751151785541708\n","-------\n","ypred: [Value(data = 0.9387271349463951), Value(data = -0.9124667174981068)]\n","step: 36, loss: 0.01141643953743351\n","-------\n","ypred: [Value(data = 0.9395938754470069), Value(data = -0.9136813526884654)]\n","step: 37, loss: 0.011099808757204815\n","-------\n","ypred: [Value(data = 0.9404260102024277), Value(data = -0.9148484351566489)]\n","step: 38, loss: 0.010799849255672674\n","-------\n","ypred: [Value(data = 0.9412257511576713), Value(data = -0.9159709545792064)]\n","step: 39, loss: 0.010515292801309774\n","-------\n","ypred: [Value(data = 0.9419951187310803), Value(data = -0.9170516454455521)]\n","step: 40, loss: 0.010244995774311871\n","-------\n","ypred: [Value(data = 0.9427359625196701), Value(data = -0.9180930142974385)]\n","step: 41, loss: 0.009987924295428245\n","-------\n","ypred: [Value(data = 0.9434499793420084), Value(data = -0.9190973635055519)]\n","step: 42, loss: 0.009743141428172081\n","-------\n","ypred: [Value(data = 0.9441387290129915), Value(data = -0.920066812090838)]\n","step: 43, loss: 0.009509796125605412\n","-------\n","ypred: [Value(data = 0.9448036481791056), Value(data = -0.9210033140143179)]\n","step: 44, loss: 0.00928711365105642\n","-------\n","ypred: [Value(data = 0.945446062489142), Value(data = -0.9219086742906422)]\n","step: 45, loss: 0.009074387248983615\n","-------\n","ypred: [Value(data = 0.9460671973313984), Value(data = -0.922784563224408)]\n","step: 46, loss: 0.008870970880135755\n","-------\n","ypred: [Value(data = 0.9466681873322422), Value(data = -0.9236325290218951)]\n","step: 47, loss: 0.008676272866020506\n","-------\n","ypred: [Value(data = 0.9472500847810292), Value(data = -0.9244540089924859)]\n","step: 48, loss: 0.00848975031291601\n","-------\n","ypred: [Value(data = 0.9478138671215666), Value(data = -0.9252503395221273)]\n","step: 49, loss: 0.008310904206362752\n","-------\n","ypred: [Value(data = 0.9483604436296453), Value(data = -0.9260227649745388)]\n","step: 50, loss: 0.008139275084139357\n","-------\n","ypred: [Value(data = 0.9488906613788802), Value(data = -0.9267724456535671)]\n","step: 51, loss: 0.00797443920984807\n","-------\n","ypred: [Value(data = 0.9494053105825856), Value(data = -0.927500464941314)]\n","step: 52, loss: 0.007816005180970267\n","-------\n","ypred: [Value(data = 0.9499051293871944), Value(data = -0.9282078357108425)]\n","step: 53, loss: 0.007663610915035111\n","-------\n","ypred: [Value(data = 0.9503908081823946), Value(data = -0.9288955060988541)]\n","step: 54, loss: 0.007516920965734062\n","-------\n","ypred: [Value(data = 0.9508629934843984), Value(data = -0.9295643647123647)]\n","step: 55, loss: 0.007375624127687049\n","-------\n","ypred: [Value(data = 0.9513222914413078), Value(data = -0.9302152453337034)]\n","step: 56, loss: 0.00723943129436019\n","-------\n","ypred: [Value(data = 0.9517692710031859), Value(data = -0.9308489311798829)]\n","step: 57, loss: 0.007108073538528702\n","-------\n","ypred: [Value(data = 0.9522044667940008), Value(data = -0.9314661587652968)]\n","step: 58, loss: 0.006981300388829272\n","-------\n","ypred: [Value(data = 0.9526283817179468), Value(data = -0.932067621410602)]\n","step: 59, loss: 0.0068588782794738595\n","-------\n","ypred: [Value(data = 0.9530414893286298), Value(data = -0.932653972435399)]\n","step: 60, loss: 0.006740589153205185\n","-------\n","ypred: [Value(data = 0.9534442359861448), Value(data = -0.9332258280677856)]\n","step: 61, loss: 0.006626229200146697\n","-------\n","ypred: [Value(data = 0.9538370428240789), Value(data = -0.9337837700999373)]\n","step: 62, loss: 0.006515607717403885\n","-------\n","ypred: [Value(data = 0.954220307545876), Value(data = -0.9343283483154655)]\n","step: 63, loss: 0.006408546076169004\n","-------\n","ypred: [Value(data = 0.9545944060677495), Value(data = -0.9348600827113336)]\n","step: 64, loss: 0.006304876784714726\n","-------\n","ypred: [Value(data = 0.9549596940233631), Value(data = -0.9353794655345413)]\n","step: 65, loss: 0.006204442637070606\n","-------\n","ypred: [Value(data = 0.955316508143788), Value(data = -0.9358869631515216)]\n","step: 66, loss: 0.006107095938398508\n","-------\n","ypred: [Value(data = 0.9556651675247462), Value(data = -0.936383017766223)]\n","step: 67, loss: 0.006012697799141518\n","-------\n","ypred: [Value(data = 0.9560059747918396), Value(data = -0.9368680490011237)]\n","step: 68, loss: 0.005921117490940767\n","-------\n","ypred: [Value(data = 0.9563392171733068), Value(data = -0.9373424553538905)]\n","step: 69, loss: 0.005832231858118871\n","-------\n","ypred: [Value(data = 0.9566651674888403), Value(data = -0.9378066155410628)]\n","step: 70, loss: 0.0057459247792274295\n","-------\n","ypred: [Value(data = 0.9569840850621011), Value(data = -0.9382608897389582)]\n","step: 71, loss: 0.005662086673769637\n","-------\n","ypred: [Value(data = 0.9572962165637765), Value(data = -0.9387056207309452)]\n","step: 72, loss: 0.00558061404974661\n","-------\n","ypred: [Value(data = 0.9576017967913331), Value(data = -0.9391411349693088)]\n","step: 73, loss: 0.005501409088147299\n","-------\n","ypred: [Value(data = 0.9579010493909935), Value(data = -0.9395677435591109)]\n","step: 74, loss: 0.005424379260916952\n","-------\n","ypred: [Value(data = 0.9581941875269187), Value(data = -0.9399857431707154)]\n","step: 75, loss: 0.0053494369793057805\n","-------\n","ypred: [Value(data = 0.9584814145020901), Value(data = -0.9403954168870053)]\n","step: 76, loss: 0.00527649926982114\n","-------\n","ypred: [Value(data = 0.958762924334949), Value(data = -0.9407970349907326)]\n","step: 77, loss: 0.005205487475293674\n","-------\n","ypred: [Value(data = 0.9590389022954628), Value(data = -0.9411908556969323)]\n","step: 78, loss: 0.005136326978819682\n","-------\n","ypred: [Value(data = 0.9593095254039452), Value(data = -0.9415771258348637)]\n","step: 79, loss: 0.005068946948567525\n","-------\n","ypred: [Value(data = 0.9595749628956407), Value(data = -0.9419560814835366)]\n","step: 80, loss: 0.005003280101634668\n","-------\n","ypred: [Value(data = 0.959835376653814), Value(data = -0.9423279485644979)]\n","step: 81, loss: 0.004939262485320192\n","-------\n","ypred: [Value(data = 0.9600909216138295), Value(data = -0.9426929433952386)]\n","step: 82, loss: 0.004876833274334834\n","-------\n","ypred: [Value(data = 0.9603417461404858), Value(data = -0.9430512732062621)]\n","step: 83, loss: 0.004815934582613465\n","-------\n","ypred: [Value(data = 0.9605879923806693), Value(data = -0.9434031366246021)]\n","step: 84, loss: 0.004756511288519643\n","-------\n","ypred: [Value(data = 0.9608297965932053), Value(data = -0.9437487241263263)]\n","step: 85, loss: 0.0046985108723458175\n","-------\n","ypred: [Value(data = 0.9610672894576304), Value(data = -0.9440882184603474)]\n","step: 86, loss: 0.0046418832651137715\n","-------\n","ypred: [Value(data = 0.9613005963634519), Value(data = -0.9444217950456638)]\n","step: 87, loss: 0.004586580707770673\n","-------\n","ypred: [Value(data = 0.961529837681335), Value(data = -0.944749622343972)]\n","step: 88, loss: 0.00453255761995815\n","-------\n","ypred: [Value(data = 0.9617551290175335), Value(data = -0.9450718622094354)]\n","step: 89, loss: 0.004479770477604762\n","-------\n","ypred: [Value(data = 0.961976581452772), Value(data = -0.9453886702172402)]\n","step: 90, loss: 0.004428177698659035\n","-------\n","ypred: [Value(data = 0.9621943017666876), Value(data = -0.9457001959724433)]\n","step: 91, loss: 0.004377739536339345\n","-------\n","ypred: [Value(data = 0.9624083926488507), Value(data = -0.9460065834004903)]\n","step: 92, loss: 0.004328417979331195\n","-------\n","ypred: [Value(data = 0.9626189528972992), Value(data = -0.9463079710206735)]\n","step: 93, loss: 0.004280176658411171\n","-------\n","ypred: [Value(data = 0.9628260776054508), Value(data = -0.9466044922037014)]\n","step: 94, loss: 0.0042329807590205555\n","-------\n","ypred: [Value(data = 0.9630298583381859), Value(data = -0.9468962754144553)]\n","step: 95, loss: 0.004186796939351986\n","-------\n","ypred: [Value(data = 0.963230383297833), Value(data = -0.9471834444409295)]\n","step: 96, loss: 0.004141593253548664\n","-------\n","ypred: [Value(data = 0.963427737480735), Value(data = -0.947466118610273)]\n","step: 97, loss: 0.004097339079647947\n","-------\n","ypred: [Value(data = 0.9636220028250171), Value(data = -0.9477444129927829)]\n","step: 98, loss: 0.004054005051931902\n","-------\n","ypred: [Value(data = 0.9638132583501369), Value(data = -0.9480184385946303)]\n","step: 99, loss: 0.004011562997374159\n","-------\n","ypred: [Value(data = 0.9640015802887476), Value(data = -0.9482883025400525)]\n","step: 100, loss: 0.003969985875896628\n","-------\n","ypred: [Value(data = 0.9641870422113712), Value(data = -0.948554108243677)]\n","step: 101, loss: 0.003929247724173409\n","-------\n","ypred: [Value(data = 0.9643697151443373), Value(data = -0.9488159555736086)]\n","step: 102, loss: 0.0038893236027384677\n","-------\n","ypred: [Value(data = 0.9645496676814148), Value(data = -0.9490739410058524)]\n","step: 103, loss: 0.003850189546173527\n","-------\n","ypred: [Value(data = 0.9647269660895281), Value(data = -0.9493281577706145)]\n","step: 104, loss: 0.003811822516169032\n","-------\n","ypred: [Value(data = 0.964901674408927), Value(data = -0.9495786959909793)]\n","step: 105, loss: 0.003774200357267051\n","-------\n","ypred: [Value(data = 0.965073854548147), Value(data = -0.9498256428144259)]\n","step: 106, loss: 0.003737301755109561\n","-------\n","ypred: [Value(data = 0.9652435663740787), Value(data = -0.9500690825376173)]\n","step: 107, loss: 0.0037011061970283407\n","-------\n","ypred: [Value(data = 0.965410867797441), Value(data = -0.950309096724866)]\n","step: 108, loss: 0.00366559393482483\n","-------\n","ypred: [Value(data = 0.9655758148539308), Value(data = -0.9505457643206472)]\n","step: 109, loss: 0.00363074594959982\n","-------\n","ypred: [Value(data = 0.9657384617813053), Value(data = -0.9507791617565129)]\n","step: 110, loss: 0.003596543918502601\n","-------\n","ypred: [Value(data = 0.9658988610926359), Value(data = -0.9510093630527312)]\n","step: 111, loss: 0.00356297018327844\n","-------\n","ypred: [Value(data = 0.966057063645956), Value(data = -0.9512364399149539)]\n","step: 112, loss: 0.0035300077205025846\n","-------\n","ypred: [Value(data = 0.9662131187105087), Value(data = -0.9514604618262011)]\n","step: 113, loss: 0.0034976401133958506\n","-------\n","ypred: [Value(data = 0.9663670740297899), Value(data = -0.9516814961344228)]\n","step: 114, loss: 0.0034658515251254325\n","-------\n","ypred: [Value(data = 0.9665189758815691), Value(data = -0.9518996081358893)]\n","step: 115, loss: 0.0034346266734999634\n","-------\n","ypred: [Value(data = 0.9666688691350538), Value(data = -0.9521148611546439)]\n","step: 116, loss: 0.003403950806975198\n","-------\n","ypred: [Value(data = 0.9668167973053607), Value(data = -0.9523273166182338)]\n","step: 117, loss: 0.0033738096818916436\n","-------\n","ypred: [Value(data = 0.9669628026054392), Value(data = -0.9525370341299209)]\n","step: 118, loss: 0.0033441895408714718\n","-------\n","ypred: [Value(data = 0.9671069259955876), Value(data = -0.9527440715375705)]\n","step: 119, loss: 0.0033150770923060114\n","-------\n","ypred: [Value(data = 0.9672492072306925), Value(data = -0.95294848499939)]\n","step: 120, loss: 0.003286459490870751\n","-------\n","ypred: [Value(data = 0.9673896849053145), Value(data = -0.9531503290466901)]\n","step: 121, loss: 0.003258324319008085\n","-------\n","ypred: [Value(data = 0.9675283964967335), Value(data = -0.9533496566438252)]\n","step: 122, loss: 0.003230659569322351\n","-------\n","ypred: [Value(data = 0.9676653784060635), Value(data = -0.9535465192454615)]\n","step: 123, loss: 0.003203453627835343\n","-------\n","ypred: [Value(data = 0.9678006659975363), Value(data = -0.953740966851312)]\n","step: 124, loss: 0.003176695258053624\n","-------\n","ypred: [Value(data = 0.9679342936360525), Value(data = -0.9539330480584706)]\n","step: 125, loss: 0.0031503735858020828\n","-------\n","ypred: [Value(data = 0.968066294723085), Value(data = -0.9541228101114662)]\n","step: 126, loss: 0.0031244780847814587\n","-------\n","ypred: [Value(data = 0.9681967017310235), Value(data = -0.9543102989501577)]\n","step: 127, loss: 0.003098998562809444\n","-------\n","ypred: [Value(data = 0.9683255462360372), Value(data = -0.9544955592555731)]\n","step: 128, loss: 0.003073925148708482\n","-------\n","ypred: [Value(data = 0.9684528589495311), Value(data = -0.9546786344937997)]\n","step: 129, loss: 0.003049248279804775\n","-------\n","ypred: [Value(data = 0.9685786697482665), Value(data = -0.9548595669580238)]\n","step: 130, loss: 0.003024958690005646\n","-------\n","ypred: [Value(data = 0.9687030077032107), Value(data = -0.9550383978088061)]\n","step: 131, loss: 0.0030010473984244633\n","-------\n","ypred: [Value(data = 0.9688259011071821), Value(data = -0.9552151671126847)]\n","step: 132, loss: 0.002977505698523943\n","-------\n","ypred: [Value(data = 0.9689473775013455), Value(data = -0.955389913879185)]\n","step: 133, loss: 0.00295432514775047\n","-------\n","ypred: [Value(data = 0.9690674637006155), Value(data = -0.955562676096312)]\n","step: 134, loss: 0.002931497557634026\n","-------\n","ypred: [Value(data = 0.9691861858180224), Value(data = -0.9557334907646006)]\n","step: 135, loss: 0.0029090149843291465\n","-------\n","ypred: [Value(data = 0.9693035692880865), Value(data = -0.9559023939297915)]\n","step: 136, loss: 0.002886869719574599\n","-------\n","ypred: [Value(data = 0.969419638889251), Value(data = -0.9560694207141965)]\n","step: 137, loss: 0.0028650542820500753\n","-------\n","ypred: [Value(data = 0.9695344187654171), Value(data = -0.9562346053468171)]\n","step: 138, loss: 0.0028435614091098143\n","-------\n","ypred: [Value(data = 0.9696479324466228), Value(data = -0.9563979811922719)]\n","step: 139, loss: 0.002822384048874243\n","-------\n","ypred: [Value(data = 0.969760202868906), Value(data = -0.9565595807785905)]\n","step: 140, loss: 0.002801515352661528\n","-------\n","ypred: [Value(data = 0.969871252393389), Value(data = -0.9567194358239235)]\n","step: 141, loss: 0.002780948667742345\n","-------\n","ypred: [Value(data = 0.9699811028246214), Value(data = -0.9568775772622213)]\n","step: 142, loss: 0.0027606775304016462\n","-------\n","ypred: [Value(data = 0.9700897754282126), Value(data = -0.9570340352679236)]\n","step: 143, loss: 0.002740695659292784\n","-------\n","ypred: [Value(data = 0.97019729094779), Value(data = -0.9571888392797105)]\n","step: 144, loss: 0.0027209969490691378\n","-------\n","ypred: [Value(data = 0.9703036696213083), Value(data = -0.957342018023351)]\n","step: 145, loss: 0.0027015754642805215\n","-------\n","ypred: [Value(data = 0.9704089311967443), Value(data = -0.9574935995336922)]\n","step: 146, loss: 0.0026824254335211456\n","-------\n","ypred: [Value(data = 0.9705130949471995), Value(data = -0.9576436111758283)]\n","step: 147, loss: 0.0026635412438172886\n","-------\n","ypred: [Value(data = 0.9706161796854396), Value(data = -0.9577920796654815)]\n","step: 148, loss: 0.002644917435243433\n","-------\n","ypred: [Value(data = 0.9707182037778951), Value(data = -0.9579390310886327)]\n","step: 149, loss: 0.002626548695755888\n","-------\n","ypred: [Value(data = 0.9708191851581455), Value(data = -0.958084490920432)]\n","step: 150, loss: 0.0026084298562339433\n","-------\n","ypred: [Value(data = 0.9709191413399099), Value(data = -0.9582284840434211)]\n","step: 151, loss: 0.002590555885718866\n","-------\n","ypred: [Value(data = 0.9710180894295659), Value(data = -0.9583710347650986)]\n","step: 152, loss: 0.002572921886841273\n","-------\n","ypred: [Value(data = 0.9711160461382154), Value(data = -0.9585121668348503)]\n","step: 153, loss: 0.0025555230914289942\n","-------\n","ypred: [Value(data = 0.9712130277933176), Value(data = -0.95865190346028)]\n","step: 154, loss: 0.0025383548562863114\n","-------\n","ypred: [Value(data = 0.9713090503499076), Value(data = -0.9587902673229571)]\n","step: 155, loss: 0.0025214126591374744\n","-------\n","ypred: [Value(data = 0.9714041294014165), Value(data = -0.9589272805936113)]\n","step: 156, loss: 0.002504692094726869\n","-------\n","ypred: [Value(data = 0.971498280190112), Value(data = -0.9590629649467952)]\n","step: 157, loss: 0.002488188871068681\n","-------\n","ypred: [Value(data = 0.9715915176171716), Value(data = -0.9591973415750357)]\n","step: 158, loss: 0.0024718988058397874\n","-------\n","ypred: [Value(data = 0.9716838562524074), Value(data = -0.9593304312024967)]\n","step: 159, loss: 0.0024558178229091804\n","-------\n","ypred: [Value(data = 0.9717753103436543), Value(data = -0.9594622540981722)]\n","step: 160, loss: 0.0024399419489981857\n","-------\n","ypred: [Value(data = 0.9718658938258351), Value(data = -0.9595928300886284)]\n","step: 161, loss: 0.0024242673104656384\n","-------\n","ypred: [Value(data = 0.9719556203297173), Value(data = -0.9597221785703139)]\n","step: 162, loss: 0.0024087901302126445\n","-------\n","ypred: [Value(data = 0.972044503190372), Value(data = -0.9598503185214539)]\n","step: 163, loss: 0.002393506724701835\n","-------\n","ypred: [Value(data = 0.9721325554553478), Value(data = -0.959977268513546)]\n","step: 164, loss: 0.002378413501086058\n","-------\n","ypred: [Value(data = 0.9722197898925715), Value(data = -0.960103046722472)]\n","step: 165, loss: 0.0023635069544421204\n","-------\n","ypred: [Value(data = 0.9723062189979843), Value(data = -0.960227670939243)]\n","step: 166, loss: 0.002348783665104742\n","-------\n","ypred: [Value(data = 0.9723918550029262), Value(data = -0.9603511585803886)]\n","step: 167, loss: 0.002334240296096945\n","-------\n","ypred: [Value(data = 0.9724767098812769), Value(data = -0.9604735266980079)]\n","step: 168, loss: 0.002319873590652496\n","-------\n","ypred: [Value(data = 0.9725607953563628), Value(data = -0.9605947919894915)]\n","step: 169, loss: 0.002305680369826848\n","-------\n","ypred: [Value(data = 0.9726441229076401), Value(data = -0.9607149708069306)]\n","step: 170, loss: 0.0022916575301926163\n","-------\n","ypred: [Value(data = 0.972726703777161), Value(data = -0.9608340791662217)]\n","step: 171, loss: 0.002277802041616518\n","-------\n","ypred: [Value(data = 0.972808548975832), Value(data = -0.9609521327558813)]\n","step: 172, loss: 0.0022641109451140428\n","-------\n","ypred: [Value(data = 0.9728896692894727), Value(data = -0.9610691469455797)]\n","step: 173, loss: 0.0022505813507790275\n","-------\n","ypred: [Value(data = 0.9729700752846812), Value(data = -0.9611851367944053)]\n","step: 174, loss: 0.0022372104357848326\n","-------\n","ypred: [Value(data = 0.973049777314515), Value(data = -0.9613001170588678)]\n","step: 175, loss: 0.0022239954424545672\n","-------\n","ypred: [Value(data = 0.9731287855239926), Value(data = -0.9614141022006536)]\n","step: 176, loss: 0.002210933676397194\n","-------\n","ypred: [Value(data = 0.9732071098554237), Value(data = -0.9615271063941369)]\n","step: 177, loss: 0.0021980225047073996\n","-------\n","ypred: [Value(data = 0.9732847600535732), Value(data = -0.9616391435336616)]\n","step: 178, loss: 0.0021852593542261703\n","-------\n","ypred: [Value(data = 0.9733617456706666), Value(data = -0.9617502272405972)]\n","step: 179, loss: 0.0021726417098602013\n","-------\n","ypred: [Value(data = 0.9734380760712398), Value(data = -0.9618603708701801)]\n","step: 180, loss: 0.0021601671129574548\n","-------\n","ypred: [Value(data = 0.9735137604368431), Value(data = -0.9619695875181458)]\n","step: 181, loss: 0.0021478331597369112\n","-------\n","ypred: [Value(data = 0.9735888077706003), Value(data = -0.962077890027162)]\n","step: 182, loss: 0.002135637499770319\n","-------\n","ypred: [Value(data = 0.9736632269016301), Value(data = -0.9621852909930664)]\n","step: 183, loss: 0.0021235778345140874\n","-------\n","ypred: [Value(data = 0.9737370264893361), Value(data = -0.9622918027709197)]\n","step: 184, loss: 0.0021116519158890477\n","-------\n","ypred: [Value(data = 0.9738102150275666), Value(data = -0.9623974374808771)]\n","step: 185, loss: 0.002099857544906842\n","-------\n","ypred: [Value(data = 0.973882800848652), Value(data = -0.9625022070138866)]\n","step: 186, loss: 0.0020881925703405925\n","-------\n","ypred: [Value(data = 0.973954792127322), Value(data = -0.9626061230372188)]\n","step: 187, loss: 0.0020766548874386224\n","-------\n","ypred: [Value(data = 0.974026196884508), Value(data = -0.9627091969998368)]\n","step: 188, loss: 0.002065242436679323\n","-------\n","ypred: [Value(data = 0.9740970229910337), Value(data = -0.9628114401376074)]\n","step: 189, loss: 0.0020539532025657895\n","-------\n","ypred: [Value(data = 0.974167278171199), Value(data = -0.9629128634783615)]\n","step: 190, loss: 0.002042785212458868\n","-------\n","ypred: [Value(data = 0.974236970006259), Value(data = -0.9630134778468104)]\n","step: 191, loss: 0.0020317365354467886\n","-------\n","ypred: [Value(data = 0.9743061059378035), Value(data = -0.9631132938693193)]\n","step: 192, loss: 0.0020208052812505707\n","-------\n","ypred: [Value(data = 0.9743746932710401), Value(data = -0.9632123219785453)]\n","step: 193, loss: 0.0020099895991634964\n","-------\n","ypred: [Value(data = 0.9744427391779817), Value(data = -0.9633105724179442)]\n","step: 194, loss: 0.0019992876770235897\n","-------\n","ypred: [Value(data = 0.974510250700546), Value(data = -0.9634080552461484)]\n","step: 195, loss: 0.001988697740217947\n","-------\n","ypred: [Value(data = 0.9745772347535655), Value(data = -0.9635047803412239)]\n","step: 196, loss: 0.001978218050717636\n","-------\n","ypred: [Value(data = 0.9746436981277146), Value(data = -0.963600757404807)]\n","step: 197, loss: 0.0019678469061421787\n","-------\n","ypred: [Value(data = 0.9747096474923532), Value(data = -0.9636959959661258)]\n","step: 198, loss: 0.001957582638852592\n","-------\n","ypred: [Value(data = 0.9747750893982929), Value(data = -0.9637905053859116)]\n","step: 199, loss: 0.0019474236150718103\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMk0lEQVR4nO3deXxU9b3/8ffMJJOFbEBIQjASNkW2oAHSKFslEigqVK1A/RVMrQsulRu30lagam8AlYsWCi5F0KpFutjW2ihEoC4RZNOKiohgwpJAwCQkgSwz5/dHMgfGBAghmTNJXs/HYx4zc873nPmcOZJ5+z3fc47NMAxDAAAA7Yjd6gIAAAB8jQAEAADaHQIQAABodwhAAACg3SEAAQCAdocABAAA2h0CEAAAaHcIQAAAoN0hAAEAgHaHAAQArcT69etls9m0fv16q0sBWj0CENCGrVixQjabTZs3b7a6FL/T0Hfz5ptvau7cudYVVef3v/+9VqxYYXUZQJtGAAKAOm+++aZ+85vfWF3GaQPQyJEjdfz4cY0cOdL3RQFtDAEIAFqQYRg6fvx4s6zLbrcrODhYdjt/uoHzxb8iANq2bZvGjx+viIgIhYWFacyYMfrwww+92lRXV+s3v/mN+vTpo+DgYHXu3FnDhw/XmjVrzDYFBQXKyMjQBRdcoKCgIHXt2lUTJ07U3r17T/vZTzzxhGw2m7755pt682bNmiWn06lvv/1WkrRr1y5df/31iouLU3BwsC644AJNmTJFJSUl5/0d3HzzzVqyZIkkyWazmQ8Pt9utRYsWqX///goODlZsbKxuv/12szaPxMREXX311Xrrrbc0ZMgQhYSE6JlnnpEkvfDCC7ryyisVExOjoKAg9evXT0uXLq23/I4dO7RhwwazhtGjR0s6/Rig1atXKzk5WSEhIYqOjtb/+3//T/v376+3fWFhYdq/f78mTZqksLAwdenSRffff79cLtd5f39AaxNgdQEArLVjxw6NGDFCERERevDBBxUYGKhnnnlGo0eP1oYNG5SSkiJJmjt3rrKysvSzn/1Mw4YNU2lpqTZv3qytW7fqqquukiRdf/312rFjh+655x4lJibq0KFDWrNmjfLy8pSYmNjg599444168MEH9dprr+mBBx7wmvfaa69p7Nix6tixo6qqqpSenq7Kykrdc889iouL0/79+/XGG2+ouLhYkZGR5/U93H777Tpw4IDWrFmjl156qcH5K1asUEZGhn7+859rz549Wrx4sbZt26b3339fgYGBZtudO3dq6tSpuv3223Xrrbfq4osvliQtXbpU/fv317XXXquAgAD985//1J133im326277rpLkrRo0SLdc889CgsL069+9StJUmxs7Gnr9tQ0dOhQZWVlqbCwUE899ZTef/99bdu2TVFRUWZbl8ul9PR0paSk6IknntDatWv15JNPqlevXpoxY8Z5fX9Aq2MAaLNeeOEFQ5Lx0UcfnbbNpEmTDKfTaezevducduDAASM8PNwYOXKkOS0pKcmYMGHCadfz7bffGpKMxx9//JzrTE1NNZKTk72mbdq0yZBkvPjii4ZhGMa2bdsMScbq1avPef0Naei7ueuuu4yG/iy+++67hiTj5Zdf9pqenZ1db3r37t0NSUZ2dna99VRUVNSblp6ebvTs2dNrWv/+/Y1Ro0bVa7tu3TpDkrFu3TrDMAyjqqrKiImJMQYMGGAcP37cbPfGG28YkozZs2eb06ZPn25IMh555BGvdV566aX1vnugPeAQGNCOuVwuvf3225o0aZJ69uxpTu/atat+/OMf67333lNpaakkKSoqSjt27NCuXbsaXFdISIicTqfWr19f77DQ2UyePFlbtmzR7t27zWmrVq1SUFCQJk6cKElmD89bb72lioqKc1r/+Vq9erUiIyN11VVXqaioyHwkJycrLCxM69at82rfo0cPpaen11tPSEiI+bqkpERFRUUaNWqUvv766yYdxtu8ebMOHTqkO++8U8HBweb0CRMmqG/fvvrXv/5Vb5k77rjD6/2IESP09ddfn/NnA60dAQhoxw4fPqyKigrzEM2pLrnkErndbuXn50uSHnnkERUXF+uiiy7SwIED9cADD+iTTz4x2wcFBWn+/Pn697//rdjYWI0cOVILFixQQUHBWev40Y9+JLvdrlWrVkmqHTi8evVqc1ySVBsqMjMz9fzzzys6Olrp6elasmRJs4z/OZtdu3appKREMTEx6tKli9ejrKxMhw4d8mrfo0ePBtfz/vvvKy0tTR06dFBUVJS6dOmiX/7yl5LUpO3wjJtqaP/17du33riq4OBgdenSxWtax44dzzmwAm0BAQhAo4wcOVK7d+/W8uXLNWDAAD3//PO67LLL9Pzzz5ttZs6cqS+//FJZWVkKDg7Www8/rEsuuUTbtm0747rj4+M1YsQIvfbaa5KkDz/8UHl5eZo8ebJXuyeffFKffPKJfvnLX+r48eP6+c9/rv79+2vfvn3Nv8GncLvdiomJ0Zo1axp8PPLII17tT+3p8di9e7fGjBmjoqIiLVy4UP/617+0Zs0a/c///I/5GS3N4XC0+GcArQUBCGjHunTpotDQUO3cubPevC+++EJ2u10JCQnmtE6dOikjI0Ovvvqq8vPzNWjQoHoXDuzVq5fuu+8+vf322/r0009VVVWlJ5988qy1TJ48WR9//LF27typVatWKTQ0VNdcc029dgMHDtSvf/1r/ec//9G7776r/fv3a9myZee+8Q049ayvU/Xq1UtHjhzRFVdcobS0tHqPpKSks677n//8pyorK/WPf/xDt99+u37wgx8oLS2twbB0ujq+q3v37pLU4P7buXOnOR9AfQQgoB1zOBwaO3as/v73v3udql5YWKhXXnlFw4cPNw9BHTlyxGvZsLAw9e7dW5WVlZKkiooKnThxwqtNr169FB4ebrY5k+uvv14Oh0OvvvqqVq9erauvvlodOnQw55eWlqqmpsZrmYEDB8put3utPy8vT1988UXjvoDv8HxecXGx1/Qbb7xRLpdLjz76aL1lampq6rVviKf3xTAMc1pJSYleeOGFButozDqHDBmimJgYLVu2zOs7+Pe//63PP/9cEyZMOOs6gPaK0+CBdmD58uXKzs6uN/3ee+/VY489pjVr1mj48OG68847FRAQoGeeeUaVlZVasGCB2bZfv34aPXq0kpOT1alTJ23evFl//vOfdffdd0uSvvzyS40ZM0Y33nij+vXrp4CAAP3tb39TYWGhpkyZctYaY2Ji9P3vf18LFy7UsWPH6h3+euedd3T33XfrRz/6kS666CLV1NTopZdeksPh0PXXX2+2mzZtmjZs2OAVNBorOTlZkvTzn/9c6enpcjgcmjJlikaNGqXbb79dWVlZ2r59u8aOHavAwEDt2rVLq1ev1lNPPaUbbrjhjOseO3asnE6nrrnmGt1+++0qKyvTc889p5iYGB08eLBeHUuXLtVjjz2m3r17KyYmRldeeWW9dQYGBmr+/PnKyMjQqFGjNHXqVPM0+MTERPPwGoAGWHwWGoAW5DnV+3SP/Px8wzAMY+vWrUZ6eroRFhZmhIaGGt///veNDz74wGtdjz32mDFs2DAjKirKCAkJMfr27Wv89re/NaqqqgzDMIyioiLjrrvuMvr27Wt06NDBiIyMNFJSUozXXnut0fU+99xzhiQjPDzc67RuwzCMr7/+2vjpT39q9OrVywgODjY6depkfP/73zfWrl3r1W7UqFENnsp+uu/m1NPga2pqjHvuucfo0qWLYbPZ6q3n2WefNZKTk42QkBAjPDzcGDhwoPHggw8aBw4cMNt07979tJcL+Mc//mEMGjTICA4ONhITE4358+cby5cvNyQZe/bsMdsVFBQYEyZMMMLDww1J5inx3z0N3mPVqlXGpZdeagQFBRmdOnUybrrpJmPfvn1ebaZPn2506NChXk1z5sxp1PcFtDU2w2jC/yYBAAC0YowBAgAA7Q4BCAAAtDsEIAAA0O4QgAAAQLtDAAIAAO0OAQgAALQ7XAixAW63WwcOHFB4eHijL0kPAACsZRiGjh07pvj4eNntZ+7jIQA14MCBA173PwIAAK1Hfn6+LrjggjO2IQA1IDw8XFLtF+i5DxIAAPBvpaWlSkhIMH/Hz4QA1ADPYa+IiAgCEAAArUxjhq8wCBoAALQ7BCAAANDuEIAAAEC7QwACAADtDgEIAAC0OwQgAADQ7hCAAABAu0MAAgAA7Q4BCAAAtDsEIAAA0O4QgAAAQLtDAAIAAO0ON0P1oWMnqlVyvFohgQ51DguyuhwAANoteoB86MXcbzR8/jo9/tZOq0sBAKBdIwD5kMNukyRVuwyLKwEAoH0jAPlQQF0AcrndFlcCAED7RgDyIU8AqnbTAwQAgJUIQD4U4Kj9ul0cAgMAwFIEIB/y9ADV0AMEAIClCEA+5DADEGOAAACwEgHIhwI9h8DoAQIAwFIEIB86eRo8PUAAAFiJAORDgQ7PafD0AAEAYCUCkA857LVfN4OgAQCwFgHIh8yzwDgNHgAASxGAfCjAwWnwAAD4A78IQEuWLFFiYqKCg4OVkpKiTZs2nbbtX//6Vw0ZMkRRUVHq0KGDBg8erJdeesmrjWEYmj17trp27aqQkBClpaVp165dLb0ZZ2WeBs8gaAAALGV5AFq1apUyMzM1Z84cbd26VUlJSUpPT9ehQ4cabN+pUyf96le/Um5urj755BNlZGQoIyNDb731ltlmwYIFevrpp7Vs2TJt3LhRHTp0UHp6uk6cOOGrzWoQp8EDAOAfbIZhWPprnJKSoqFDh2rx4sWSJLfbrYSEBN1zzz36xS9+0ah1XHbZZZowYYIeffRRGYah+Ph43Xfffbr//vslSSUlJYqNjdWKFSs0ZcqUs66vtLRUkZGRKikpUURERNM37ju25n2r637/gS7sFKr/PPj9ZlsvAAA4t99vS3uAqqqqtGXLFqWlpZnT7Ha70tLSlJube9blDcNQTk6Odu7cqZEjR0qS9uzZo4KCAq91RkZGKiUlpVHrbEkBHAIDAMAvBFj54UVFRXK5XIqNjfWaHhsbqy+++OK0y5WUlKhbt26qrKyUw+HQ73//e1111VWSpIKCAnMd312nZ953VVZWqrKy0nxfWlrapO05mwBOgwcAwC9YGoCaKjw8XNu3b1dZWZlycnKUmZmpnj17avTo0U1aX1ZWln7zm980b5EN4CwwAAD8g6WHwKKjo+VwOFRYWOg1vbCwUHFxcaddzm63q3fv3ho8eLDuu+8+3XDDDcrKypIkc7lzWeesWbNUUlJiPvLz889ns06LQ2AAAPgHSwOQ0+lUcnKycnJyzGlut1s5OTlKTU1t9Hrcbrd5CKtHjx6Ki4vzWmdpaak2btx42nUGBQUpIiLC69ESPIfAOAsMAABrWX4ILDMzU9OnT9eQIUM0bNgwLVq0SOXl5crIyJAkTZs2Td26dTN7eLKysjRkyBD16tVLlZWVevPNN/XSSy9p6dKlkiSbzaaZM2fqscceU58+fdSjRw89/PDDio+P16RJk6zaTEmSo+4QWDUBCAAAS1kegCZPnqzDhw9r9uzZKigo0ODBg5WdnW0OYs7Ly5PdfrKjqry8XHfeeaf27dunkJAQ9e3bV3/84x81efJks82DDz6o8vJy3XbbbSouLtbw4cOVnZ2t4OBgn2/fqQLt3AwVAAB/YPl1gPxRS10H6EhZpZIfWytJ2pP1A9lstmZbNwAA7V2ruQ5QexNwSk8WZ4IBAGAdApAPeU6DlzgMBgCAlQhAPuS5GaokVXMqPAAAliEA+ZDnZqgSPUAAAFiJAORDp3QAqdpFAAIAwCoEIB+y2Wzm1aDpAQIAwDoEIB87eT8wxgABAGAVApCPmXeE5xAYAACWIQD5GHeEBwDAegQgHzPvCM8hMAAALEMA8jHPtYA4BAYAgHUIQD7mGQPEWWAAAFiHAORjnAUGAID1CEA+FsAhMAAALEcA8jHzNHgOgQEAYBkCkI+Zg6AJQAAAWIYA5GOBDs+tMBgDBACAVQhAPubpAeJmqAAAWIcA5GMBDk6DBwDAagQgHwswe4A4BAYAgFUIQD7mOQRGDxAAANYhAPlYoIPT4AEAsBoByMe4FxgAANYjAPkYp8EDAGA9ApCPOequBM1p8AAAWIcA5GMBDIIGAMByBCAfC+BWGAAAWI4A5GMBDs8gaMYAAQBgFQKQj3E3eAAArEcA8rGTd4OnBwgAAKsQgHyMMUAAAFiPAORj5s1QOQ0eAADLEIB8jB4gAACsRwDyMfMsMMYAAQBgGQKQjwVwLzAAACxHAPIxB6fBAwBgOQKQj528GSoBCAAAqxCAfMxzHaBqrgQNAIBlCEA+Zp4GTw8QAACWIQD5WIDZA0QAAgDAKgQgH/McAnNxGjwAAJYhAPlYoIMLIQIAYDUCkI+Zp8FzCAwAAMv4RQBasmSJEhMTFRwcrJSUFG3atOm0bZ977jmNGDFCHTt2VMeOHZWWllav/c033yybzeb1GDduXEtvRqME2jkNHgAAq1kegFatWqXMzEzNmTNHW7duVVJSktLT03Xo0KEG269fv15Tp07VunXrlJubq4SEBI0dO1b79+/3ajdu3DgdPHjQfLz66qu+2JyzMk+DZwwQAACWsTwALVy4ULfeeqsyMjLUr18/LVu2TKGhoVq+fHmD7V9++WXdeeedGjx4sPr27avnn39ebrdbOTk5Xu2CgoIUFxdnPjp27OiLzTmrAC6ECACA5SwNQFVVVdqyZYvS0tLMaXa7XWlpacrNzW3UOioqKlRdXa1OnTp5TV+/fr1iYmJ08cUXa8aMGTpy5Mhp11FZWanS0lKvR0sJYAwQAACWszQAFRUVyeVyKTY21mt6bGysCgoKGrWOhx56SPHx8V4haty4cXrxxReVk5Oj+fPna8OGDRo/frxcLleD68jKylJkZKT5SEhIaPpGnYV5M1QOgQEAYJkAqws4H/PmzdOf/vQnrV+/XsHBweb0KVOmmK8HDhyoQYMGqVevXlq/fr3GjBlTbz2zZs1SZmam+b60tLTFQpDnStCcBg8AgHUs7QGKjo6Ww+FQYWGh1/TCwkLFxcWdcdknnnhC8+bN09tvv61BgwadsW3Pnj0VHR2tr776qsH5QUFBioiI8Hq0FM8gaA6BAQBgHUsDkNPpVHJystcAZs+A5tTU1NMut2DBAj366KPKzs7WkCFDzvo5+/bt05EjR9S1a9dmqft8BHAaPAAAlrP8LLDMzEw999xzWrlypT7//HPNmDFD5eXlysjIkCRNmzZNs2bNMtvPnz9fDz/8sJYvX67ExEQVFBSooKBAZWVlkqSysjI98MAD+vDDD7V3717l5ORo4sSJ6t27t9LT0y3ZxlMFOBgDBACA1SwfAzR58mQdPnxYs2fPVkFBgQYPHqzs7GxzYHReXp7s9pM5benSpaqqqtINN9zgtZ45c+Zo7ty5cjgc+uSTT7Ry5UoVFxcrPj5eY8eO1aOPPqqgoCCfbltDOAsMAADr2QzD4Jf4O0pLSxUZGamSkpJmHw+0+3CZxjy5QZEhgfp4zthmXTcAAO3Zufx+W34IrL0xT4N3cQgMAACrEIB8zDwLjEHQAABYhgDkY4F11wHiLDAAAKxDAPKxU3uAGH4FAIA1CEA+FnjKGW30AgEAYA0CkI856q4DJDEOCAAAqxCAfMxzFphEAAIAwCoEIB87NQC5uBgiAACWIAD5mOOUAFTN7TAAALAEAcjHbDYbN0QFAMBiBCALeHqBqrkaNAAAliAAWYAeIAAArEUAskBA3dWgOQsMAABrEIAscPKGqAQgAACsQACyQIDDczsMxgABAGAFApAFAupuh0EPEAAA1iAAWeDUG6ICAADfIwBZwHMIjLPAAACwBgHIAicHQTMGCAAAKxCALGCOAaIHCAAASxCALMBZYAAAWIsAZAEH1wECAMBSBCALBNYdAmMQNAAA1iAAWcC8GSoBCAAASxCALHDyNHjGAAEAYAUCkAU8p8FXMwYIAABLEIAs4GAMEAAAliIAWSDQwa0wAACwEgHIAg6uBA0AgKUIQBbwjAHiEBgAANYgAFkgwFH7tTMIGgAAaxCALHCyB4hDYAAAWIEAZIEABkEDAGApApAFzLvBcwgMAABLEIAsYJ4FRg8QAACWIABZwDwExmnwAABYggBkgQB6gAAAsBQByAIB3AoDAABLEYAscLIHiENgAABYgQBkAYc5BogeIAAArEAAskCg5zR4DoEBAGAJApAFOA0eAABr+UUAWrJkiRITExUcHKyUlBRt2rTptG2fe+45jRgxQh07dlTHjh2VlpZWr71hGJo9e7a6du2qkJAQpaWladeuXS29GY0W6OBWGAAAWMnyALRq1SplZmZqzpw52rp1q5KSkpSenq5Dhw412H79+vWaOnWq1q1bp9zcXCUkJGjs2LHav3+/2WbBggV6+umntWzZMm3cuFEdOnRQenq6Tpw44avNOiOHnZuhAgBgJZthGJb+CqekpGjo0KFavHixJMntdishIUH33HOPfvGLX5x1eZfLpY4dO2rx4sWaNm2aDMNQfHy87rvvPt1///2SpJKSEsXGxmrFihWaMmXKWddZWlqqyMhIlZSUKCIi4vw2sAGvfZSvB//yia7sG6PlNw9t9vUDANAencvvt6U9QFVVVdqyZYvS0tLMaXa7XWlpacrNzW3UOioqKlRdXa1OnTpJkvbs2aOCggKvdUZGRiolJaXR62xp3AwVAABrBVj54UVFRXK5XIqNjfWaHhsbqy+++KJR63jooYcUHx9vBp6CggJzHd9dp2fed1VWVqqystJ8X1pa2uhtaApzEDS3wgAAwBKWjwE6H/PmzdOf/vQn/e1vf1NwcHCT15OVlaXIyEjzkZCQ0IxV1hfo4DR4AACsZGkAio6OlsPhUGFhodf0wsJCxcXFnXHZJ554QvPmzdPbb7+tQYMGmdM9y53LOmfNmqWSkhLzkZ+f35TNaTR6gAAAsJalAcjpdCo5OVk5OTnmNLfbrZycHKWmpp52uQULFujRRx9Vdna2hgwZ4jWvR48eiouL81pnaWmpNm7ceNp1BgUFKSIiwuvRkjy3wuBeYAAAWMPSMUCSlJmZqenTp2vIkCEaNmyYFi1apPLycmVkZEiSpk2bpm7duikrK0uSNH/+fM2ePVuvvPKKEhMTzXE9YWFhCgsLk81m08yZM/XYY4+pT58+6tGjhx5++GHFx8dr0qRJVm2mlwAOgQEAYCnLA9DkyZN1+PBhzZ49WwUFBRo8eLCys7PNQcx5eXmy2092VC1dulRVVVW64YYbvNYzZ84czZ07V5L04IMPqry8XLfddpuKi4s1fPhwZWdnn9c4oeZk3gyV6wABAGAJy68D5I9a+jpAG78+osnPfqheXToo577Rzb5+AADao1ZzHaD2iusAAQBgLQKQBQI8d4PnEBgAAJYgAFkgONAhSaqscVlcCQAA7RMByAIhdQGooooABACAFQhAFgh21n7tx6tdYgw6AAC+RwCygKcHyDCkyhquBg0AgK8RgCzgCUCSdKKaw2AAAPgaAcgCAQ67nHVXg2YcEAAAvkcAskhw4MlxQAAAwLcIQBYJcdYeBjtODxAAAD5HALJIqLP2NmyMAQIAwPcIQBYJ5lpAAABYhgBkkRDGAAEAYBkCkEU8Y4A4BAYAgO8RgCwSElg7BohB0AAA+B4ByCLmWWD0AAEA4HMEIIt4xgAxCBoAAN8jAFnEczsMxgABAOB7BCCLBHMhRAAALEMAskioZxA0PUAAAPgcAcgiIc666wDRAwQAgM8RgCziGQNEDxAAAL5HALJIMAEIAADLEIAs4rkZKofAAADwPQKQRcwxQPQAAQDgcwQgi5iHwOgBAgDA55oUgPLz87Vv3z7z/aZNmzRz5kw9++yzzVZYW8cgaAAArNOkAPTjH/9Y69atkyQVFBToqquu0qZNm/SrX/1KjzzySLMW2FZ5xgBxJWgAAHyvSQHo008/1bBhwyRJr732mgYMGKAPPvhAL7/8slasWNGc9bVZnh4g7gUGAIDvNSkAVVdXKygoSJK0du1aXXvttZKkvn376uDBg81XXRsWfMogaMMwLK4GAID2pUkBqH///lq2bJneffddrVmzRuPGjZMkHThwQJ07d27WAtsqTw+QYUiVNW6LqwEAoH1pUgCaP3++nnnmGY0ePVpTp05VUlKSJOkf//iHeWgMZ+YJQBLjgAAA8LWApiw0evRoFRUVqbS0VB07djSn33bbbQoNDW224tqyAIddToddVS63jle7FGV1QQAAtCNN6gE6fvy4KisrzfDzzTffaNGiRdq5c6diYmKatcC2LDiw9utnIDQAAL7VpAA0ceJEvfjii5Kk4uJipaSk6Mknn9SkSZO0dOnSZi2wLQtxcjFEAACs0KQAtHXrVo0YMUKS9Oc//1mxsbH65ptv9OKLL+rpp59u1gLbMq4FBACANZoUgCoqKhQeHi5Jevvtt3XdddfJbrfre9/7nr755ptmLbAt447wAABYo0kBqHfv3nr99deVn5+vt956S2PHjpUkHTp0SBEREc1aYFsWwhggAAAs0aQANHv2bN1///1KTEzUsGHDlJqaKqm2N+jSSy9t1gLbMs8YIA6BAQDgW006Df6GG27Q8OHDdfDgQfMaQJI0ZswY/fCHP2y24tq6kMDar59B0AAA+FaTApAkxcXFKS4uzrwr/AUXXMBFEM+ReRYYPUAAAPhUkw6Bud1uPfLII4qMjFT37t3VvXt3RUVF6dFHH5XbzW0dGosxQAAAWKNJPUC/+tWv9Ic//EHz5s3TFVdcIUl67733NHfuXJ04cUK//e1vm7XItspzOwzGAAEA4FtN6gFauXKlnn/+ec2YMUODBg3SoEGDdOedd+q5557TihUrzmldS5YsUWJiooKDg5WSkqJNmzadtu2OHTt0/fXXKzExUTabTYsWLarXZu7cubLZbF6Pvn37nuMW+kaIkzFAAABYoUkB6OjRow2Gir59++ro0aONXs+qVauUmZmpOXPmaOvWrUpKSlJ6eroOHTrUYPuKigr17NlT8+bNU1xc3GnX279/fx08eNB8vPfee42uyZdCuA4QAACWaFIASkpK0uLFi+tNX7x4sQYNGtTo9SxcuFC33nqrMjIy1K9fPy1btkyhoaFavnx5g+2HDh2qxx9/XFOmTFFQUNBp1xsQEGAO0o6Li1N0dHSja/KlEGft108PEAAAvtWkMUALFizQhAkTtHbtWvMaQLm5ucrPz9ebb77ZqHVUVVVpy5YtmjVrljnNbrcrLS1Nubm5TSnLtGvXLsXHxys4OFipqanKysrShRdeeNr2lZWVqqysNN+Xlpae1+c3Fj1AAABYo0k9QKNGjdKXX36pH/7whyouLlZxcbGuu+467dixQy+99FKj1lFUVCSXy6XY2Fiv6bGxsSooKGhKWZKklJQUrVixQtnZ2Vq6dKn27NmjESNG6NixY6ddJisrS5GRkeYjISGhyZ9/LswxQAQgAAB8qsnXAYqPj693ttfHH3+sP/zhD3r22WfPu7CmGj9+vPl60KBBSklJUffu3fXaa6/plltuaXCZWbNmKTMz03xfWlrqkxBk9gBxCAwAAJ9qcgA6X9HR0XI4HCosLPSaXlhYeMYBzucqKipKF110kb766qvTtgkKCjrjmKKW4hkDxGnwAAD4VpMOgTUHp9Op5ORk5eTkmNPcbrdycnLMcUXNoaysTLt371bXrl2bbZ3NxXM3eC6ECACAb1nWAyRJmZmZmj59uoYMGaJhw4Zp0aJFKi8vV0ZGhiRp2rRp6tatm7KysiTVDpz+7LPPzNf79+/X9u3bFRYWpt69e0uS7r//fl1zzTXq3r27Dhw4oDlz5sjhcGjq1KnWbOQZhDIGCAAAS5xTALruuuvOOL+4uPicPnzy5Mk6fPiwZs+erYKCAg0ePFjZ2dnmwOi8vDzZ7Sc7qQ4cOOB1t/knnnhCTzzxhEaNGqX169dLkvbt26epU6fqyJEj6tKli4YPH64PP/xQXbp0OafafIErQQMAYA2bYRhGYxt7embO5oUXXmhyQf6gtLRUkZGRKikpUURERIt9Tt6RCo18fJ06OB3a8ci4FvscAADag3P5/T6nHqDWHmz8TXDdIOiKapcMw5DNZrO4IgAA2gfLBkHj5CEww5Aqa9wWVwMAQPtBALKQJwBJjAMCAMCXCEAWCnDY5XTU3Q+MAAQAgM8QgCwW4qztBSqvrLG4EgAA2g8CkMUiQwIlSSXHqy2uBACA9oMAZDECEAAAvkcAslhUaG0AKq4gAAEA4CsEIItF0AMEAIDPEYAsFkUAAgDA5whAFvOMAeIQGAAAvkMAsphnDFApPUAAAPgMAchiZg8QAQgAAJ8hAFmM0+ABAPA9ApDFIkOckqTiiiqLKwEAoP0gAFnsZA8Qt8IAAMBXCEAW8wyCLjleJcMwLK4GAID2gQBkMU8PULXL4I7wAAD4CAHIYqFOhwIdNklcCwgAAF8hAFnMZrNxJhgAAD5GAPIDEVwNGgAAnyIA+QHuBwYAgG8RgPyA5xAYt8MAAMA3CEB+ICq07mKIx7kYIgAAvkAA8gMMggYAwLcIQH4gkkHQAAD4FAHID9ADBACAbxGA/AABCAAA3yIA+YGT9wMjAAEA4AsEID/AGCAAAHyLAOQH6AECAMC3CEB+wHMrjNIT1XK7DYurAQCg7SMA+QHPITDDkI6dqLG4GgAA2j4CkB8ICnAoJNAhicNgAAD4AgHIT5gDobkdBgAALY4A5CcYCA0AgO8QgPxEBKfCAwDgMwQgPxFlBiAOgQEA0NIIQH6iS3iQJOnQsUqLKwEAoO0jAPmJuIhgSVJByQmLKwEAoO0jAPmJ2Mi6AFRKAAIAoKURgPyEpwfoUCmHwAAAaGkEID8RRw8QAAA+Y3kAWrJkiRITExUcHKyUlBRt2rTptG137Nih66+/XomJibLZbFq0aNF5r9NfxIbXBqCS49U6Ue2yuBoAANo2SwPQqlWrlJmZqTlz5mjr1q1KSkpSenq6Dh061GD7iooK9ezZU/PmzVNcXFyzrNNfRIQEKDiwdncwEBoAgJZlaQBauHChbr31VmVkZKhfv35atmyZQkNDtXz58gbbDx06VI8//rimTJmioKCgZlmnv7DZbCfPBOMwGAAALcqyAFRVVaUtW7YoLS3tZDF2u9LS0pSbm+vTdVZWVqq0tNTrYYXYugBUSAACAKBFWRaAioqK5HK5FBsb6zU9NjZWBQUFPl1nVlaWIiMjzUdCQkKTPv98EYAAAPANywdB+4NZs2appKTEfOTn51tSh3kmWAmnwgMA0JICrPrg6OhoORwOFRYWek0vLCw87QDnllpnUFDQaccU+RI9QAAA+IZlPUBOp1PJycnKyckxp7ndbuXk5Cg1NdVv1ulLDIIGAMA3LOsBkqTMzExNnz5dQ4YM0bBhw7Ro0SKVl5crIyNDkjRt2jR169ZNWVlZkmoHOX/22Wfm6/3792v79u0KCwtT7969G7VOfxYXWdsLRQ8QAAAty9IANHnyZB0+fFizZ89WQUGBBg8erOzsbHMQc15enuz2k51UBw4c0KWXXmq+f+KJJ/TEE09o1KhRWr9+faPW6c9iwk/eDsMwDNlsNosrAgCgbbIZhmFYXYS/KS0tVWRkpEpKShQREeGzz62qceuiX/9bkrTl12nqHGb9uCQAAFqLc/n95iwwP+IMsKtzB6ckxgEBANCSCEB+hjPBAABoeQQgP+O5FlBhKdcCAgCgpRCA/ExsRO24H26ICgBAyyEA+RkOgQEA0PIIQH4mPjJEkrTv2+MWVwIAQNtFAPIzPbp0kCTtKSq3uBIAANouApCf6RFdG4AOlBzXiWqXxdUAANA2EYD8TOcOToUHB8gwpL1H6AUCAKAlEID8jM1mU88uYZKkPYcJQAAAtAQCkB/qWXcY7GvGAQEA0CIIQH7IMw7oa3qAAABoEQQgP9TTPBOszOJKAABomwhAfqgHh8AAAGhRBCA/5AlAxRXV+ra8yuJqAABoewhAfijUGaCudTdFpRcIAIDmRwDyUycHQjMOCACA5kYA8lM9uSUGAAAthgDkp3pE110MkQAEAECzIwD5qZ5cCwgAgBZDAPJTvbqc7AGqqnFbXA0AAG0LAchPJXQKUXhwgKpcbn1ZeMzqcgAAaFMIQH7KZrNp0AWRkqT/7i+xuBoAANoWApAfG9gtSpL0yT4CEAAAzYkA5MdO9gAVW1sIAABtDAHIj3kC0BcHj+lEtcviagAAaDsIQH6sW1SIOnVwqsZt6IsCBkIDANBcCEB+zGazaWC3usNg+4qtLQYAgDaEAOTnPIfBGAgNAEDzIQD5ObMHiFPhAQBoNgQgPzfogihJ0peFx3S8ioHQAAA0BwKQn4uLDFZMeJDchvQx44AAAGgWBKBW4Hs9O0uS3v+qyOJKAABoGwhArcDw3tGSpPcIQAAANAsCUCtwRZ/aAPRxfrFKT1RbXA0AAK0fAagV6BYVop7RHeQ2pA93H7G6HAAAWj0CUCtxBYfBAABoNgSgVoIABABA8yEAtRKpvTrLbpO+PlyuA8XHrS4HAIBWjQDUSkSGBJoXRXxvF71AAACcDwJQKzL64i6SpOwdBRZXAgBA60YAakUmDOwqSXp312GVVHA6PAAATeUXAWjJkiVKTExUcHCwUlJStGnTpjO2X716tfr27avg4GANHDhQb775ptf8m2++WTabzesxbty4ltwEn+gTG66LY8NV7TL01mf0AgEA0FSWB6BVq1YpMzNTc+bM0datW5WUlKT09HQdOnSowfYffPCBpk6dqltuuUXbtm3TpEmTNGnSJH366ade7caNG6eDBw+aj1dffdUXm9Pirh5U2wv0r08OWlwJAACtl80wDMPKAlJSUjR06FAtXrxYkuR2u5WQkKB77rlHv/jFL+q1nzx5ssrLy/XGG2+Y0773ve9p8ODBWrZsmaTaHqDi4mK9/vrrTaqptLRUkZGRKikpUURERJPW0VK+PlymK5/coAC7TR/9Kk0dOzitLgkAAL9wLr/flvYAVVVVacuWLUpLSzOn2e12paWlKTc3t8FlcnNzvdpLUnp6er3269evV0xMjC6++GLNmDFDR46c/grKlZWVKi0t9Xr4q55dwtSva4Rq3AaDoQEAaCJLA1BRUZFcLpdiY2O9psfGxqqgoOEf94KCgrO2HzdunF588UXl5ORo/vz52rBhg8aPHy+Xy9XgOrOyshQZGWk+EhISznPLWtbVSbWHwV7ftt/iSgAAaJ0sHwPUEqZMmaJrr71WAwcO1KRJk/TGG2/oo48+0vr16xtsP2vWLJWUlJiP/Px83xZ8jiYN7ia7Tdq456i+LDxmdTkAALQ6lgag6OhoORwOFRYWek0vLCxUXFxcg8vExcWdU3tJ6tmzp6Kjo/XVV181OD8oKEgRERFeD38WHxWisf1qt/fF3L3WFgMAQCtkaQByOp1KTk5WTk6OOc3tdisnJ0epqakNLpOamurVXpLWrFlz2vaStG/fPh05ckRdu3ZtnsL9wLTU7pKkv23dr2MnuCYQAADnwvJDYJmZmXruuee0cuVKff7555oxY4bKy8uVkZEhSZo2bZpmzZpltr/33nuVnZ2tJ598Ul988YXmzp2rzZs36+6775YklZWV6YEHHtCHH36ovXv3KicnRxMnTlTv3r2Vnp5uyTa2hNRendU7JkzlVS79dStjgQAAOBeWB6DJkyfriSee0OzZszV48GBt375d2dnZ5kDnvLw8HTx48po3l19+uV555RU9++yzSkpK0p///Ge9/vrrGjBggCTJ4XDok08+0bXXXquLLrpIt9xyi5KTk/Xuu+8qKCjIkm1sCTabzewFWpm7V263pVczAACgVbH8OkD+yJ+vA3SqssoapWbl6NiJGv1u6qW6Jine6pIAALBMq7kOEM5PWFCAfja8pyRp0dov5aIXCACARiEAtXI/HZ6oyJBA7T5crn98zFggAAAagwDUyoUHB+q2kbW9QE+t3aUal9viigAA8H8EoDbg5ssT1amDU3uPVOilD7+xuhwAAPweAagN6BAUoMyrLpIkLXz7Sx0qPWFxRQAA+DcCUBsxddiFSrogUscqa/TYvz63uhwAAPwaAaiNcNhtemzSQNlt0j8+PqB3dx22uiQAAPwWAagNGXhBpH7yvdqLIz6w+hMVV1RZXBEAAP6JANTGPDS+r3pGd1BB6Qn94i//Fde5BACgPgJQGxPqDNDTUy9VoMOm7B0FemVTntUlAQDgdwhAbdCAbpF6ML2vJGnuP3boo71HLa4IAAD/QgBqo24Z3kPjB8Sp2mXo9pe2KP9ohdUlAQDgNwhAbZTdbtOTNyZpQLcIHS2v0k9XfKRvyxkUDQCARABq00KdAXp+2lDFRgRp16Ey/WT5RpUcr7a6LAAALEcAauPiIoP18s9S1LmDU5/uL9X05ZtUeoIQBABo3whA7UDvmHD98WcpigoN1Pb8Yk1+5kNulwEAaNcIQO3EJV0j9PLPUhQdFqTPD5bquqUfaPfhMqvLAgDAEgSgdqR/fKT+OuNyde8cqn3fHtekxe8r5/NCq8sCAMDnCEDtzIWdQ/WXGZdraGJHHaus0S0rN2vhmi9V43JbXRoAAD5DAGqHosOC9PLPvqdpqbX3DXs6Z5dufCZXeUe4VhAAoH0gALVTzgC7Hpk4QE9NGazwoABtzSvW+Kf+oxfe3yOXm/uHAQDaNgJQOzdxcDe9ee8IDUvspPIql37zz8903e/f15ZvvrW6NAAAWozN4Hbh9ZSWlioyMlIlJSWKiIiwuhyfcLsNvfpRnua9+YWOVdZIktL7x+rBcX3Vq0uYxdUBAHB25/L7TQBqQHsMQB6HSk/oybe/1Oot+XIbksNu041DEjQzrY9iI4KtLg8AgNMiAJ2n9hyAPHYVHtP87J1aW3eavNNh17WD43XL8B66pGv7/E4AAP6NAHSeCEAnbdpzVI+/9YU+2ntyTNCIPtG6ZXgPjezTRXa7zcLqAAA4iQB0nghA9W3N+1Z/eG+P/v3fg/KcJNYtKkTXXdZN1192gRKjO1hbIACg3SMAnScC0OnlH63Qyg/2atXmfB07UWNOH5rYUZMu7aar+sUqJpyxQgAA3yMAnScC0NmdqHZpzWeF+svWffrPl4fNXiGbTUq+sKPS+8cpvX+cLuwcam2hAIB2gwB0nghA56aw9IRe37Zfb35aoI/zi73m9ezSQSN6R2tEny76Xq/OCgsKsKZIAECbRwA6TwSgpjtYclxv7yjUWzsKtHHPUa+rSgfYbUpKiNKQ7h2VXPfoHBZkYbUAgLaEAHSeCEDNo+R4tXJ3H9G7uw7r3V1Fyjta/15jPaM7mGEoKSFKvWPCFOjgAuUAgHNHADpPBKCW8c2Rcm3ac1RbvvlWW775VrsOldVr43TYdVFcmPp1jVD/+Ej1i49Q37hwhQcHWlAxAKA1IQCdJwKQbxRXVGlr3rdmINqxv9S8Dcd3dYsKUa+YMPXq0kG9Y8LUu0uYesWEqXMHp2w2rkUEACAAnTcCkDUMw1D+0eP67GCJdhwo1WcHSrXjQKkKSk+cdpnIkEB17xyqhE6hSugYqgs71T4SOoUoPiqEw2kA0I4QgM4TAci/fFtepa8Ol+mrQ2XafahMXx0u0+7DZdr37XGd6b9eu03qGhlSG4YiQxQXGayukcGKjQhW17r3nTs4uZo1ALQR5/L7zTnJ8HsdOzg1tEMnDU3s5DX9eJVLe4+UK/9ohfKOVij/aIXyvz1uvq6scWt/8XHtLz5+2nUHOmyKCQ9WXGSwuoQFqXOYU9FhQYoOD1J0B6eiw4PUue45PCiAw20A0EYQgNBqhTgduqRrRIM3Z3W7DRWVVdaGoW8rVFBSqYKS4zpYckKFpSd0sOSEDpdVqtplnDUkeTgD7GYo6hjqVMfQQEWFOhUZEqioUM/DqaiQk88RIYFy0MMEAH6HAIQ2yW63KSYiWDERwRrynZ4jj2qXW4ePVZqhqKisUkXHKlVUXlX7XFapI3Wvy6tcqqpx60DJCR0oOf2YpO+y2aSI4NpwFBkSqPDgAIUHBSosOEBhQQGKCA6oe107Lyw4QOFBAQoPPtkmLCiAEAUAzYwAhHYr0GFXfFTtYOmzOV7lqg1IZZUqKqvStxVVKqmoVvHxKhVXVKv4eLX5/tvyapUcr1ZZZY0Mo/Z6SCXHq8+r1g5Oh8KCAxTqDFBIoEMdghwKcQYoNNChUKdDoUGOBueZr5117epeBwc4FBRoV1CAncN6ANolAhDQCCFOR+2ZZp0af2+zapdbJcerVVxRrZK6oHTsRI2OVdao7ESNjp2oDUllJ2pUeqJGZZW174+d8MyvUZXLLUkqr3KpvMolqbLZty0owK7gQIeCA+0KCqh9Dg48NSSdnHZqW8/877Z1BtjkdDjkDLAr0GGTM6A2aAU67HXTap+djtoHg9ABWIEABLSQQIe9dkD1edzuo7LGZYahssoaVVS5VFFVo+N1geh4Ve20U1/Xzjv19cl5nuVPuUOJKmvcqqxxq+Tsw6BaRIDdVj8YnfLsCVHOAIecnteO2vYBjtr5Afa651NeOzzT7DavdgEOW+2y9rpnr+W9pwfa7XI4bAqsW4dnWkDdeuk9A1ovAhDgx4ICHAoKczTrPdMMw1CN29CJapdOVLt1otqlyhrPc+00z/Op8xrbttrlVlVN3cN18rna8+zyvnZBjdtQTZVLkqvZttFXasNVbYCy26QAh10Ou00Om6322V4blBx27/d2z7OtdnmH3S6HTbXPdinAbj/zMnabHI7az6ldf+1ypz6fusypNdnrXttttWPl7DabHHbJZvNMt8luV+3ruvl2m2qX/c58m+3kum2ntrGrdr2277y317UzXxMgYR2/CEBLlizR448/roKCAiUlJel3v/udhg0bdtr2q1ev1sMPP6y9e/eqT58+mj9/vn7wgx+Y8w3D0Jw5c/Tcc8+puLhYV1xxhZYuXao+ffr4YnMAv2az2RRY1wsSHuz7z3e7DVW7a4NRtctoOCw1FKI84eqUdjUuQzUut6rddc8uQzXu2uner92qcdc9101vaH7NaaZVu90NXnOqxl0bJiW3z7/HtqI2NMkMV2ZIOiVwOeyqF77sZkDzXtZuq/1vvMFnnQxjNptNNslc3lYX4k5dn8zXqmtrO2WdDb+3163HppMh0+t9XXvZTr636WQ7873tlPen1tHQ+7r2nm2TvGs5+R14Puvkcqd+N7a67fC8PrktJz+3qcuc+r14lgkPrj05xCqWB6BVq1YpMzNTy5YtU0pKihYtWqT09HTt3LlTMTEx9dp/8MEHmjp1qrKysnT11VfrlVde0aRJk7R161YNGDBAkrRgwQI9/fTTWrlypXr06KGHH35Y6enp+uyzzxQcbMFffAAmu92mILtDQQEOq0s5Jy73qaHIO0C56oKQ2zDM9y7DkMvtlsst1bjdtdO+8/Baxjhl2neePetxud1y1fXguU5Zxmt9nmWM2jbmZ9Stx+2W3IZR96jdLsPwrKv2fyBd7pPz3XXvjbq2nmVrX9cGWnfd8qe+dzfyErsut1HX98c1edubGaN76aFxfS37fMuvBJ2SkqKhQ4dq8eLFkiS3262EhATdc889+sUvflGv/eTJk1VeXq433njDnPa9731PgwcP1rJly2QYhuLj43Xffffp/vvvlySVlJQoNjZWK1as0JQpU85aE1eCBoDzYxh1ockTttynvq4fvrzClNs7oHmW9wQtoy6sea3LfF37uSdD2Mkg5zZO1nXa9zo53e02ZEgn35tta9tLMj/fbdS29bSrXab+e3dd/fU/6+R7t2FIp9TktW73dz9LXtvc0LPxne9A5vdYV+Mp++vUmuqamq/dpy5z6nyd3D59Z3nPdyiv7ah9ffvInsoce3Gz/nfXaq4EXVVVpS1btmjWrFnmNLvdrrS0NOXm5ja4TG5urjIzM72mpaen6/XXX5ck7dmzRwUFBUpLSzPnR0ZGKiUlRbm5uQ0GoMrKSlVWnjy7prS09Hw2CwDaPfNQjRjnA/9k6Z0ii4qK5HK5FBsb6zU9NjZWBQUFDS5TUFBwxvae53NZZ1ZWliIjI81HQkJCk7YHAAC0DtwqW9KsWbNUUlJiPvLz860uCQAAtCBLA1B0dLQcDocKCwu9phcWFiouLq7BZeLi4s7Y3vN8LusMCgpSRESE1wMAALRdlgYgp9Op5ORk5eTkmNPcbrdycnKUmpra4DKpqale7SVpzZo1ZvsePXooLi7Oq01paak2btx42nUCAID2xfLT4DMzMzV9+nQNGTJEw4YN06JFi1ReXq6MjAxJ0rRp09StWzdlZWVJku69916NGjVKTz75pCZMmKA//elP2rx5s5599llJtQPvZs6cqccee0x9+vQxT4OPj4/XpEmTrNpMAADgRywPQJMnT9bhw4c1e/ZsFRQUaPDgwcrOzjYHMefl5cluP9lRdfnll+uVV17Rr3/9a/3yl79Unz599Prrr5vXAJKkBx98UOXl5brttttUXFys4cOHKzs7m2sAAQAASX5wHSB/xHWAAABofc7l95uzwAAAQLtDAAIAAO0OAQgAALQ7BCAAANDuEIAAAEC7QwACAADtDgEIAAC0O5ZfCNEfeS6NVFpaanElAACgsTy/2425xCEBqAHHjh2TJCUkJFhcCQAAOFfHjh1TZGTkGdtwJegGuN1uHThwQOHh4bLZbM267tLSUiUkJCg/P79NXmW6rW+fxDa2BW19+6S2v41tffsktrEpDMPQsWPHFB8f73UbrYbQA9QAu92uCy64oEU/IyIios3+By21/e2T2Ma2oK1vn9T2t7Gtb5/ENp6rs/X8eDAIGgAAtDsEIAAA0O4QgHwsKChIc+bMUVBQkNWltIi2vn0S29gWtPXtk9r+Nrb17ZPYxpbGIGgAANDu0AMEAADaHQIQAABodwhAAACg3SEAAQCAdocA5ENLlixRYmKigoODlZKSok2bNlldUpNkZWVp6NChCg8PV0xMjCZNmqSdO3d6tRk9erRsNpvX44477rCo4nM3d+7cevX37dvXnH/ixAnddddd6ty5s8LCwnT99dersLDQworPXWJiYr1ttNlsuuuuuyS1zn34n//8R9dcc43i4+Nls9n0+uuve803DEOzZ89W165dFRISorS0NO3atcurzdGjR3XTTTcpIiJCUVFRuuWWW1RWVubDrTi9M21fdXW1HnroIQ0cOFAdOnRQfHy8pk2bpgMHDnito6H9Pm/ePB9vyemdbR/efPPN9eofN26cV5vWug8lNfhv0maz6fHHHzfb+Ps+bMxvRGP+hubl5WnChAkKDQ1VTEyMHnjgAdXU1DRbnQQgH1m1apUyMzM1Z84cbd26VUlJSUpPT9ehQ4esLu2cbdiwQXfddZc+/PBDrVmzRtXV1Ro7dqzKy8u92t166606ePCg+ViwYIFFFTdN//79vep/7733zHn/8z//o3/+859avXq1NmzYoAMHDui6666zsNpz99FHH3lt35o1ayRJP/rRj8w2rW0flpeXKykpSUuWLGlw/oIFC/T0009r2bJl2rhxozp06KD09HSdOHHCbHPTTTdpx44dWrNmjd544w395z//0W233earTTijM21fRUWFtm7dqocfflhbt27VX//6V+3cuVPXXnttvbaPPPKI13695557fFF+o5xtH0rSuHHjvOp/9dVXvea31n0oyWu7Dh48qOXLl8tms+n666/3aufP+7AxvxFn+xvqcrk0YcIEVVVV6YMPPtDKlSu1YsUKzZ49u/kKNeATw4YNM+666y7zvcvlMuLj442srCwLq2oehw4dMiQZGzZsMKeNGjXKuPfee60r6jzNmTPHSEpKanBecXGxERgYaKxevdqc9vnnnxuSjNzcXB9V2Pzuvfdeo1evXobb7TYMo/XvQ0nG3/72N/O92+024uLijMcff9ycVlxcbAQFBRmvvvqqYRiG8dlnnxmSjI8++shs8+9//9uw2WzG/v37fVZ7Y3x3+xqyadMmQ5LxzTffmNO6d+9u/N///V/LFtdMGtrG6dOnGxMnTjztMm1tH06cONG48sorvaa1pn1oGPV/IxrzN/TNN9807Ha7UVBQYLZZunSpERERYVRWVjZLXfQA+UBVVZW2bNmitLQ0c5rdbldaWppyc3MtrKx5lJSUSJI6derkNf3ll19WdHS0BgwYoFmzZqmiosKK8pps165dio+PV8+ePXXTTTcpLy9PkrRlyxZVV1d77c++ffvqwgsvbLX7s6qqSn/84x/105/+1OsGwK19H55qz549Kigo8NpvkZGRSklJMfdbbm6uoqKiNGTIELNNWlqa7Ha7Nm7c6POaz1dJSYlsNpuioqK8ps+bN0+dO3fWpZdeqscff7xZDyv4wvr16xUTE6OLL75YM2bM0JEjR8x5bWkfFhYW6l//+pduueWWevNa0z787m9EY/6G5ubmauDAgYqNjTXbpKenq7S0VDt27GiWurgZqg8UFRXJ5XJ57UhJio2N1RdffGFRVc3D7XZr5syZuuKKKzRgwABz+o9//GN1795d8fHx+uSTT/TQQw9p586d+utf/2phtY2XkpKiFStW6OKLL9bBgwf1m9/8RiNGjNCnn36qgoICOZ3Oej8qsbGxKigosKbg8/T666+ruLhYN998szmtte/D7/Lsm4b+HXrmFRQUKCYmxmt+QECAOnXq1Or27YkTJ/TQQw9p6tSpXjeZ/PnPf67LLrtMnTp10gcffKBZs2bp4MGDWrhwoYXVNt64ceN03XXXqUePHtq9e7d++ctfavz48crNzZXD4WhT+3DlypUKDw+vd3i9Ne3Dhn4jGvM3tKCgoMF/q555zYEAhPNy11136dNPP/UaHyPJ63j7wIED1bVrV40ZM0a7d+9Wr169fF3mORs/frz5etCgQUpJSVH37t312muvKSQkxMLKWsYf/vAHjR8/XvHx8ea01r4P27Pq6mrdeOONMgxDS5cu9ZqXmZlpvh40aJCcTqduv/12ZWVltYpbLkyZMsV8PXDgQA0aNEi9evXS+vXrNWbMGAsra37Lly/XTTfdpODgYK/prWkfnu43wh9wCMwHoqOj5XA46o1wLywsVFxcnEVVnb+7775bb7zxhtatW6cLLrjgjG1TUlIkSV999ZUvSmt2UVFRuuiii/TVV18pLi5OVVVVKi4u9mrTWvfnN998o7Vr1+pnP/vZGdu19n3o2Tdn+ncYFxdX78SEmpoaHT16tNXsW0/4+eabb7RmzRqv3p+GpKSkqKamRnv37vVNgc2sZ8+eio6ONv+7bAv7UJLeffdd7dy586z/LiX/3Yen+41ozN/QuLi4Bv+teuY1BwKQDzidTiUnJysnJ8ec5na7lZOTo9TUVAsraxrDMHT33Xfrb3/7m9555x316NHjrMts375dktS1a9cWrq5llJWVaffu3eratauSk5MVGBjotT937typvLy8Vrk/X3jhBcXExGjChAlnbNfa92GPHj0UFxfntd9KS0u1ceNGc7+lpqaquLhYW7ZsMdu88847crvdZgD0Z57ws2vXLq1du1adO3c+6zLbt2+X3W6vd9iotdi3b5+OHDli/nfZ2vehxx/+8AclJycrKSnprG39bR+e7TeiMX9DU1NT9d///tcrzHoCfb9+/ZqtUPjAn/70JyMoKMhYsWKF8dlnnxm33XabERUV5TXCvbWYMWOGERkZaaxfv944ePCg+aioqDAMwzC++uor45FHHjE2b95s7Nmzx/j73/9u9OzZ0xg5cqTFlTfefffdZ6xfv97Ys2eP8f777xtpaWlGdHS0cejQIcMwDOOOO+4wLrzwQuOdd94xNm/ebKSmphqpqakWV33uXC6XceGFFxoPPfSQ1/TWug+PHTtmbNu2zdi2bZshyVi4cKGxbds28yyoefPmGVFRUcbf//5345NPPjEmTpxo9OjRwzh+/Li5jnHjxhmXXnqpsXHjRuO9994z+vTpY0ydOtWqTfJypu2rqqoyrr32WuOCCy4wtm/f7vVv03PWzAcffGD83//9n7F9+3Zj9+7dxh//+EejS5cuxrRp0yzespPOtI3Hjh0z7r//fiM3N9fYs2ePsXbtWuOyyy4z+vTpY5w4ccJcR2vdhx4lJSVGaGiosXTp0nrLt4Z9eLbfCMM4+9/QmpoaY8CAAcbYsWON7du3G9nZ2UaXLl2MWbNmNVudBCAf+t3vfmdceOGFhtPpNIYNG2Z8+OGHVpfUJJIafLzwwguGYRhGXl6eMXLkSKNTp05GUFCQ0bt3b+OBBx4wSkpKrC38HEyePNno2rWr4XQ6jW7duhmTJ082vvrqK3P+8ePHjTvvvNPo2LGjERoaavzwhz80Dh48aGHFTfPWW28ZkoydO3d6TW+t+3DdunUN/rc5ffp0wzBqT4V/+OGHjdjYWCMoKMgYM2ZMvW0/cuSIMXXqVCMsLMyIiIgwMjIyjGPHjlmwNfWdafv27Nlz2n+b69atMwzDMLZs2WKkpKQYkZGRRnBwsHHJJZcY//u//+sVHqx2pm2sqKgwxo4da3Tp0sUIDAw0unfvbtx66631/keyte5Dj2eeecYICQkxiouL6y3fGvbh2X4jDKNxf0P37t1rjB8/3ggJCTGio6ON++67z6iurm62Om11xQIAALQbjAECAADtDgEIAAC0OwQgAADQ7hCAAABAu0MAAgAA7Q4BCAAAtDsEIAAA0O4QgACgAYmJiVq0aJHVZQBoIQQgAJa7+eabNWnSJEnS6NGjNXPmTJ999ooVKxQVFVVv+kcffaTbbrvNZ3UA8K0AqwsAgJZQVVUlp9PZ5OW7dOnSjNUA8Df0AAHwGzfffLM2bNigp556SjabTTabTXv37pUkffrppxo/frzCwsIUGxurn/zkJyoqKjKXHT16tO6++27NnDlT0dHRSk9PlyQtXLhQAwcOVIcOHZSQkKA777xTZWVlkqT169crIyNDJSUl5ufNnTtXUv1DYHl5eZo4caLCwsIUERGhG2+8UYWFheb8uXPnavDgwXrppZeUmJioyMhITZkyRceOHWvZLw1AkxCAAPiNp556Sqmpqbr11lt18OBBHTx4UAkJCSouLtaVV16pSy+9VJs3b1Z2drYKCwt14403ei2/cuVKOZ1Ovf/++1q2bJkkyW636+mnn9aOHTu0cuVKvfPOO3rwwQclSZdffrkWLVqkiIgI8/Puv//+enW53W5NnDhRR48e1YYNG7RmzRp9/fXXmjx5sle73bt36/XXX9cbb7yhN954Qxs2bNC8efNa6NsCcD44BAbAb0RGRsrpdCo0NFRxcXHm9MWLF+vSSy/V//7v/5rTli9froSEBH355Ze66KKLJEl9+vTRggULvNZ56niixMREPfbYY7rjjjv0+9//Xk6nU5GRkbLZbF6f9105OTn673//qz179ighIUGS9OKLL6p///766KOPNHToUEm1QWnFihUKDw+XJP3kJz9RTk6Ofvvb357fFwOg2dEDBMDvffzxx1q3bp3CwsLMR9++fSXV9rp4JCcn11t27dq1GjNmjLp166bw8HD95Cc/0ZEjR1RRUdHoz//888+VkJBghh9J6tevn6KiovT555+b0xITE83wI0ldu3bVoUOHzmlbAfgGPUAA/F5ZWZmuueYazZ8/v968rl27mq87dOjgNW/v3r26+uqrNWPGDP32t79Vp06d9N577+mWW25RVVWVQkNDm7XOwMBAr/c2m01ut7tZPwNA8yAAAfArTqdTLpfLa9pll12mv/zlL0pMTFRAQOP/bG3ZskVut1tPPvmk7PbaDu/XXnvtrJ/3XZdccony8/OVn59v9gJ99tlnKi4uVr9+/RpdDwD/wSEwAH4lMTFRGzdu1N69e1VUVCS326277rpLR48e1dSpU/XRRx9p9+7deuutt5SRkXHG8NK7d29VV1frd7/7nb7++mu99NJL5uDoUz+vrKxMOTk5KioqavDQWFpamgYOHKibbrpJW7du1aZNmzRt2jSNGjVKQ4YMafbvAEDLIwAB8Cv333+/HA6H+vXrpy5duigvL0/x8fF6//335XK5NHbsWA0cOFAzZ85UVFSU2bPTkKSkJC1cuFDz58/XgAED9PLLLysrK8urzeWXX6477rhDkydPVpcuXeoNopZqD2X9/e9/V8eOHTVy5EilpaWpZ8+eWrVqVbNvPwDfsBmGYVhdBAAAgC/RAwQAANodAhAAAGh3CEAAAKDdIQABAIB2hwAEAADaHQIQAABodwhAAACg3SEAAQCAdocABAAA2h0CEAAAaHcIQAAAoN0hAAEAgHbn/wPkzANL4t38kwAAAABJRU5ErkJggg==","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 1.7117071151733398\n","Epoch 10 loss: 0.8498051166534424\n","Epoch 20 loss: 0.2081148624420166\n","Epoch 30 loss: 0.011945633217692375\n","\n","Prediction:\n","tensor([[ 0.9857],\n","        [-0.9731]])\n","Loss: 0.0006465670885518193\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[ 0.0323,  0.5735, -0.4385],\n","        [ 0.3694,  0.1024,  0.0826],\n","        [ 0.3109, -0.5274,  0.4482],\n","        [ 0.4119,  0.4129,  0.3148]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 2.2238, -0.6958],\n","        [ 0.9633,  1.0470],\n","        [-1.4085,  1.6843],\n","        [ 1.7476,  0.9802]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.3189],\n","        [ 0.0361],\n","        [ 0.2407],\n","        [-0.1140]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 1.9049, -1.0147],\n","        [ 0.9994,  1.0832],\n","        [-1.1678,  1.9250],\n","        [ 1.6337,  0.8662]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.9567, -0.7677],\n","        [ 0.7613,  0.7944],\n","        [-0.8236,  0.9583],\n","        [ 0.9266,  0.6994]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[ 0.5313,  0.0538, -0.6634, -0.1498],\n","        [-0.5228, -0.3272,  0.2357,  0.0646],\n","        [-0.0029, -0.3868,  0.0338, -0.4125],\n","        [ 0.2891, -0.0398, -0.0495,  0.1878]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.9567, -0.7677],\n","        [ 0.7613,  0.7944],\n","        [-0.8236,  0.9583],\n","        [ 0.9266,  0.6994]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 0.9568, -1.1057],\n","        [-0.8834,  0.4125],\n","        [-0.7073, -0.5611],\n","        [ 0.4611, -0.1697]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.2815],\n","        [ 0.3775],\n","        [-0.4429],\n","        [ 0.0314]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 0.6753, -1.3871],\n","        [-0.5059,  0.7901],\n","        [-1.1502, -1.0040],\n","        [ 0.4925, -0.1383]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.5885, -0.8825],\n","        [-0.4667,  0.6585],\n","        [-0.8178, -0.7633],\n","        [ 0.4562, -0.1374]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[ 0.8917, -0.4731, -0.0835,  0.1857]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.5885, -0.8825],\n","        [-0.4667,  0.6585],\n","        [-0.8178, -0.7633],\n","        [ 0.4562, -0.1374]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 0.8985, -1.0603]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[0.0872]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9857, -0.9731]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9857, -0.9731]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9857, -0.9731]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(0.0005)"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","\n","# loss = torch.sum(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":192,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9993, -0.9989]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-0.0007,  0.0011]])\n","squared_difference: tensor([[5.2957e-07, 1.1297e-06]])\n"]},{"data":{"text/plain":["tensor(8.2963e-07)"]},"execution_count":192,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"data":{"text/plain":["[0.9856690168380737, -0.9730886816978455]"]},"execution_count":167,"metadata":{},"output_type":"execute_result"}],"source":["# for item in output.item:\n","#   print(item)\n","# type(output)\n","output.tolist()[0]\n"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0004647980655736461\n"]}],"source":["import numpy as np\n","\n","def mse_loss(y_true, y_pred):\n","  \"\"\"Calculates the mean squared error loss.\n","\n","  Args:\n","    y_true: The ground truth labels.\n","    y_pred: The predicted labels.\n","\n","  Returns:\n","    The mean squared error loss.\n","  \"\"\"\n","\n","  loss = np.mean((y_true - y_pred)**2)\n","  return loss\n","\n","def main():\n","  \"\"\"Main function.\"\"\"\n","\n","  # y_true = np.array([1, 2, 3, 4, 5])\n","  y_true = np.array([1.0, -1.0])\n","\n","  # y_pred = np.array([0, 1, 2, 3, 4])\n","  # y_pred = np.array([0.9997345209121704, -0.9980572462081909])\n","  y_pred = np.array(output.tolist()[0])  \n","\n","  loss = mse_loss(y_true, y_pred)\n","  print(loss)\n","\n","if __name__ == \"__main__\":\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["len(squared_difference)\n"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.0005)"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(0.0009)"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- torch results matched backward pass results ----\n","x0.data.item()  = -3.000000\n","x0.grad.item()  =  1.000000\n","w0.data.item()  =  2.000000\n","w0.grad.item()  = -1.500000 <-- result matched micrograd\n","---\n","x1.data.item()  =  0.000000\n","x1.grad.item()  =  0.500000\n","w1.data.item()  =  1.000000\n","w1.grad.item()  =  0.000000\n","---\n","x2.data.item()  =  0.500000\n","x2.grad.item()  =  0.500000\n","w2.data.item()  =  1.000000\n","w2.grad.item()  =  0.250000\n","---\n","out.data.item() = -0.707107 <-- result matched micrograd\n"]}],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <-- result matched micrograd')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <-- result matched micrograd')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["parameters in MLP: 41\n","\n","i:  0,  -0.0595583587\n","i:  1,   0.0479337552\n","i:  2,  -0.5171004357\n","i:  3,  -0.9632198723\n","i:  4,   0.3617205167\n","---\n","i: 36,  -0.1352628129\n","i: 37,   0.4807745370\n","i: 38,  -0.0210609339\n","i: 39,  -0.6264901106\n","i: 40,   0.5305867788\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'parameters in MLP: {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 3)\n","xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","layer: 0, neuron 0\n","w0: -0.0595584,   w0.grad:  0.0000000\n","w1:  0.0479338,   w1.grad:  0.0000000\n","w2: -0.5171004,   w2.grad:  0.0000000\n","b:  -0.9632199\n","\n","layer: 0, neuron 1\n","w0:  0.3617205,   w0.grad:  0.0000000\n","w1: -0.1557562,   w1.grad:  0.0000000\n","w2:  0.6212911,   w2.grad:  0.0000000\n","b:  -0.7099469\n","\n","layer: 0, neuron 2\n","w0:  0.8091953,   w0.grad:  0.0000000\n","w1:  0.8478112,   w1.grad:  0.0000000\n","w2:  0.1197954,   w2.grad:  0.0000000\n","b:  -0.1039363\n","\n","layer: 0, neuron 3\n","w0:  0.2065243,   w0.grad:  0.0000000\n","w1:  0.4537167,   w1.grad:  0.0000000\n","w2: -0.8501041,   w2.grad:  0.0000000\n","b:   0.7787010\n","\n","------\n","layer: 1, neuron_cnt: 4\n","----\n","layer: 1, neuron 0\n","w0:  0.3733739,   w0.grad:  0.0000000\n","w1: -0.0950549,   w1.grad:  0.0000000\n","w2:  0.9266131,   w2.grad:  0.0000000\n","w3:  0.2690674,   w3.grad:  0.0000000\n","b:  -0.1632371\n","\n","layer: 1, neuron 1\n","w0: -0.3290612,   w0.grad:  0.0000000\n","w1:  0.8096186,   w1.grad:  0.0000000\n","w2: -0.5836620,   w2.grad:  0.0000000\n","w3: -0.8749482,   w3.grad:  0.0000000\n","b:   0.6982834\n","\n","layer: 1, neuron 2\n","w0:  0.0921258,   w0.grad:  0.0000000\n","w1: -0.3371598,   w1.grad:  0.0000000\n","w2: -0.3895054,   w2.grad:  0.0000000\n","w3: -0.1276331,   w3.grad:  0.0000000\n","b:   0.4822092\n","\n","layer: 1, neuron 3\n","w0: -0.3557605,   w0.grad:  0.0000000\n","w1:  0.7711136,   w1.grad:  0.0000000\n","w2:  0.0050812,   w2.grad:  0.0000000\n","w3:  0.8763464,   w3.grad:  0.0000000\n","b:  -0.6131516\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0: -0.1352628,   w0.grad:  0.0000000\n","w1:  0.4807745,   w1.grad:  0.0000000\n","w2: -0.0210609,   w2.grad:  0.0000000\n","w3: -0.6264901,   w3.grad:  0.0000000\n","b:   0.5305868\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(4, 3):\n","[[-0.05955836  0.04793376 -0.51710044]\n"," [ 0.36172052 -0.1557562   0.6212911 ]\n"," [ 0.80919532  0.84781121  0.11979539]\n"," [ 0.20652425  0.45371666 -0.85010415]]\n","b_mat(1, 4):\n","[[-0.96321987 -0.70994686 -0.10393633  0.77870102]]\n","\n","i: 1\n","w_mat(4, 4):\n","[[ 0.37337389 -0.0950549   0.92661309  0.26906739]\n"," [-0.32906122  0.80961855 -0.58366203 -0.87494825]\n"," [ 0.09212576 -0.33715978 -0.38950542 -0.12763305]\n"," [-0.35576052  0.7711136   0.00508118  0.87634639]]\n","b_mat(1, 4):\n","[[-0.16323709  0.69828338  0.48220923 -0.61315159]]\n","\n","i: 2\n","w_mat(1, 4):\n","[[-0.13526281  0.48077454 -0.02106093 -0.62649011]]\n","b_mat(1, 1):\n","[[0.53058678]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[-0.05955836  0.04793376 -0.51710044]\n"," [ 0.36172052 -0.1557562   0.6212911 ]\n"," [ 0.80919532  0.84781121  0.11979539]\n"," [ 0.20652425  0.45371666 -0.85010415]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","weights_x_inputs (4, 2):\n","[[ 0.54178498 -0.48515905]\n"," [-0.36511866  1.5515633 ]\n"," [ 4.04202889  1.63967244]\n"," [ 2.62430263 -0.25919597]]\n","\n","bias (4, 1):\n","[[-0.96321987]\n"," [-0.70994686]\n"," [-0.10393633]\n"," [ 0.77870102]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-0.42143489 -1.44837892]\n"," [-1.07506552  0.84161643]\n"," [ 3.93809256  1.53573611]\n"," [ 3.40300366  0.51950505]]\n","\n","output (4, 2):\n","[[-0.39813856 -0.89537187]\n"," [-0.79136204  0.68666428]\n"," [ 0.99924093  0.91140109]\n"," [ 0.99778824  0.47731792]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[ 0.37337389 -0.0950549   0.92661309  0.26906739]\n"," [-0.32906122  0.80961855 -0.58366203 -0.87494825]\n"," [ 0.09212576 -0.33715978 -0.38950542 -0.12763305]\n"," [-0.35576052  0.7711136   0.00508118  0.87634639]]\n","\n","input (4, 2):\n","[[-0.39813856 -0.89537187]\n"," [-0.79136204  0.68666428]\n"," [ 0.99924093  0.91140109]\n"," [ 0.99778824  0.47731792]]\n","\n","weights_x_inputs (4, 2):\n","[[ 1.1209503   0.57336759]\n"," [-1.9659215  -0.09901038]\n"," [-0.28642388 -0.7299196 ]\n"," [ 0.4108974   1.27096096]]\n","\n","bias (4, 1):\n","[[-0.16323709]\n"," [ 0.69828338]\n"," [ 0.48220923]\n"," [-0.61315159]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[ 0.95771321  0.4101305 ]\n"," [-1.26763812  0.599273  ]\n"," [ 0.19578535 -0.24771037]\n"," [-0.20225419  0.65780937]]\n","\n","output (4, 2):\n","[[ 0.7432551   0.38858348]\n"," [-0.85315622  0.53653204]\n"," [ 0.19332151 -0.24276517]\n"," [-0.19954073  0.57690371]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[-0.13526281  0.48077454 -0.02106093 -0.62649011]]\n","\n","input (4, 2):\n","[[ 0.7432551   0.38858348]\n"," [-0.85315622  0.53653204]\n"," [ 0.19332151 -0.24276517]\n"," [-0.19954073  0.57690371]]\n","\n","weights_x_inputs (1, 2):\n","[[-0.3897718  -0.15092156]]\n","\n","bias (1, 1):\n","[[0.53058678]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[0.14081498 0.37966522]]\n","\n","output (1, 2):\n","[[0.13989157 0.3624167 ]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [0.13989157 0.3624167 ]\n","desired output:     [1.0, -1.0]\n","loss:               2.5959657721625162\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [0.1398915676253791, 0.3624166971674228]\n","ys:                 [1.0, -1.0]\n","loss_data:          2.5959657721625167\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0     -0.0595583587    -0.4279813948           0.05000        -0.0381592890\n","  1      0.0479337552    -0.6196041784           0.05000         0.0789139641\n","  2     -0.5171004357     0.2058569106           0.05000        -0.5273932812\n","  3     -0.9632198723    -0.2119572507           0.05000        -0.9526220098\n","  4      0.3617205167     0.3320974174           0.05000         0.3451156459\n","  5     -0.1557561986     0.6874861087           0.05000        -0.1901305040\n","  6      0.6212910959    -0.2348996115           0.05000         0.6330360765\n","  7     -0.7099468621     0.1832614344           0.05000        -0.7191099338\n","  8      0.8091953202    -0.3608768081           0.05000         0.8272391606\n","  9      0.8478112127     0.1214899154           0.05000         0.8417367169\n"," 10      0.1197953892    -0.0605816423           0.05000         0.1228244713\n"," 11     -0.1039363307    -0.1201833925           0.05000        -0.0979271611\n"," 12      0.2065242545    -3.7966997785           0.05000         0.3963592435\n"," 13      0.4537166597     1.2834735354           0.05000         0.3895429830\n"," 14     -0.8501041458    -0.6392949119           0.05000        -0.8181394002\n"," 15      0.7787010212    -1.2639386890           0.05000         0.8418979557\n"," 16      0.3733738885     0.2027246793           0.05000         0.3632376546\n"," 17     -0.0950548981    -0.2674470109           0.05000        -0.0816825475\n"," 18      0.9266130925    -0.1457069461           0.05000         0.9338984398\n"," 19      0.2690673854    -0.0278646820           0.05000         0.2704606195\n"," 20     -0.1632370878    -0.1697120114           0.05000        -0.1547514872\n"," 21     -0.3290612204    -0.6377426993           0.05000        -0.2971740854\n"," 22      0.8096185548     0.7310762547           0.05000         0.7730647421\n"," 23     -0.5836620300     0.5180977051           0.05000        -0.6095669153\n"," 24     -0.8749482488     0.1666451792           0.05000        -0.8832805078\n"," 25      0.6982833786     0.5897291539           0.05000         0.6687969209\n"," 26      0.0921257604     0.0283902032           0.05000         0.0907062503\n"," 27     -0.3371597775    -0.0592717453           0.05000        -0.3341961902\n"," 28     -0.3895054194    -0.0085887529           0.05000        -0.3890759818\n"," 29     -0.1276330529     0.0117252933           0.05000        -0.1282193176\n"," 30      0.4822092303    -0.0127191520           0.05000         0.4828451879\n"," 31     -0.3557605205     0.4819001995           0.05000        -0.3798555305\n"," 32      0.7711135972    -1.4822112956           0.05000         0.8452241620\n"," 33      0.0050811780     0.1120827728           0.05000        -0.0005229606\n"," 34      0.8763463945     0.5400646953           0.05000         0.8493431598\n"," 35     -0.6131515897     0.0251984516           0.05000        -0.6144115122\n"," 36     -0.1352628129    -0.3337860641           0.05000        -0.1185735097\n"," 37      0.4807745370     2.7088310103           0.05000         0.3453329865\n"," 38     -0.0210609339    -0.9006569921           0.05000         0.0239719157\n"," 39     -0.6264901106     1.7020311757           0.05000        -0.7115916694\n"," 40      0.5305867788     0.6803849807           0.05000         0.4965675298\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.3484949404476199), Value(data = -0.06414098258326337)]\n","step: 0, loss: 1.3002909431025702\n","-------\n","ypred: [Value(data = 0.47591100267528946), Value(data = -0.25044545608999774)]\n","step: 1, loss: 0.8365012914129517\n","-------\n","ypred: [Value(data = 0.5536886388317345), Value(data = -0.3766863694698142)]\n","step: 2, loss: 0.587713713112591\n","-------\n","ypred: [Value(data = 0.6092386286644018), Value(data = -0.4658751640251882)]\n","step: 3, loss: 0.4379837897331969\n","-------\n","ypred: [Value(data = 0.6508921687464994), Value(data = -0.5310307229077517)]\n","step: 4, loss: 0.34180846069894866\n","-------\n","ypred: [Value(data = 0.6830544743852438), Value(data = -0.5802415553464975)]\n","step: 5, loss: 0.2766516180651416\n","-------\n","ypred: [Value(data = 0.7085733657775131), Value(data = -0.6185746023336756)]\n","step: 6, loss: 0.23041481711916093\n","-------\n","ypred: [Value(data = 0.7293237342911756), Value(data = -0.6492517019913071)]\n","step: 7, loss: 0.19629000937406896\n","-------\n","ypred: [Value(data = 0.7465571430463738), Value(data = -0.6743729383402879)]\n","step: 8, loss: 0.1702662650259542\n","-------\n","ypred: [Value(data = 0.7611282899190567), Value(data = -0.6953476778065063)]\n","step: 9, loss: 0.14987273129488254\n","-------\n","ypred: [Value(data = 0.7736362580295492), Value(data = -0.7131498716749329)]\n","step: 10, loss: 0.1335235397989723\n","-------\n","ypred: [Value(data = 0.7845118931060664), Value(data = -0.7284713225027452)]\n","step: 11, loss: 0.12016294691613959\n","-------\n","ypred: [Value(data = 0.794072520711815), Value(data = -0.741815832054465)]\n","step: 12, loss: 0.10906519130371409\n","-------\n","ypred: [Value(data = 0.8025569399739715), Value(data = -0.7535585749806367)]\n","step: 13, loss: 0.09971713791801635\n","-------\n","ypred: [Value(data = 0.8101483433494049), Value(data = -0.7639845489198351)]\n","step: 14, loss: 0.09174694468154915\n","-------\n","ypred: [Value(data = 0.8169896909955263), Value(data = -0.7733141137091762)]\n","step: 15, loss: 0.08487926424536922\n","-------\n","ypred: [Value(data = 0.8231942606684759), Value(data = -0.7817203599361628)]\n","step: 16, loss: 0.07890627072696516\n","-------\n","ypred: [Value(data = 0.8288530414569258), Value(data = -0.7893411798063328)]\n","step: 17, loss: 0.07366841994393256\n","-------\n","ypred: [Value(data = 0.8340400170076082), Value(data = -0.7962878241420579)]\n","step: 18, loss: 0.0690413665476121\n","-------\n","ypred: [Value(data = 0.8388160081845653), Value(data = -0.80265107906619)]\n","step: 19, loss: 0.06492687581129732\n","-------\n","ypred: [Value(data = 0.843231513542327), Value(data = -0.8085057986254451)]\n","step: 20, loss: 0.06124638750630819\n","-------\n","ypred: [Value(data = 0.8473288402357165), Value(data = -0.8139142814118628)]\n","step: 21, loss: 0.05793637768623475\n","-------\n","ypred: [Value(data = 0.8511437243584774), Value(data = -0.8189288208957831)]\n","step: 22, loss: 0.054944962700056346\n","-------\n","ypred: [Value(data = 0.8547065783882393), Value(data = -0.8235936561513552)]\n","step: 23, loss: 0.052229376513699174\n","-------\n","ypred: [Value(data = 0.858043462530472), Value(data = -0.8279464813690101)]\n","step: 24, loss: 0.0497540718036419\n","-------\n","ypred: [Value(data = 0.8611768490272981), Value(data = -0.8320196265088943)]\n","step: 25, loss: 0.04748927312420097\n","-------\n","ypred: [Value(data = 0.8641262293950353), Value(data = -0.8358409899087096)]\n","step: 26, loss: 0.04540986213256297\n","-------\n","ypred: [Value(data = 0.866908601199631), Value(data = -0.839434781728998)]\n","step: 27, loss: 0.043494509753053384\n","-------\n","ypred: [Value(data = 0.8695388615138883), Value(data = -0.8428221216582277)]\n","step: 28, loss: 0.041724994095113405\n","-------\n","ypred: [Value(data = 0.872030127405667), Value(data = -0.8460215232589682)]\n","step: 29, loss: 0.0400856595912983\n","-------\n","ypred: [Value(data = 0.874393998870135), Value(data = -0.8490492893560773)]\n","step: 30, loss: 0.038562984563740924\n","-------\n","ypred: [Value(data = 0.8766407759930963), Value(data = -0.8519198370343884)]\n","step: 31, loss: 0.037145232811507514\n","-------\n","ypred: [Value(data = 0.8787796394408176), Value(data = -0.8546459665040883)]\n","step: 32, loss: 0.0358221708676288\n","-------\n","ypred: [Value(data = 0.8808188013541967), Value(data = -0.857239084876953)]\n","step: 33, loss: 0.034584836997420265\n","-------\n","ypred: [Value(data = 0.8827656321988048), Value(data = -0.8597093934755913)]\n","step: 34, loss: 0.03342535127273239\n","-------\n","ypred: [Value(data = 0.8846267679576896), Value(data = -0.8620660454587211)]\n","step: 35, loss: 0.032336758487284394\n","-------\n"]},{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.8864082011578738), Value(data = -0.8643172791368611)]\n","step: 36, loss: 0.03131289750501454\n","-------\n","ypred: [Value(data = 0.888115358526941), Value(data = -0.8664705312654679)]\n","step: 37, loss: 0.030348292018081334\n","-------\n","ypred: [Value(data = 0.8897531675337693), Value(data = -0.8685325337570236)]\n","step: 38, loss: 0.029438058749185286\n","-------\n","ypred: [Value(data = 0.8913261136410932), Value(data = -0.8705093965914892)]\n","step: 39, loss: 0.028577829947448834\n","-------\n","ypred: [Value(data = 0.8928382897601619), Value(data = -0.8724066791826914)]\n","step: 40, loss: 0.027763687658715654\n","-------\n","ypred: [Value(data = 0.8942934391291157), Value(data = -0.874229452044284)]\n","step: 41, loss: 0.026992107744231008\n","-------\n","ypred: [Value(data = 0.8956949926215435), Value(data = -0.8759823502686772)]\n","step: 42, loss: 0.02625991200910093\n","-------\n","ypred: [Value(data = 0.8970461013184136), Value(data = -0.8776696200673255)]\n","step: 43, loss: 0.025564227108210852\n","-------\n","ypred: [Value(data = 0.8983496650362812), Value(data = -0.8792951594069848)]\n","step: 44, loss: 0.024902449140821445\n","-------\n","ypred: [Value(data = 0.8996083573905341), Value(data = -0.8808625536031964)]\n","step: 45, loss: 0.024272213039777966\n","-------\n","ypred: [Value(data = 0.900824647879093), Value(data = -0.8823751065909992)]\n","step: 46, loss: 0.023671366017784704\n","-------\n","ypred: [Value(data = 0.9020008213953365), Value(data = -0.8838358684772336)]\n","step: 47, loss: 0.023097944459627297\n","-------\n","ypred: [Value(data = 0.903138995515763), Value(data = -0.8852476598836938)]\n","step: 48, loss: 0.02255015375186379\n","-------\n","ypred: [Value(data = 0.9042411358555112), Value(data = -0.8866130935118577)]\n","step: 49, loss: 0.022026350625193383\n","-------\n","ypred: [Value(data = 0.9053090697412761), Value(data = -0.8879345932947881)]\n","step: 50, loss: 0.021525027653267068\n","-------\n","ypred: [Value(data = 0.9063444984147607), Value(data = -0.8892144114475864)]\n","step: 51, loss: 0.02104479960808743\n","-------\n","ypred: [Value(data = 0.9073490079493053), Value(data = -0.8904546436824641)]\n","step: 52, loss: 0.020584391418713804\n","-------\n","ypred: [Value(data = 0.908324079036678), Value(data = -0.891657242816504)]\n","step: 53, loss: 0.020142627518595226\n","-------\n","ypred: [Value(data = 0.9092710957793628), Value(data = -0.8928240309681957)]\n","step: 54, loss: 0.01971842239898382\n","-------\n","ypred: [Value(data = 0.9101913536053425), Value(data = -0.8939567105118286)]\n","step: 55, loss: 0.01931077221271275\n","-------\n","ypred: [Value(data = 0.9110860664068061), Value(data = -0.8950568739359497)]\n","step: 56, loss: 0.018918747295110047\n","-------\n","ypred: [Value(data = 0.9119563729909398), Value(data = -0.8961260127326431)]\n","step: 57, loss: 0.01854148548772953\n","-------\n","ypred: [Value(data = 0.9128033429196234), Value(data = -0.8971655254278311)]\n","step: 58, loss: 0.01817818616652684\n","-------\n","ypred: [Value(data = 0.9136279818051414), Value(data = -0.8981767248486227)]\n","step: 59, loss: 0.017828104889606088\n","-------\n","ypred: [Value(data = 0.9144312361206669), Value(data = -0.8991608447116008)]\n","step: 60, loss: 0.01749054859111494\n","-------\n","ypred: [Value(data = 0.9152139975770867), Value(data = -0.900119045605504)]\n","step: 61, loss: 0.017164871257613647\n","-------\n","ypred: [Value(data = 0.9159771071115251), Value(data = -0.9010524204327659)]\n","step: 62, loss: 0.016850470031562242\n","-------\n","ypred: [Value(data = 0.9167213585275452), Value(data = -0.901961999366612)]\n","step: 63, loss: 0.016546781693689845\n","-------\n","ypred: [Value(data = 0.9174475018223365), Value(data = -0.9028487543736843)]\n","step: 64, loss: 0.01625327948211787\n","-------\n","ypred: [Value(data = 0.9181562462321438), Value(data = -0.9037136033463287)]\n","step: 65, loss: 0.015969470211361615\n","-------\n","ypred: [Value(data = 0.9188482630236449), Value(data = -0.9045574138836027)]\n","step: 66, loss: 0.015694891658865447\n","-------\n","ypred: [Value(data = 0.9195241880558973), Value(data = -0.9053810067556282)]\n","step: 67, loss: 0.015429110190641065\n","-------\n","ypred: [Value(data = 0.9201846241347647), Value(data = -0.9061851590820525)]\n","step: 68, loss: 0.015171718600968581\n","-------\n","ypred: [Value(data = 0.9208301431793569), Value(data = -0.9069706072519877)]\n","step: 69, loss: 0.01492233414406505\n","-------\n","ypred: [Value(data = 0.9214612882179327), Value(data = -0.9077380496098286)]\n","step: 70, loss: 0.014680596738185078\n","-------\n","ypred: [Value(data = 0.9220785752288704), Value(data = -0.9084881489287386)]\n","step: 71, loss: 0.014446167324851532\n","-------\n","ypred: [Value(data = 0.9226824948406938), Value(data = -0.909221534691295)]\n","step: 72, loss: 0.014218726367863112\n","-------\n","ypred: [Value(data = 0.9232735139037032), Value(data = -0.9099388051947441)]\n","step: 73, loss: 0.01399797247843548\n","-------\n","ypred: [Value(data = 0.923852076944492), Value(data = -0.9106405294965437)]\n","step: 74, loss: 0.013783621154325638\n","-------\n","ypred: [Value(data = 0.9244186075135037), Value(data = -0.9113272492142567)]\n","step: 75, loss: 0.013575403622108346\n","-------\n","ypred: [Value(data = 0.9249735094347823), Value(data = -0.9119994801924741)]\n","step: 76, loss: 0.013373065772927455\n","-------\n","ypred: [Value(data = 0.9255171679661787), Value(data = -0.9126577140481801)]\n","step: 77, loss: 0.01317636718306792\n","-------\n","ypred: [Value(data = 0.926049950877482), Value(data = -0.9133024196048621)]\n","step: 78, loss: 0.01298508021159422\n","-------\n","ypred: [Value(data = 0.9265722094532362), Value(data = -0.9139340442246817)]\n","step: 79, loss: 0.012798989168098466\n","-------\n","ypred: [Value(data = 0.9270842794263682), Value(data = -0.9145530150471274)]\n","step: 80, loss: 0.012617889544308395\n","-------\n","ypred: [Value(data = 0.9275864818481885), Value(data = -0.9151597401417874)]\n","step: 81, loss: 0.012441587303931783\n","-------\n","ypred: [Value(data = 0.9280791238998137), Value(data = -0.9157546095821675)]\n","step: 82, loss: 0.012269898225671372\n","-------\n","ypred: [Value(data = 0.9285624996496086), Value(data = -0.916337996446846)]\n","step: 83, loss: 0.012102647294840129\n","-------\n","ypred: [Value(data = 0.9290368907608287), Value(data = -0.9169102577536858)]\n","step: 84, loss: 0.011939668139449477\n","-------\n","ypred: [Value(data = 0.9295025671532829), Value(data = -0.917471735332314)]\n","step: 85, loss: 0.011780802507037016\n","-------\n","ypred: [Value(data = 0.9299597876224954), Value(data = -0.9180227566396167)]\n","step: 86, loss: 0.011625899778853446\n","-------\n","ypred: [Value(data = 0.9304088004195499), Value(data = -0.9185636355225822)]\n","step: 87, loss: 0.01147481651834487\n","-------\n","ypred: [Value(data = 0.9308498437945284), Value(data = -0.9190946729324491)]\n","step: 88, loss: 0.011327416051148512\n","-------\n","ypred: [Value(data = 0.9312831465062076), Value(data = -0.9196161575937802)]\n","step: 89, loss: 0.011183568074075306\n","-------\n","ypred: [Value(data = 0.9317089283004576), Value(data = -0.9201283666317764)]\n","step: 90, loss: 0.011043148290779965\n","-------\n","ypred: [Value(data = 0.9321274003595836), Value(data = -0.9206315661608635)]\n","step: 91, loss: 0.010906038072025635\n","-------\n","ypred: [Value(data = 0.9325387657246702), Value(data = -0.9211260118373394)]\n","step: 92, loss: 0.010772124138634449\n","-------\n","ypred: [Value(data = 0.9329432196928179), Value(data = -0.9216119493786397)]\n","step: 93, loss: 0.010641298265382636\n","-------\n","ypred: [Value(data = 0.9333409501910174), Value(data = -0.9220896150515671)]\n","step: 94, loss: 0.010513457004249434\n","-------\n","ypred: [Value(data = 0.9337321381282614), Value(data = -0.9225592361316496)]\n","step: 95, loss: 0.010388501425565435\n","-------\n","ypred: [Value(data = 0.9341169577273737), Value(data = -0.923021031335619)]\n","step: 96, loss: 0.010266336875728423\n","-------\n","ypred: [Value(data = 0.9344955768379186), Value(data = -0.9234752112288371)]\n","step: 97, loss: 0.010146872750268118\n","-------\n","ypred: [Value(data = 0.9348681572314511), Value(data = -0.9239219786093688)]\n","step: 98, loss: 0.010030022281140319\n","-------\n","ypred: [Value(data = 0.9352348548802686), Value(data = -0.9243615288702561)]\n","step: 99, loss: 0.009915702337224967\n","-------\n","ypred: [Value(data = 0.9355958202207435), Value(data = -0.9247940503414431)]\n","step: 100, loss: 0.009803833237084182\n","-------\n","ypred: [Value(data = 0.9359511984022304), Value(data = -0.9252197246126755)]\n","step: 101, loss: 0.009694338573114539\n","-------\n","ypred: [Value(data = 0.9363011295224714), Value(data = -0.9256387268386204)]\n","step: 102, loss: 0.00958714504629429\n","-------\n","ypred: [Value(data = 0.9366457488503545), Value(data = -0.9260512260273378)]\n","step: 103, loss: 0.009482182310792247\n","-------\n","ypred: [Value(data = 0.9369851870368192), Value(data = -0.9264573853131713)]\n","step: 104, loss: 0.009379382827760005\n","-------\n","ypred: [Value(data = 0.9373195703146462), Value(data = -0.9268573622150372)]\n","step: 105, loss: 0.009278681727682847\n","-------\n","ypred: [Value(data = 0.9376490206878131), Value(data = -0.9272513088810196)]\n","step: 106, loss: 0.00918001668071358\n","-------\n","ypred: [Value(data = 0.9379736561110568), Value(data = -0.9276393723201285)]\n","step: 107, loss: 0.009083327774454432\n","-------\n","ypred: [Value(data = 0.9382935906602303), Value(data = -0.9280216946219956)]\n","step: 108, loss: 0.008988557398696475\n","-------\n","ypred: [Value(data = 0.9386089346940089), Value(data = -0.9283984131652488)]\n","step: 109, loss: 0.008895650136658878\n","-------\n","ypred: [Value(data = 0.9389197950074548), Value(data = -0.9287696608152449)]\n","step: 110, loss: 0.008804552662306595\n","-------\n","ypred: [Value(data = 0.9392262749779259), Value(data = -0.929135566111791)]\n","step: 111, loss: 0.008715213643355016\n","-------\n","ypred: [Value(data = 0.9395284747037658), Value(data = -0.9294962534474511)]\n","step: 112, loss: 0.008627583649599143\n","-------\n","ypred: [Value(data = 0.9398264911362003), Value(data = -0.9298518432369867)]\n","step: 113, loss: 0.008541615066230072\n","-------\n","ypred: [Value(data = 0.9401204182048263), Value(data = -0.9302024520784465)]\n","step: 114, loss: 0.008457262011826451\n","-------\n","ypred: [Value(data = 0.9404103469370559), Value(data = -0.9305481929063881)]\n","step: 115, loss: 0.008374480260730314\n","-------\n","ypred: [Value(data = 0.9406963655718585), Value(data = -0.930889175137675)]\n","step: 116, loss: 0.008293227169537605\n","-------\n","ypred: [Value(data = 0.9409785596681166), Value(data = -0.9312255048102765)]\n","step: 117, loss: 0.008213461607451365\n","-------\n","ypred: [Value(data = 0.9412570122078963), Value(data = -0.9315572847154561)]\n","step: 118, loss: 0.008135143890264393\n","-------\n","ypred: [Value(data = 0.9415318036949091), Value(data = -0.9318846145237201)]\n","step: 119, loss: 0.008058235717752856\n","-------\n","ypred: [Value(data = 0.9418030122484288), Value(data = -0.9322075909048713)]\n","step: 120, loss: 0.007982700114277826\n","-------\n","ypred: [Value(data = 0.9420707136929068), Value(data = -0.9325263076424841)]\n","step: 121, loss: 0.007908501372405867\n","-------\n","ypred: [Value(data = 0.9423349816435194), Value(data = -0.9328408557431137)]\n","step: 122, loss: 0.007835604999370511\n","-------\n","ypred: [Value(data = 0.942595887587858), Value(data = -0.9331513235405121)]\n","step: 123, loss: 0.007763977666211131\n","-------\n","ypred: [Value(data = 0.9428535009639719), Value(data = -0.9334577967951256)]\n","step: 124, loss: 0.007693587159433551\n","-------\n","ypred: [Value(data = 0.9431078892349495), Value(data = -0.9337603587891191)]\n","step: 125, loss: 0.0076244023350490065\n","-------\n","ypred: [Value(data = 0.9433591179602184), Value(data = -0.9340590904171644)]\n","step: 126, loss: 0.007556393074856151\n","-------\n","ypred: [Value(data = 0.9436072508637359), Value(data = -0.9343540702732114)]\n","step: 127, loss: 0.007489530244840089\n","-------\n","ypred: [Value(data = 0.9438523498992244), Value(data = -0.9346453747334522)]\n","step: 128, loss: 0.007423785655570012\n","-------\n","ypred: [Value(data = 0.9440944753126053), Value(data = -0.9349330780356707)]\n","step: 129, loss: 0.007359132024485019\n","-------\n","ypred: [Value(data = 0.9443336857017706), Value(data = -0.935217252355165)]\n","step: 130, loss: 0.007295542939963644\n","-------\n","ypred: [Value(data = 0.9445700380738236), Value(data = -0.9354979678774128)]\n","step: 131, loss: 0.007232992827080641\n","-------\n","ypred: [Value(data = 0.9448035878999177), Value(data = -0.9357752928676448)]\n","step: 132, loss: 0.007171456914958906\n","-------\n","ypred: [Value(data = 0.9450343891678076), Value(data = -0.9360492937374788)]\n","step: 133, loss: 0.007110911205631297\n","-------\n","ypred: [Value(data = 0.945262494432227), Value(data = -0.9363200351087612)]\n","step: 134, loss: 0.007051332444331393\n","-------\n","ypred: [Value(data = 0.9454879548631965), Value(data = -0.9365875798747522)]\n","step: 135, loss: 0.0069926980911378415\n","-------\n","ypred: [Value(data = 0.9457108202923629), Value(data = -0.9368519892587825)]\n","step: 136, loss: 0.006934986293901034\n","-------\n","ypred: [Value(data = 0.9459311392574618), Value(data = -0.9371133228705056)]\n","step: 137, loss: 0.006878175862385263\n","-------\n","ypred: [Value(data = 0.9461489590449943), Value(data = -0.9373716387598608)]\n","step: 138, loss: 0.006822246243563066\n","-------\n","ypred: [Value(data = 0.9463643257312009), Value(data = -0.9376269934688557)]\n","step: 139, loss: 0.00676717749800289\n","-------\n","ypred: [Value(data = 0.9465772842214115), Value(data = -0.9378794420812686)]\n","step: 140, loss: 0.006712950277294308\n","-------\n","ypred: [Value(data = 0.9467878782878488), Value(data = -0.9381290382703743)]\n","step: 141, loss: 0.006659545802457598\n","-------\n","ypred: [Value(data = 0.9469961506059542), Value(data = -0.9383758343447797)]\n","step: 142, loss: 0.006606945843288725\n","-------\n","ypred: [Value(data = 0.947202142789305), Value(data = -0.9386198812924593)]\n","step: 143, loss: 0.006555132698592719\n","-------\n","ypred: [Value(data = 0.947405895423187), Value(data = -0.9388612288230739)]\n","step: 144, loss: 0.006504089177261276\n","-------\n","ypred: [Value(data = 0.9476074480968815), Value(data = -0.9390999254086498)]\n","step: 145, loss: 0.006453798580152983\n","-------\n","ypred: [Value(data = 0.9478068394347272), Value(data = -0.9393360183226912)]\n","step: 146, loss: 0.0064042446827372035\n","-------\n","ypred: [Value(data = 0.9480041071260089), Value(data = -0.9395695536778017)]\n","step: 147, loss: 0.006355411718463647\n","-------\n","ypred: [Value(data = 0.9481992879537255), Value(data = -0.9398005764618736)]\n","step: 148, loss: 0.006307284362823764\n","-------\n","ypred: [Value(data = 0.9483924178222881), Value(data = -0.9400291305729171)]\n","step: 149, loss: 0.006259847718069506\n","-------\n","ypred: [Value(data = 0.9485835317841916), Value(data = -0.9402552588525837)]\n","step: 150, loss: 0.006213087298559015\n","-------\n","ypred: [Value(data = 0.9487726640657088), Value(data = -0.9404790031184408)]\n","step: 151, loss: 0.006166989016699302\n","-------\n","ypred: [Value(data = 0.9489598480916445), Value(data = -0.9407004041950574)]\n","step: 152, loss: 0.006121539169457575\n","-------\n","ypred: [Value(data = 0.9491451165091925), Value(data = -0.9409195019439464)]\n","step: 153, loss: 0.0060767244254149606\n","-------\n","ypred: [Value(data = 0.9493285012109333), Value(data = -0.9411363352924141)]\n","step: 154, loss: 0.006032531812337481\n","-------\n","ypred: [Value(data = 0.9495100333570081), Value(data = -0.9413509422613682)]\n","step: 155, loss: 0.005988948705239802\n","-------\n","ypred: [Value(data = 0.9496897433965042), Value(data = -0.9415633599921193)]\n","step: 156, loss: 0.0059459628149202345\n","-------\n","ypred: [Value(data = 0.9498676610880841), Value(data = -0.9417736247722284)]\n","step: 157, loss: 0.0059035621769444584\n","-------\n","ypred: [Value(data = 0.9500438155198901), Value(data = -0.941981772060433)]\n","step: 158, loss: 0.005861735141058323\n","-------\n","ypred: [Value(data = 0.9502182351287551), Value(data = -0.9421878365106935)]\n","step: 159, loss: 0.005820470361010214\n","-------\n","ypred: [Value(data = 0.9503909477187452), Value(data = -0.9423918519953958)]\n","step: 160, loss: 0.0057797567847646565\n","-------\n","ypred: [Value(data = 0.9505619804790642), Value(data = -0.9425938516277448)]\n","step: 161, loss: 0.005739583645089814\n","-------\n","ypred: [Value(data = 0.9507313600013451), Value(data = -0.9427938677833804)]\n","step: 162, loss: 0.005699940450502428\n","-------\n","ypred: [Value(data = 0.9508991122963514), Value(data = -0.9429919321212501)]\n","step: 163, loss: 0.005660816976554468\n","-------\n","ypred: [Value(data = 0.951065262810114), Value(data = -0.9431880756037668)]\n","step: 164, loss: 0.00562220325744652\n","-------\n","ypred: [Value(data = 0.9512298364395254), Value(data = -0.9433823285162791)]\n","step: 165, loss: 0.005584089577953984\n","-------\n","ypred: [Value(data = 0.9513928575474117), Value(data = -0.9435747204858848)]\n","step: 166, loss: 0.00554646646565225\n","-------\n","ypred: [Value(data = 0.9515543499771034), Value(data = -0.9437652804996108)]\n","step: 167, loss: 0.005509324683428429\n","-------\n","ypred: [Value(data = 0.9517143370665269), Value(data = -0.9439540369219857)]\n","step: 168, loss: 0.00547265522226713\n","-------\n","ypred: [Value(data = 0.9518728416618292), Value(data = -0.9441410175120283)]\n","step: 169, loss: 0.005436449294298897\n","-------\n","ypred: [Value(data = 0.9520298861305616), Value(data = -0.9443262494396746)]\n","step: 170, loss: 0.00540069832610022\n","-------\n","ypred: [Value(data = 0.9521854923744327), Value(data = -0.9445097593016663)]\n","step: 171, loss: 0.005365393952234445\n","-------\n","ypred: [Value(data = 0.952339681841651), Value(data = -0.9446915731369203)]\n","step: 172, loss: 0.005330528009023689\n","-------\n","ypred: [Value(data = 0.9524924755388705), Value(data = -0.9448717164413986)]\n","step: 173, loss: 0.005296092528542383\n","-------\n","ypred: [Value(data = 0.9526438940427563), Value(data = -0.9450502141824995)]\n","step: 174, loss: 0.005262079732822876\n","-------\n","ypred: [Value(data = 0.9527939575111818), Value(data = -0.9452270908129857)]\n","step: 175, loss: 0.005228482028265024\n","-------\n","ypred: [Value(data = 0.9529426856940738), Value(data = -0.9454023702844699)]\n","step: 176, loss: 0.005195292000240857\n","-------\n","ypred: [Value(data = 0.9530900979439166), Value(data = -0.9455760760604713)]\n","step: 177, loss: 0.005162502407886935\n","-------\n","ypred: [Value(data = 0.953236213225929), Value(data = -0.9457482311290598)]\n","step: 178, loss: 0.0051301061790766924\n","-------\n","ypred: [Value(data = 0.9533810501279247), Value(data = -0.945918858015105)]\n","step: 179, loss: 0.005098096405565439\n","-------\n","ypred: [Value(data = 0.9535246268698697), Value(data = -0.9460879787921425)]\n","step: 180, loss: 0.005066466338301317\n","-------\n","ypred: [Value(data = 0.9536669613131453), Value(data = -0.946255615093872)]\n","step: 181, loss: 0.00503520938289561\n","-------\n","ypred: [Value(data = 0.9538080709695291), Value(data = -0.9464217881253005)]\n","step: 182, loss: 0.005004319095246244\n","-------\n","ypred: [Value(data = 0.9539479730099033), Value(data = -0.9465865186735448)]\n","step: 183, loss: 0.004973789177308171\n","-------\n","ypred: [Value(data = 0.9540866842726997), Value(data = -0.9467498271183022)]\n","step: 184, loss: 0.004943613473005459\n","-------\n","ypred: [Value(data = 0.9542242212720923), Value(data = -0.9469117334420062)]\n","step: 185, loss: 0.004913785964278973\n","-------\n","ypred: [Value(data = 0.9543606002059437), Value(data = -0.9470722572396731)]\n","step: 186, loss: 0.004884300767265037\n","-------\n","ypred: [Value(data = 0.9544958369635168), Value(data = -0.9472314177284551)]\n","step: 187, loss: 0.004855152128599646\n","-------\n","ypred: [Value(data = 0.9546299471329579), Value(data = -0.9473892337569064)]\n","step: 188, loss: 0.004826334421843629\n","-------\n","ypred: [Value(data = 0.9547629460085603), Value(data = -0.9475457238139752)]\n","step: 189, loss: 0.004797842144024205\n","-------\n","ypred: [Value(data = 0.9548948485978148), Value(data = -0.9477009060377302)]\n","step: 190, loss: 0.0047696699122883775\n","-------\n","ypred: [Value(data = 0.9550256696282566), Value(data = -0.9478547982238291)]\n","step: 191, loss: 0.004741812460664293\n","-------\n","ypred: [Value(data = 0.9551554235541124), Value(data = -0.9480074178337409)]\n","step: 192, loss: 0.004714264636926261\n","-------\n","ypred: [Value(data = 0.955284124562757), Value(data = -0.9481587820027289)]\n","step: 193, loss: 0.00468702139955962\n","-------\n","ypred: [Value(data = 0.9554117865809857), Value(data = -0.9483089075476024)]\n","step: 194, loss: 0.0046600778148218796\n","-------\n","ypred: [Value(data = 0.9555384232811063), Value(data = -0.9484578109742453)]\n","step: 195, loss: 0.0046334290538967\n","-------\n","ypred: [Value(data = 0.95566404808686), Value(data = -0.9486055084849284)]\n","step: 196, loss: 0.004607070390137026\n","-------\n","ypred: [Value(data = 0.9557886741791746), Value(data = -0.9487520159854153)]\n","step: 197, loss: 0.004580997196394319\n","-------\n","ypred: [Value(data = 0.9559123145017563), Value(data = -0.9488973490918644)]\n","step: 198, loss: 0.004555204942430821\n","-------\n","ypred: [Value(data = 0.9560349817665255), Value(data = -0.9490415231375381)]\n","step: 199, loss: 0.004529689192411815\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 1.164993405342102\n","Epoch 10 loss: 0.19609788060188293\n","Epoch 20 loss: 0.005849550478160381\n","Epoch 30 loss: 7.394792919512838e-05\n","\n","Prediction:\n","tensor([[ 0.9993],\n","        [-0.9989]])\n","Loss: 1.3012120234634494e-06\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[ 0.1047, -0.3425,  0.0168],\n","        [-0.4104,  0.5775, -0.3041],\n","        [ 0.0247, -0.5172, -0.5745],\n","        [ 0.5708, -0.1385, -0.0259]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.8348,  0.6650],\n","        [ 1.2159, -1.9606],\n","        [-0.9276,  0.3041],\n","        [ 0.7518,  1.8380]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.1152],\n","        [ 0.4799],\n","        [-0.1531],\n","        [ 0.2568]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.9500,  0.5498],\n","        [ 1.6958, -1.4807],\n","        [-1.0807,  0.1510],\n","        [ 1.0086,  2.0948]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.7398,  0.5004],\n","        [ 0.9349, -0.9016],\n","        [-0.7935,  0.1498],\n","        [ 0.7652,  0.9701]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[-0.2302, -0.6287, -0.1714, -0.2861],\n","        [-0.5563,  0.4800, -0.0171,  0.0949],\n","        [ 0.2643,  0.7002, -0.4562, -0.2607],\n","        [-0.4190,  0.3951,  0.0029,  0.3185]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.7398,  0.5004],\n","        [ 0.9349, -0.9016],\n","        [-0.7935,  0.1498],\n","        [ 0.7652,  0.9701]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.5003,  0.1485],\n","        [ 0.9464, -0.6216],\n","        [ 0.6215, -0.8204],\n","        [ 0.9208, -0.2564]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[ 0.4848],\n","        [ 0.1854],\n","        [-0.0786],\n","        [ 0.2506]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.0156,  0.6332],\n","        [ 1.1318, -0.4362],\n","        [ 0.5429, -0.8989],\n","        [ 1.1714, -0.0058]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.0156,  0.5603],\n","        [ 0.8116, -0.4105],\n","        [ 0.4952, -0.7158],\n","        [ 0.8247, -0.0058]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[-0.5285,  0.4649,  0.6298,  0.4372]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.0156,  0.5603],\n","        [ 0.8116, -0.4105],\n","        [ 0.4952, -0.7158],\n","        [ 0.8247, -0.0058]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 1.0580, -0.9402]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[-0.0587]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9993, -0.9989]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9993, -0.9989]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":186,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9993, -0.9989]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(1.6593e-06)"]},"execution_count":187,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9993, -0.9989]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-0.0007,  0.0011]])\n","squared_difference: tensor([[5.2957e-07, 1.1297e-06]])\n"]},{"data":{"text/plain":["tensor(8.2963e-07)"]},"execution_count":188,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference) / 2\n","loss"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":189,"metadata":{},"output_type":"execute_result"}],"source":["difference\n","len(squared_difference)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":190,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(8.2963e-07)"]},"execution_count":190,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":191,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.6593e-06)"]},"execution_count":191,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
