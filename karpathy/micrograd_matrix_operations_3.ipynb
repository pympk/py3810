{"cells":[{"cell_type":"markdown","metadata":{},"source":["### [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import math, random, torch\n","import numpy as np\n","# import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def plot_losses(losses):\n","  # import matplotlib.pyplot as plt\n","  \n","  # Create a list of iterations\n","  iterations = range(len(losses))\n","\n","  # Plot the loss as a function of iteration\n","  plt.plot(iterations, losses)\n","\n","  # Add a title to the plot\n","  plt.title('Loss vs. Iteration')\n","\n","  # Add labels to the x-axis and y-axis\n","  plt.xlabel('Iteration')\n","  plt.ylabel('Loss')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions<br>* limited to neural network with one output, e.g. MLP(2, [3, 1])<br>* neural network with multiple outputs, e.g.  MLP(2, [3, 3]), will produce errors in backward pass "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class Neuron:\n","    \n","    def __init__(self, nin):\n","        # random numbers evenly distributed between -1 and 1    \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]  \n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["### Basic Neuron Function\n","\n","<img src=\"..\\karpathy\\img\\Basic Neuron Function.png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Simple Neural Network MLP(2, [3, 3, 1])<br>* input layer: 2 nodes<br>* hidden layer 1: 3 nodes<br>* hidden layer 2: 3 nodes<br>*  output layer: 1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\MLP (2, [3, 3, 1]).png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Hidden Layer Matrix Operations<br>* Hidden layer with two inputs (X1, X2), and three neurons (b1, b2, b3)<br>* Two sets of inputs (X1, X2) are shown in different shades of gray<br>* Two sets of outputs (Y1, Y2, Y3) are shown in corresponding shades of gray<br>* Multiple sets of inputs are processed in one matrix operation \n","\n","<img src=\"..\\karpathy\\img\\Hidden Layer Matrix Operations.png\">"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(2, [3, 3, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0],\n","  [3.0, -1.0]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"markdown","metadata":{},"source":["### Parameters in MLP(2, [3, 3, 1])<br>* parameters in layer 1: 3 neurons * (2 inputs + 1 bias) = 9<br>* parameters in layer 2: 3 neurons * (3 neurons + 1 bias) = 12<br>* parameters in layer 3: 1 output * (3 neurons + 1 bias) = 4<br>* Total 25 parameters "]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in MLP(2, [3, 3, 1]): 25\n","\n","i:  0,  -0.2694500942\n","i:  1,   0.1148735259\n","i:  2,   0.0089593754\n","i:  3,  -0.9897171276\n","i:  4,   0.4789269219\n","---\n","i: 20,   0.8752282375\n","i: 21,  -0.4997107436\n","i: 22,   0.4702356671\n","i: 23,   0.0678061778\n","i: 24,   0.8687607951\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'Number of parameters in MLP(2, [3, 3, 1]): {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### ---- Start: Manual Calculation of Output and Loss ----"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 2)\n","xs_mats:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n","\n","xs_mats_T[0].shape: (2, 2)\n","xs_mats_T:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 3\n","----\n","layer: 0, neuron 0\n","w0: -0.2694401,   w0.grad:  0.0000000\n","w1:  0.1148735,   w1.grad:  0.0000000\n","b:   0.0089594\n","\n","layer: 0, neuron 1\n","w0: -0.9897171,   w0.grad:  0.0000000\n","w1:  0.4789269,   w1.grad:  0.0000000\n","b:  -0.2001723\n","\n","layer: 0, neuron 2\n","w0:  0.9922659,   w0.grad:  0.0000000\n","w1: -0.3262280,   w1.grad:  0.0000000\n","b:   0.9601656\n","\n","------\n","layer: 1, neuron_cnt: 3\n","----\n","layer: 1, neuron 0\n","w0: -0.5985556,   w0.grad:  0.0000000\n","w1:  0.4709895,   w1.grad:  0.0000000\n","w2: -0.3488664,   w2.grad:  0.0000000\n","b:   0.0435745\n","\n","layer: 1, neuron 1\n","w0:  0.8177165,   w0.grad:  0.0000000\n","w1: -0.0456572,   w1.grad:  0.0000000\n","w2:  0.3413267,   w2.grad:  0.0000000\n","b:   0.2788525\n","\n","layer: 1, neuron 2\n","w0:  0.1335780,   w0.grad:  0.0000000\n","w1: -0.3645127,   w1.grad:  0.0000000\n","w2: -0.3221990,   w2.grad:  0.0000000\n","b:   0.8752282\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0: -0.4997107,   w0.grad:  0.0000000\n","w1:  0.4702357,   w1.grad:  0.0000000\n","w2:  0.0678062,   w2.grad:  0.0000000\n","b:   0.8687608\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(3, 2):\n","[[-0.26944009  0.11487353]\n"," [-0.98971713  0.47892692]\n"," [ 0.9922659  -0.326228  ]]\n","b_mat(1, 3):\n","[[ 0.00895938 -0.20017231  0.96016562]]\n","\n","i: 1\n","w_mat(3, 3):\n","[[-0.59855563  0.47098949 -0.34886637]\n"," [ 0.8177165  -0.04565724  0.34132668]\n"," [ 0.133578   -0.36451266 -0.322199  ]]\n","b_mat(1, 3):\n","[[0.04357448 0.27885251 0.87522824]]\n","\n","i: 2\n","w_mat(1, 3):\n","[[-0.49971074  0.47023567  0.06780618]]\n","b_mat(1, 1):\n","[[0.8687608]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (3, 2):\n","[[-0.26944009  0.11487353]\n"," [-0.98971713  0.47892692]\n"," [ 0.9922659  -0.326228  ]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[-0.19425961 -0.92319381]\n"," [-0.54265349 -3.4480783 ]\n"," [ 1.00584782  3.30302571]]\n","\n","bias (3, 1):\n","[[ 0.00895938]\n"," [-0.20017231]\n"," [ 0.96016562]]\n","\n","weights_x_inputs_plus_bias (3, 2):\n","[[-0.18530024 -0.91423443]\n"," [-0.7428258  -3.64825061]\n"," [ 1.96601344  4.26319133]]\n","\n","output (3, 2):\n","[[-0.18320813 -0.72315845]\n"," [-0.63084941 -0.99864511]\n"," [ 0.96154605  0.99960374]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (3, 3):\n","[[-0.59855563  0.47098949 -0.34886637]\n"," [ 0.8177165  -0.04565724  0.34132668]\n"," [ 0.133578   -0.36451266 -0.322199  ]]\n","\n","input (3, 2):\n","[[-0.18320813 -0.72315845]\n"," [-0.63084941 -0.99864511]\n"," [ 0.96154605  0.99960374]]\n","\n","weights_x_inputs (3, 2):\n","[[-0.52291426 -0.38622891]\n"," [ 0.20719185 -0.20455179]\n"," [-0.10432916 -0.05465061]]\n","\n","bias (3, 1):\n","[[0.04357448]\n"," [0.27885251]\n"," [0.87522824]]\n","\n","weights_x_inputs_plus_bias (3, 2):\n","[[-0.47933978 -0.34265443]\n"," [ 0.48604436  0.07430072]\n"," [ 0.77089908  0.82057763]]\n","\n","output (3, 2):\n","[[-0.44571471 -0.3298451 ]\n"," [ 0.45107126  0.07416429]\n"," [ 0.64745194  0.67538415]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 3):\n","[[-0.49971074  0.47023567  0.06780618]]\n","\n","input (3, 2):\n","[[-0.44571471 -0.3298451 ]\n"," [ 0.45107126  0.07416429]\n"," [ 0.64745194  0.67538415]]\n","\n","weights_x_inputs (1, 2):\n","[[0.47873946 0.24549705]]\n","\n","bias (1, 1):\n","[[0.8687608]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[1.34750026 1.11425785]]\n","\n","output (1, 2):\n","[[0.87346198 0.80556233]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [0.87346198 0.80556233]\n","desired output:     [1.0, -1.0]\n","err_sq:             [0.01601187 3.26005531]\n","loss_sum:           3.276067182992225\n","loss_mean:          1.6380335914961126\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","err_sq = ((yout - ys)**2)\n","loss_sum = err_sq.sum()\n","loss_mean = err_sq.mean()\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'err_sq:             {err_sq}')\n","print(f'loss_sum:           {loss_sum}')\n","print(f'loss_mean:          {loss_mean}')\n"]},{"cell_type":"markdown","metadata":{},"source":["### ---- End: Manual Calculation of Output and Loss ----"]},{"cell_type":"markdown","metadata":{},"source":["### TODO calculate w1 gradient by incrementing w1"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss_sum_before: 3.2760559586918983\n","param_before: -0.2694500942193594\n","param_after: -0.2694400942193594\n","param_dif: 1.0000000000010001e-05\n"]}],"source":["loss_sum_before = loss_sum\n","print(f'loss_sum_before: {loss_sum_before}')\n","param_before = n.parameters()[0].data\n","print(f'param_before: {param_before}')\n","n.parameters()[0].data += .00001\n","param_after = n.parameters()[0].data\n","print(f'param_after: {param_after}') \n","param_dif = param_after - param_before\n","print(f'param_dif: {param_dif}') "]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss_sum_after: 3.276067182992225\n","loss_sum_dif: 1.1224300326961867e-05\n","param_grad: 1.1224300326950643\n"]}],"source":["loss_sum_after = loss_sum\n","print(f'loss_sum_after: {loss_sum_after}')\n","loss_sum_dif = loss_sum_after - loss_sum_before\n","print(f'loss_sum_dif: {loss_sum_dif}')\n","param_grad = loss_sum_dif / param_dif\n","print(f'param_grad: {param_grad}')"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[],"source":["# change W1 back to before\n","n.parameters()[0].data = param_before"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["W1: -0.2694500942193594\n","W1_grad: 1.1224127422738681\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","W1 = n.parameters()[0].data\n","W1_grad = n.parameters()[0].grad\n","print(f'W1: {W1}')\n","print(f'W1_grad: {W1_grad}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [-0.9622527581123879, -0.0304811135565296]\n","ys:                 [1.0, -1.0]\n","err_sq:             [3.8504358867196733, 0.9399668711705869]\n","loss_sum:           4.79040275789026\n","loss_mean:          2.39520137894513\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","err_sq_ = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","err_sq = [l.data for l in err_sq_]\n","loss_sum = sum(err_sq)\n","loss_len = len(err_sq)\n","loss_mean = loss_sum / loss_len\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'err_sq:             {err_sq}')\n","print(f'loss_sum:           {loss_sum}')\n","print(f'loss_mean:          {loss_mean}')\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0      0.9042753259     0.0085292121           0.05000         0.9038488653\n","  1     -0.1135466718    -0.0030165497           0.05000        -0.1133958443\n","  2      0.8374569669     0.0028272999           0.05000         0.8373156019\n","  3     -0.2758301602    -3.0199668099           0.05000        -0.1248318197\n","  4     -0.9894600062     1.0066768617           0.05000        -1.0397938493\n","  5     -0.6424141642    -1.0066536707           0.05000        -0.5920814806\n","  6      0.4033841780    -0.6274317535           0.05000         0.4347557657\n","  7     -0.3372036384     0.2500609850           0.05000        -0.3497066876\n","  8      0.0004579422    -0.2054241845           0.05000         0.0107291514\n","  9     -0.3076180978     0.0329361405           0.05000        -0.3092649049\n"," 10      0.8642472400    -0.0139140621           0.05000         0.8649429431\n"," 11     -0.2587943169     0.0317183875           0.05000        -0.2603802362\n"," 12     -0.0568760379     0.0329527389           0.05000        -0.0585236749\n"," 13      0.7144360045    -0.0669137867           0.05000         0.7177816939\n"," 14     -0.1312002138     0.0254050288           0.05000        -0.1324704652\n"," 15      0.3900130631    -0.0700431266           0.05000         0.3935152194\n"," 16      0.3554538040    -0.0668536054           0.05000         0.3587964842\n"," 17     -0.8739256215    -1.3163648049           0.05000        -0.8081073812\n"," 18      0.9778366879     0.5839020493           0.05000         0.9486415854\n"," 19      0.9569418076    -1.2132946819           0.05000         1.0176065417\n"," 20      0.2580118460    -1.3179398468           0.05000         0.3239088383\n"," 21      0.0377025454    -1.5098657167           0.05000         0.1131958312\n"," 22     -0.1891143277     1.8238457949           0.05000        -0.2803066175\n"," 23     -0.6429581259    -0.2603115933           0.05000        -0.6299425462\n"," 24      0.7279062808     2.0084653256           0.05000         0.6274830145\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.25278830638250105), Value(data = -0.36725002221262704)]\n","step: 0, loss: 0.9586978494686522\n","-------\n","ypred: [Value(data = 0.3957968552147713), Value(data = -0.5117836037217744)]\n","step: 1, loss: 0.6034166897632575\n","-------\n","ypred: [Value(data = 0.5003879463917076), Value(data = -0.5788721408917417)]\n","step: 2, loss: 0.42696087782780034\n","-------\n","ypred: [Value(data = 0.5678052659210773), Value(data = -0.6320974917372586)]\n","step: 3, loss: 0.32214454375156726\n","-------\n","ypred: [Value(data = 0.6166179035062025), Value(data = -0.6729521628539409)]\n","step: 4, loss: 0.2539421196938947\n","-------\n","ypred: [Value(data = 0.6538250516629083), Value(data = -0.7050805082756674)]\n","step: 5, loss: 0.2068146014551268\n","-------\n","ypred: [Value(data = 0.6832225516647018), Value(data = -0.7309153550397554)]\n","step: 6, loss: 0.17275449792720343\n","-------\n","ypred: [Value(data = 0.7070905732974779), Value(data = -0.7520989427692868)]\n","step: 7, loss: 0.1472508664273055\n","-------\n","ypred: [Value(data = 0.7268918032134312), Value(data = -0.7697662695957529)]\n","step: 8, loss: 0.12759565776786674\n","-------\n","ypred: [Value(data = 0.7436108362415143), Value(data = -0.7847202605555753)]\n","step: 9, loss: 0.112080769508035\n","-------\n","ypred: [Value(data = 0.7579358609758065), Value(data = -0.7975415545882515)]\n","step: 10, loss: 0.09958446952006605\n","-------\n","ypred: [Value(data = 0.7703628361682462), Value(data = -0.8086586874810516)]\n","step: 11, loss: 0.0893447248891656\n","-------\n","ypred: [Value(data = 0.7812584420187669), Value(data = -0.8183938986416781)]\n","step: 12, loss: 0.08082864523862623\n","-------\n","ypred: [Value(data = 0.7908997500169304), Value(data = -0.8269936548248473)]\n","step: 13, loss: 0.0736541100138463\n","-------\n","ypred: [Value(data = 0.7995001036935576), Value(data = -0.834649403447001)]\n","step: 14, loss: 0.0675410281993268\n","-------\n","ypred: [Value(data = 0.8072265066694976), Value(data = -0.8415119557871413)]\n","step: 15, loss: 0.062280079889262305\n","-------\n","ypred: [Value(data = 0.8142116047202175), Value(data = -0.8477016363928449)]\n","step: 16, loss: 0.05771211937805394\n","-------\n","ypred: [Value(data = 0.8205621277280134), Value(data = -0.8533155661868184)]\n","step: 17, loss: 0.05371427312859144\n","-------\n","ypred: [Value(data = 0.8263649552745134), Value(data = -0.8584329708718902)]\n","step: 18, loss: 0.05019035249298083\n","-------\n","ypred: [Value(data = 0.8316915517709919), Value(data = -0.86311910635756)]\n","step: 19, loss: 0.0470641127896097\n","-------\n","ypred: [Value(data = 0.8366012608419001), Value(data = -0.8674282006151711)]\n","step: 20, loss: 0.044274429950588085\n","-------\n","ypred: [Value(data = 0.841143787450788), Value(data = -0.87140568590769)]\n","step: 21, loss: 0.04177179388235211\n","-------\n","ypred: [Value(data = 0.8453610924000888), Value(data = -0.8750899121424807)]\n","step: 22, loss: 0.03951572179226706\n","-------\n","ypred: [Value(data = 0.8492888555193039), Value(data = -0.8785134760587548)]\n","step: 23, loss: 0.03747282457000799\n","-------\n","ypred: [Value(data = 0.8529576180587035), Value(data = -0.881704262658307)]\n","step: 24, loss: 0.03561534356018493\n","-------\n","ypred: [Value(data = 0.8563936835706932), Value(data = -0.8846862687399978)]\n","step: 25, loss: 0.033920030735498194\n","-------\n","ypred: [Value(data = 0.8596198349191209), Value(data = -0.8874802597549775)]\n","step: 26, loss: 0.03236728269294223\n","-------\n","ypred: [Value(data = 0.8626559098425416), Value(data = -0.8901042979446514)]\n","step: 27, loss: 0.030940464431418013\n","-------\n","ypred: [Value(data = 0.8655192666575862), Value(data = -0.8925741701893002)]\n","step: 28, loss: 0.029625376550830855\n","-------\n","ypred: [Value(data = 0.8682251638689119), Value(data = -0.8949037370621032)]\n","step: 29, loss: 0.028409831920886648\n","-------\n","ypred: [Value(data = 0.8707870717458548), Value(data = -0.8971052194892348)]\n","step: 30, loss: 0.027283316684369417\n","-------\n","ypred: [Value(data = 0.8732169297167045), Value(data = -0.8991894356373261)]\n","step: 31, loss: 0.02623671679757986\n","-------\n","ypred: [Value(data = 0.8755253602953557), Value(data = -0.9011659978197002)]\n","step: 32, loss: 0.025262095916576528\n","-------\n","ypred: [Value(data = 0.877721847896327), Value(data = -0.9030434770754329)]\n","step: 33, loss: 0.024352513819511097\n","-------\n","ypred: [Value(data = 0.879814889104353), Value(data = -0.9048295414476559)]\n","step: 34, loss: 0.023501877062062405\n","-------\n","ypred: [Value(data = 0.8818121195953721), Value(data = -0.906531072738399)]\n","step: 35, loss: 0.0227048154379731\n","-------\n","ypred: [Value(data = 0.8837204218497912), Value(data = -0.908154265551547)]\n","step: 36, loss: 0.021956579231166274\n","-------\n","ypred: [Value(data = 0.8855460169786081), Value(data = -0.9097047116836289)]\n","step: 37, loss: 0.021252953321597646\n","-------\n","ypred: [Value(data = 0.8872945433413012), Value(data = -0.9111874723329083)]\n","step: 38, loss: 0.02059018503126376\n","-------\n","ypred: [Value(data = 0.888971124129088), Value(data = -0.9126071401325201)]\n","step: 39, loss: 0.019964923232975374\n","-------\n","ypred: [Value(data = 0.8905804256867623), Value(data = -0.9139678926446562)]\n","step: 40, loss: 0.01937416673889155\n","-------\n","ypred: [Value(data = 0.8921267080271533), Value(data = -0.9152735386585612)]\n","step: 41, loss: 0.01881522037250135\n","-------\n","ypred: [Value(data = 0.8936138687363819), Value(data = -0.9165275583990162)]\n","step: 42, loss: 0.01828565743206942\n","-------\n","ypred: [Value(data = 0.8950454812618778), Value(data = -0.9177331385615645)]\n","step: 43, loss: 0.017783287494481603\n","-------\n","ypred: [Value(data = 0.8964248284080557), Value(data = -0.9188932029363929)]\n","step: 44, loss: 0.017306128700217853\n","-------\n","ypred: [Value(data = 0.8977549317285443), Value(data = -0.9200104392570846)]\n","step: 45, loss: 0.0168523838136792\n","-------\n","ypred: [Value(data = 0.8990385773926173), Value(data = -0.9210873228076031)]\n","step: 46, loss: 0.01642041947657797\n","-------\n","ypred: [Value(data = 0.9002783390120929), Value(data = -0.9221261372363709)]\n","step: 47, loss: 0.016008748171915613\n","-------\n","ypred: [Value(data = 0.9014765978395447), Value(data = -0.923128992956562)]\n","step: 48, loss: 0.015616012497143098\n","-------\n","ypred: [Value(data = 0.9026355606862081), Value(data = -0.9240978434539551)]\n","step: 49, loss: 0.015240971411229378\n","-------\n","ypred: [Value(data = 0.9037572758559882), Value(data = -0.9250344997756476)]\n","step: 50, loss: 0.01488248817454774\n","-------\n","ypred: [Value(data = 0.9048436473486029), Value(data = -0.9259406434328118)]\n","step: 51, loss: 0.01453951974506297\n","-------\n","ypred: [Value(data = 0.905896447548532), Value(data = -0.9268178379170771)]\n","step: 52, loss: 0.014211107431117385\n","-------\n","ypred: [Value(data = 0.9069173285858863), Value(data = -0.9276675390018722)]\n","step: 53, loss: 0.013896368631633554\n","-------\n","ypred: [Value(data = 0.9079078325295146), Value(data = -0.9284911039762346)]\n","step: 54, loss: 0.01359448951994963\n","-------\n","ypred: [Value(data = 0.9088694005508456), Value(data = -0.9292897999384405)]\n","step: 55, loss: 0.01330471854870798\n","-------\n","ypred: [Value(data = 0.9098033811784273), Value(data = -0.9300648112596974)]\n","step: 56, loss: 0.013026360670985827\n","-------\n","ypred: [Value(data = 0.9107110377473475), Value(data = -0.9308172463135813)]\n","step: 57, loss: 0.012758772187791277\n","-------\n","ypred: [Value(data = 0.9115935551342453), Value(data = -0.9315481435544686)]\n","step: 58, loss: 0.01250135614464136\n","-------\n","ypred: [Value(data = 0.9124520458570751), Value(data = -0.9322584770175726)]\n","step: 59, loss: 0.012253558210590428\n","-------\n","ypred: [Value(data = 0.913287555608882), Value(data = -0.9329491613040626)]\n","step: 60, loss: 0.012014862982111355\n","-------\n","ypred: [Value(data = 0.91410106828631), Value(data = -0.9336210561068825)]\n","step: 61, loss: 0.011784790661918818\n","-------\n","ypred: [Value(data = 0.9148935105662024), Value(data = -0.9342749703261096)]\n","step: 62, loss: 0.011562894069378877\n","-------\n","ypred: [Value(data = 0.9156657560772794), Value(data = -0.93491166581683)]\n","step: 63, loss: 0.011348755944756945\n","-------\n","ypred: [Value(data = 0.9164186292083569), Value(data = -0.9355318608074276)]\n","step: 64, loss: 0.01114198651436302\n","-------\n","ypred: [Value(data = 0.9171529085897628), Value(data = -0.9361362330217678)]\n","step: 65, loss: 0.010942221287786127\n","-------\n","ypred: [Value(data = 0.9178693302804246), Value(data = -0.9367254225349172)]\n","step: 66, loss: 0.010749119061970742\n","-------\n","ypred: [Value(data = 0.9185685906894508), Value(data = -0.9373000343886844)]\n","step: 67, loss: 0.010562360109962355\n","-------\n","ypred: [Value(data = 0.9192513492578309), Value(data = -0.9378606409903367)]\n","step: 68, loss: 0.010381644534812624\n","-------\n","ypred: [Value(data = 0.9199182309230781), Value(data = -0.9384077843152837)]\n","step: 69, loss: 0.010206690771442054\n","-------\n","ypred: [Value(data = 0.9205698283871808), Value(data = -0.9389419779322562)]\n","step: 70, loss: 0.010037234221266995\n","-------\n","ypred: [Value(data = 0.9212067042060608), Value(data = -0.9394637088675362)]\n","step: 71, loss: 0.009873026006145622\n","-------\n","ypred: [Value(data = 0.9218293927168295), Value(data = -0.9399734393230392)]\n","step: 72, loss: 0.009713831829724521\n","-------\n","ypred: [Value(data = 0.9224384018174494), Value(data = -0.9404716082615165)]\n","step: 73, loss: 0.00955943093560178\n","-------\n","ypred: [Value(data = 0.9230342146119124), Value(data = -0.940958632870775)]\n","step: 74, loss: 0.009409615152893097\n","-------\n","ypred: [Value(data = 0.9236172909327306), Value(data = -0.9414349099176094)]\n","step: 75, loss: 0.009264188020813649\n","-------\n","ypred: [Value(data = 0.9241880687513567), Value(data = -0.9419008170010728)]\n","step: 76, loss: 0.009122963984791852\n","-------\n","ypred: [Value(data = 0.924746965486115), Value(data = -0.9423567137137536)]\n","step: 77, loss: 0.008985767657426124\n","-------\n","ypred: [Value(data = 0.9252943792162859), Value(data = -0.9428029427188889)]\n","step: 78, loss: 0.008852433138298808\n","-------\n","ypred: [Value(data = 0.9258306898101694), Value(data = -0.9432398307503828)]\n","step: 79, loss: 0.008722803387280511\n","-------\n","ypred: [Value(data = 0.9263562599742035), Value(data = -0.9436676895421274)]\n","step: 80, loss: 0.008596729646509246\n","-------\n","ypred: [Value(data = 0.9268714362295596), Value(data = -0.9440868166924263)]\n","step: 81, loss: 0.008474070906713704\n","-------\n","ypred: [Value(data = 0.927376549822039), Value(data = -0.9444974964687717)]\n","step: 82, loss: 0.008354693413984798\n","-------\n","ypred: [Value(data = 0.9278719175705642), Value(data = -0.944900000557755)]\n","step: 83, loss: 0.008238470213482882\n","-------\n","ypred: [Value(data = 0.9283578426590859), Value(data = -0.9452945887644486)]\n","step: 84, loss: 0.008125280726911079\n","-------\n","ypred: [Value(data = 0.9288346153762882), Value(data = -0.9456815096652101)]\n","step: 85, loss: 0.008015010360891497\n","-------\n","ypred: [Value(data = 0.9293025138070977), Value(data = -0.9460610012175108)]\n","step: 86, loss: 0.00790755014365298\n","-------\n","ypred: [Value(data = 0.9297618044796457), Value(data = -0.9464332913300724)]\n","step: 87, loss: 0.007802796387684415\n","-------\n","ypred: [Value(data = 0.9302127429710241), Value(data = -0.9467985983963116)]\n","step: 88, loss: 0.007700650376225283\n","-------\n","ypred: [Value(data = 0.9306555744748863), Value(data = -0.9471571317938356)]\n","step: 89, loss: 0.0076010180716620905\n","-------\n","ypred: [Value(data = 0.9310905343336917), Value(data = -0.9475090923524968)]\n","step: 90, loss: 0.007503809844074829\n","-------\n","ypred: [Value(data = 0.9315178485381547), Value(data = -0.9478546727933073)]\n","step: 91, loss: 0.007408940218336175\n","-------\n","ypred: [Value(data = 0.9319377341962513), Value(data = -0.9481940581403199)]\n","step: 92, loss: 0.007316327638308704\n","-------\n","ypred: [Value(data = 0.9323503999739435), Value(data = -0.948527426107412)]\n","step: 93, loss: 0.007225894246813348\n","-------\n","ypred: [Value(data = 0.9327560465096084), Value(data = -0.948854947461748)]\n","step: 94, loss: 0.0071375656801585\n","-------\n","ypred: [Value(data = 0.9331548668039938), Value(data = -0.9491767863655556)]\n","step: 95, loss: 0.007051270876124182\n","-------\n","ypred: [Value(data = 0.9335470465873911), Value(data = -0.949493100697721)]\n","step: 96, loss: 0.006966941894388927\n","-------\n","ypred: [Value(data = 0.9339327646655686), Value(data = -0.949804042356587)]\n","step: 97, loss: 0.006884513748474449\n","-------\n","ypred: [Value(data = 0.9343121932459015), Value(data = -0.9501097575452339)]\n","step: 98, loss: 0.006803924248359135\n","-------\n","ypred: [Value(data = 0.9346854982450163), Value(data = -0.9504103870404181)]\n","step: 99, loss: 0.006725113852982892\n","-------\n","ypred: [Value(data = 0.9350528395791721), Value(data = -0.9507060664462635)]\n","step: 100, loss: 0.006648025531928943\n","-------\n","ypred: [Value(data = 0.9354143714385063), Value(data = -0.9509969264337034)]\n","step: 101, loss: 0.006572604635627116\n","-------\n","ypred: [Value(data = 0.9357702425461928), Value(data = -0.951283092966611)]\n","step: 102, loss: 0.006498798773474774\n","-------\n","ypred: [Value(data = 0.9361205964034742), Value(data = -0.951564687515477)]\n","step: 103, loss: 0.006426557699321224\n","-------\n","ypred: [Value(data = 0.936465571521473), Value(data = -0.9518418272594309)]\n","step: 104, loss: 0.006355833203803552\n","-------\n","ypred: [Value(data = 0.9368053016406067), Value(data = -0.952114625277349)]\n","step: 105, loss: 0.006286579013063405\n","-------\n","ypred: [Value(data = 0.9371399159383851), Value(data = -0.9523831907287336)]\n","step: 106, loss: 0.006218750693409459\n","-------\n","ypred: [Value(data = 0.9374695392263029), Value(data = -0.9526476290250006)]\n","step: 107, loss: 0.006152305561524837\n","-------\n","ypred: [Value(data = 0.9377942921364987), Value(data = -0.9529080419917688)]\n","step: 108, loss: 0.00608720259984828\n","-------\n","ypred: [Value(data = 0.9381142912987966), Value(data = -0.9531645280227014)]\n","step: 109, loss: 0.0060234023767865245\n","-------\n","ypred: [Value(data = 0.9384296495087141), Value(data = -0.9534171822254096)]\n","step: 110, loss: 0.005960866971440484\n","-------\n","ypred: [Value(data = 0.9387404758869672), Value(data = -0.953666096559899)]\n","step: 111, loss: 0.005899559902551854\n","-------\n","ypred: [Value(data = 0.9390468760309807), Value(data = -0.9539113599699998)]\n","step: 112, loss: 0.005839446061397576\n","-------\n","ypred: [Value(data = 0.9393489521588679), Value(data = -0.9541530585081954)]\n","step: 113, loss: 0.005780491648380242\n","-------\n","ypred: [Value(data = 0.9396468032463186), Value(data = -0.9543912754542375)]\n","step: 114, loss: 0.005722664113079822\n","-------\n","ypred: [Value(data = 0.9399405251567994), Value(data = -0.9546260914279071)]\n","step: 115, loss: 0.0056659320975496925\n","-------\n","ypred: [Value(data = 0.9402302107654539), Value(data = -0.9548575844962557)]\n","step: 116, loss: 0.00561026538265476\n","-------\n","ypred: [Value(data = 0.9405159500770514), Value(data = -0.9550858302756468)]\n","step: 117, loss: 0.005555634837263857\n","-------\n","ypred: [Value(data = 0.9407978303383233), Value(data = -0.9553109020288871)]\n","step: 118, loss: 0.005502012370121675\n","-------\n","ypred: [Value(data = 0.9410759361449971), Value(data = -0.955532870757724)]\n","step: 119, loss: 0.005449370884237742\n","-------\n","ypred: [Value(data = 0.9413503495438191), Value(data = -0.9557518052909675)]\n","step: 120, loss: 0.00539768423364065\n","-------\n","ypred: [Value(data = 0.9416211501298416), Value(data = -0.9559677723684789)]\n","step: 121, loss: 0.005346927182356588\n","-------\n","ypred: [Value(data = 0.9418884151392306), Value(data = -0.9561808367212501)]\n","step: 122, loss: 0.005297075365480146\n","-------\n","ypred: [Value(data = 0.9421522195378339), Value(data = -0.9563910611477862)]\n","step: 123, loss: 0.005248105252215092\n","-------\n","ypred: [Value(data = 0.9424126361057381), Value(data = -0.9565985065869924)]\n","step: 124, loss: 0.005199994110769479\n","-------\n","ypred: [Value(data = 0.9426697355180241), Value(data = -0.9568032321877499)]\n","step: 125, loss: 0.005152719974998752\n","-------\n","ypred: [Value(data = 0.9429235864219225), Value(data = -0.9570052953753563)]\n","step: 126, loss: 0.005106261612696106\n","-------\n","ypred: [Value(data = 0.9431742555105556), Value(data = -0.9572047519149949)]\n","step: 127, loss: 0.00506059849543676\n","-------\n","ypred: [Value(data = 0.9434218075934429), Value(data = -0.9574016559723911)]\n","step: 128, loss: 0.005015710769887921\n","-------\n","ypred: [Value(data = 0.9436663056639362), Value(data = -0.9575960601717973)]\n","step: 129, loss: 0.0049715792305029035\n","-------\n","ypred: [Value(data = 0.9439078109637388), Value(data = -0.9577880156514439)]\n","step: 130, loss: 0.00492818529352241\n","-------\n","ypred: [Value(data = 0.9441463830446594), Value(data = -0.957977572116591)]\n","step: 131, loss: 0.004885510972210228\n","-------\n","ypred: [Value(data = 0.944382079827736), Value(data = -0.9581647778902929)]\n","step: 132, loss: 0.004843538853256856\n","-------\n","ypred: [Value(data = 0.9446149576598628), Value(data = -0.9583496799619999)]\n","step: 133, loss: 0.004802252074286619\n","-------\n","ypred: [Value(data = 0.9448450713680419), Value(data = -0.9585323240340975)]\n","step: 134, loss: 0.0047616343024094844\n","-------\n","ypred: [Value(data = 0.945072474311378), Value(data = -0.9587127545664892)]\n","step: 135, loss: 0.004721669713761192\n","-------\n","ypred: [Value(data = 0.9452972184309238), Value(data = -0.9588910148193183)]\n","step: 136, loss: 0.004682342973979568\n","-------\n","ypred: [Value(data = 0.9455193542974825), Value(data = -0.959067146893918)]\n","step: 137, loss: 0.004643639219567328\n","-------\n","ypred: [Value(data = 0.9457389311574611), Value(data = -0.9592411917720777)]\n","step: 138, loss: 0.004605544040095295\n","-------\n","ypred: [Value(data = 0.945955996976874), Value(data = -0.9594131893537033)]\n","step: 139, loss: 0.004568043461201987\n","-------\n","ypred: [Value(data = 0.9461705984835769), Value(data = -0.9595831784929528)]\n","step: 140, loss: 0.004531123928348807\n","-------\n","ypred: [Value(data = 0.9463827812078189), Value(data = -0.9597511970329136)]\n","step: 141, loss: 0.00449477229129196\n","-------\n","ypred: [Value(data = 0.946592589521191), Value(data = -0.9599172818388952)]\n","step: 142, loss: 0.004458975789234558\n","-------\n","ypred: [Value(data = 0.9468000666740419), Value(data = -0.9600814688303994)]\n","step: 143, loss: 0.004423722036624763\n","-------\n","ypred: [Value(data = 0.9470052548314354), Value(data = -0.9602437930118283)]\n","step: 144, loss: 0.004388999009567457\n","-------\n","ypred: [Value(data = 0.9472081951077134), Value(data = -0.9604042885019929)]\n","step: 145, loss: 0.004354795032818671\n","-------\n","ypred: [Value(data = 0.9474089275997291), Value(data = -0.96056298856247)]\n","step: 146, loss: 0.00432109876733441\n","-------\n","ypred: [Value(data = 0.9476074914188092), Value(data = -0.9607199256248677)]\n","step: 147, loss: 0.004287899198346076\n","-------\n","ypred: [Value(data = 0.9478039247215037), Value(data = -0.9608751313170413)]\n","step: 148, loss: 0.004255185623937221\n","-------\n","ypred: [Value(data = 0.9479982647391743), Value(data = -0.9610286364883126)]\n","step: 149, loss: 0.004222947644097088\n","-------\n","ypred: [Value(data = 0.9481905478064754), Value(data = -0.9611804712337332)]\n","step: 150, loss: 0.00419117515022813\n","-------\n","ypred: [Value(data = 0.9483808093887757), Value(data = -0.9613306649174357)]\n","step: 151, loss: 0.0041598583150855495\n","-------\n","ypred: [Value(data = 0.9485690841085641), Value(data = -0.9614792461951129)]\n","step: 152, loss: 0.0041289875831286744\n","-------\n","ypred: [Value(data = 0.9487554057708881), Value(data = -0.9616262430356622)]\n","step: 153, loss: 0.004098553661264387\n","-------\n","ypred: [Value(data = 0.948939807387862), Value(data = -0.9617716827420322)]\n","step: 154, loss: 0.004068547509964474\n","-------\n","ypred: [Value(data = 0.9491223212022875), Value(data = -0.9619155919713054)]\n","step: 155, loss: 0.004038960334739307\n","-------\n","ypred: [Value(data = 0.9493029787104235), Value(data = -0.9620579967540503)]\n","step: 156, loss: 0.004009783577951431\n","-------\n","ypred: [Value(data = 0.9494818106839404), Value(data = -0.9621989225129736)]\n","step: 157, loss: 0.003981008910953411\n","-------\n","ypred: [Value(data = 0.9496588471910942), Value(data = -0.9623383940809009)]\n","step: 158, loss: 0.0039526282265351246\n","-------\n","ypred: [Value(data = 0.9498341176171514), Value(data = -0.9624764357181177)]\n","step: 159, loss: 0.003924633631666351\n","-------\n","ypred: [Value(data = 0.9500076506840976), Value(data = -0.9626130711290923)]\n","step: 160, loss: 0.003897017440521511\n","-------\n","ypred: [Value(data = 0.9501794744696569), Value(data = -0.9627483234786113)]\n","step: 161, loss: 0.0038697721677737533\n","-------\n","ypred: [Value(data = 0.9503496164256532), Value(data = -0.9628822154073473)]\n","step: 162, loss: 0.003842890522146333\n","-------\n","ypred: [Value(data = 0.9505181033957366), Value(data = -0.9630147690468867)]\n","step: 163, loss: 0.003816365400210146\n","-------\n","ypred: [Value(data = 0.9506849616325045), Value(data = -0.963146006034235)]\n","step: 164, loss: 0.003790189880416201\n","-------\n","ypred: [Value(data = 0.9508502168140377), Value(data = -0.9632759475258249)]\n","step: 165, loss: 0.003764357217353069\n","-------\n","ypred: [Value(data = 0.9510138940598777), Value(data = -0.9634046142110438)]\n","step: 166, loss: 0.003738860836219421\n","-------\n","ypred: [Value(data = 0.9511760179464666), Value(data = -0.9635320263253027)]\n","step: 167, loss: 0.003713694327502174\n","-------\n","ypred: [Value(data = 0.9513366125220698), Value(data = -0.9636582036626615)]\n","step: 168, loss: 0.0036888514418517617\n","-------\n","ypred: [Value(data = 0.9514957013212032), Value(data = -0.9637831655880331)]\n","step: 169, loss: 0.0036643260851457575\n","-------\n","ypred: [Value(data = 0.9516533073785832), Value(data = -0.9639069310489764)]\n","step: 170, loss: 0.003640112313733099\n","-------\n","ypred: [Value(data = 0.9518094532426182), Value(data = -0.9640295185871014)]\n","step: 171, loss: 0.0036162043298510827\n","-------\n","ypred: [Value(data = 0.9519641609884599), Value(data = -0.964150946349095)]\n","step: 172, loss: 0.0035925964772080573\n","-------\n","ypred: [Value(data = 0.9521174522306297), Value(data = -0.964271232097386)]\n","step: 173, loss: 0.00356928323672489\n","-------\n","ypred: [Value(data = 0.9522693481352382), Value(data = -0.9643903932204625)]\n","step: 174, loss: 0.0035462592224283704\n","-------\n","ypred: [Value(data = 0.9524198694318126), Value(data = -0.964508446742855)]\n","step: 175, loss: 0.003523519177490527\n","-------\n","ypred: [Value(data = 0.9525690364247448), Value(data = -0.9646254093347952)]\n","step: 176, loss: 0.00350105797040798\n","-------\n","ypred: [Value(data = 0.9527168690043785), Value(data = -0.9647412973215685)]\n","step: 177, loss: 0.0034788705913151355\n","-------\n","ypred: [Value(data = 0.9528633866577445), Value(data = -0.9648561266925646)]\n","step: 178, loss: 0.0034569521484263694\n","-------\n","ypred: [Value(data = 0.9530086084789595), Value(data = -0.9649699131100437)]\n","step: 179, loss: 0.003435297864601607\n","-------\n","ypred: [Value(data = 0.9531525531793008), Value(data = -0.9650826719176258)]\n","step: 180, loss: 0.0034139030740303992\n","-------\n","ypred: [Value(data = 0.9532952390969673), Value(data = -0.9651944181485128)]\n","step: 181, loss: 0.0033927632190300224\n","-------\n","ypred: [Value(data = 0.9534366842065403), Value(data = -0.9653051665334548)]\n","step: 182, loss: 0.0033718738469527636\n","-------\n","ypred: [Value(data = 0.9535769061281527), Value(data = -0.9654149315084688)]\n","step: 183, loss: 0.0033512306071982493\n","-------\n","ypred: [Value(data = 0.9537159221363796), Value(data = -0.965523727222321)]\n","step: 184, loss: 0.0033308292483266054\n","-------\n","ypred: [Value(data = 0.9538537491688568), Value(data = -0.9656315675437771)]\n","step: 185, loss: 0.0033106656152687404\n","-------\n","ypred: [Value(data = 0.9539904038346413), Value(data = -0.9657384660686328)]\n","step: 186, loss: 0.0032907356466296183\n","-------\n","ypred: [Value(data = 0.9541259024223192), Value(data = -0.965844436126531)]\n","step: 187, loss: 0.003271035372081197\n","-------\n","ypred: [Value(data = 0.9542602609078713), Value(data = -0.9659494907875723)]\n","step: 188, loss: 0.0032515609098416292\n","-------\n","ypred: [Value(data = 0.9543934949623055), Value(data = -0.9660536428687277)]\n","step: 189, loss: 0.0032323084642371307\n","-------\n","ypred: [Value(data = 0.9545256199590635), Value(data = -0.9661569049400601)]\n","step: 190, loss: 0.0032132743233436517\n","-------\n","ypred: [Value(data = 0.9546566509812084), Value(data = -0.9662592893307594)]\n","step: 191, loss: 0.0031944548567053492\n","-------\n","ypred: [Value(data = 0.9547866028284042), Value(data = -0.966360808135001)]\n","step: 192, loss: 0.0031758465131266787\n","-------\n","ypred: [Value(data = 0.95491549002369), Value(data = -0.9664614732176312)]\n","step: 193, loss: 0.003157445818535662\n","-------\n","ypred: [Value(data = 0.9550433268200591), Value(data = -0.9665612962196856)]\n","step: 194, loss: 0.003139249373915632\n","-------\n","ypred: [Value(data = 0.9551701272068485), Value(data = -0.9666602885637485)]\n","step: 195, loss: 0.0031212538533026656\n","-------\n","ypred: [Value(data = 0.9552959049159451), Value(data = -0.9667584614591556)]\n","step: 196, loss: 0.0031034560018466705\n","-------\n","ypred: [Value(data = 0.9554206734278148), Value(data = -0.966855825907048)]\n","step: 197, loss: 0.00308585263393345\n","-------\n","ypred: [Value(data = 0.955544445977361), Value(data = -0.9669523927052807)]\n","step: 198, loss: 0.003068440631365762\n","-------\n","ypred: [Value(data = 0.955667235559617), Value(data = -0.9670481724531905)]\n","step: 199, loss: 0.0030512169416011623\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 1.3195509910583496\n","Epoch 10 loss: 0.4273889362812042\n","Epoch 20 loss: 0.027823850512504578\n","Epoch 30 loss: 0.0009819257538765669\n","\n","Prediction:\n","tensor([[ 0.9917],\n","        [-0.9984]])\n","Loss: 4.9306665459880605e-05\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[ 0.3463, -0.3841,  0.0556],\n","        [-0.1493,  0.0847, -0.4362],\n","        [-0.1314,  0.2131, -0.0163],\n","        [ 0.0778,  0.4130, -0.6206]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.5153,  1.4506],\n","        [ 0.3917, -0.7508],\n","        [ 0.3927, -0.6154],\n","        [ 2.0153, -0.4898]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.4412],\n","        [-0.1923],\n","        [ 0.0503],\n","        [-0.5673]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.9565,  1.0094],\n","        [ 0.1993, -0.9431],\n","        [ 0.4430, -0.5651],\n","        [ 1.4479, -1.0571]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.7427,  0.7655],\n","        [ 0.1967, -0.7367],\n","        [ 0.4162, -0.5117],\n","        [ 0.8953, -0.7846]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[-0.2441,  0.2168,  0.2726,  0.5512],\n","        [ 0.1061, -0.2513, -0.3463, -0.6542],\n","        [-0.1749, -0.4597, -0.3441,  0.4000],\n","        [ 0.1722,  0.0302,  0.2387, -0.2786]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.7427,  0.7655],\n","        [ 0.1967, -0.7367],\n","        [ 0.4162, -0.5117],\n","        [ 0.8953, -0.7846]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 0.8308, -0.9184],\n","        [-0.8580,  0.9568],\n","        [ 0.2544,  0.0670],\n","        [-0.2721,  0.2061]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[ 0.2819],\n","        [-0.1690],\n","        [-0.0639],\n","        [ 0.1841]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 1.1127, -0.6365],\n","        [-1.0270,  0.7878],\n","        [ 0.1905,  0.0030],\n","        [-0.0880,  0.3902]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.8050, -0.5625],\n","        [-0.7727,  0.6571],\n","        [ 0.1882,  0.0030],\n","        [-0.0877,  0.3715]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[ 0.6981, -0.6738,  0.1175, -0.1092]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.8050, -0.5625],\n","        [-0.7727,  0.6571],\n","        [ 0.1882,  0.0030],\n","        [-0.0877,  0.3715]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 1.1144, -0.8757]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[-0.1227]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9917, -0.9984]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9917, -0.9984]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9917, -0.9984]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(3.5483e-05)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","\n","# loss = torch.sum(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9917, -0.9984]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-0.0083,  0.0016]])\n","squared_difference: tensor([[6.8448e-05, 2.5179e-06]])\n"]},{"data":{"text/plain":["tensor(3.5483e-05)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["[0.9917266964912415, -0.9984132051467896]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# for item in output.item:\n","#   print(item)\n","# type(output)\n","output.tolist()[0]\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3.548273442710581e-05\n"]}],"source":["import numpy as np\n","\n","def mse_loss(y_true, y_pred):\n","  \"\"\"Calculates the mean squared error loss.\n","\n","  Args:\n","    y_true: The ground truth labels.\n","    y_pred: The predicted labels.\n","\n","  Returns:\n","    The mean squared error loss.\n","  \"\"\"\n","\n","  loss = np.mean((y_true - y_pred)**2)\n","  return loss\n","\n","def main():\n","  \"\"\"Main function.\"\"\"\n","\n","  # y_true = np.array([1, 2, 3, 4, 5])\n","  y_true = np.array([1.0, -1.0])\n","\n","  # y_pred = np.array([0, 1, 2, 3, 4])\n","  # y_pred = np.array([0.9997345209121704, -0.9980572462081909])\n","  y_pred = np.array(output.tolist()[0])  \n","\n","  loss = mse_loss(y_true, y_pred)\n","  print(loss)\n","\n","if __name__ == \"__main__\":\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["len(squared_difference)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(3.5483e-05)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(7.0965e-05)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- torch results matched backward pass results ----\n","x0.data.item()  = -3.000000\n","x0.grad.item()  =  1.000000\n","w0.data.item()  =  2.000000\n","w0.grad.item()  = -1.500000 <-- result matched micrograd\n","---\n","x1.data.item()  =  0.000000\n","x1.grad.item()  =  0.500000\n","w1.data.item()  =  1.000000\n","w1.grad.item()  =  0.000000\n","---\n","x2.data.item()  =  0.500000\n","x2.grad.item()  =  0.500000\n","w2.data.item()  =  1.000000\n","w2.grad.item()  =  0.250000\n","---\n","out.data.item() = -0.707107 <-- result matched micrograd\n"]}],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <-- result matched micrograd')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <-- result matched micrograd')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["parameters in MLP: 41\n","\n","i:  0,   0.6238185157\n","i:  1,  -0.4846932912\n","i:  2,   0.9071775278\n","i:  3,  -0.8117812646\n","i:  4,  -0.7937747591\n","---\n","i: 36,  -0.5866888834\n","i: 37,  -0.7141357036\n","i: 38,   0.2106535828\n","i: 39,  -0.6678599824\n","i: 40,   0.1180751661\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'parameters in MLP: {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 3)\n","xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","layer: 0, neuron 0\n","w0:  0.6238185,   w0.grad:  0.0000000\n","w1: -0.4846933,   w1.grad:  0.0000000\n","w2:  0.9071775,   w2.grad:  0.0000000\n","b:  -0.8117813\n","\n","layer: 0, neuron 1\n","w0: -0.7937748,   w0.grad:  0.0000000\n","w1:  0.5893272,   w1.grad:  0.0000000\n","w2:  0.4087466,   w2.grad:  0.0000000\n","b:   0.8013964\n","\n","layer: 0, neuron 2\n","w0: -0.5152178,   w0.grad:  0.0000000\n","w1: -0.9908527,   w1.grad:  0.0000000\n","w2: -0.4739887,   w2.grad:  0.0000000\n","b:  -0.1538711\n","\n","layer: 0, neuron 3\n","w0:  0.2336745,   w0.grad:  0.0000000\n","w1: -0.7774972,   w1.grad:  0.0000000\n","w2: -0.6889257,   w2.grad:  0.0000000\n","b:   0.9689319\n","\n","------\n","layer: 1, neuron_cnt: 4\n","----\n","layer: 1, neuron 0\n","w0:  0.0760687,   w0.grad:  0.0000000\n","w1:  0.2950475,   w1.grad:  0.0000000\n","w2:  0.3495016,   w2.grad:  0.0000000\n","w3:  0.4515870,   w3.grad:  0.0000000\n","b:  -0.7572596\n","\n","layer: 1, neuron 1\n","w0:  0.0132854,   w0.grad:  0.0000000\n","w1: -0.1261447,   w1.grad:  0.0000000\n","w2:  0.6458190,   w2.grad:  0.0000000\n","w3: -0.7132814,   w3.grad:  0.0000000\n","b:  -0.5656837\n","\n","layer: 1, neuron 2\n","w0: -0.2063025,   w0.grad:  0.0000000\n","w1:  0.9401353,   w1.grad:  0.0000000\n","w2:  0.2242878,   w2.grad:  0.0000000\n","w3: -0.1323255,   w3.grad:  0.0000000\n","b:  -0.6442987\n","\n","layer: 1, neuron 3\n","w0:  0.5981871,   w0.grad:  0.0000000\n","w1: -0.5089891,   w1.grad:  0.0000000\n","w2: -0.3406943,   w2.grad:  0.0000000\n","w3:  0.8465547,   w3.grad:  0.0000000\n","b:   0.5150957\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0: -0.5866889,   w0.grad:  0.0000000\n","w1: -0.7141357,   w1.grad:  0.0000000\n","w2:  0.2106536,   w2.grad:  0.0000000\n","w3: -0.6678600,   w3.grad:  0.0000000\n","b:   0.1180752\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(4, 3):\n","[[ 0.62381852 -0.48469329  0.90717753]\n"," [-0.79377476  0.58932717  0.40874663]\n"," [-0.51521781 -0.99085274 -0.47398872]\n"," [ 0.2336745  -0.7774972  -0.68892565]]\n","b_mat(1, 4):\n","[[-0.81178126  0.8013964  -0.1538711   0.9689319 ]]\n","\n","i: 1\n","w_mat(4, 4):\n","[[ 0.07606871  0.29504752  0.34950159  0.45158704]\n"," [ 0.01328539 -0.12614475  0.64581901 -0.7132814 ]\n"," [-0.20630252  0.9401353   0.22428775 -0.13232552]\n"," [ 0.59818706 -0.50898908 -0.34069425  0.84655468]]\n","b_mat(1, 4):\n","[[-0.75725956 -0.56568375 -0.64429868  0.51509571]]\n","\n","i: 2\n","w_mat(1, 4):\n","[[-0.58668888 -0.7141357   0.21065358 -0.66785998]]\n","b_mat(1, 1):\n","[[0.11807517]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[ 0.62381852 -0.48469329  0.90717753]\n"," [-0.79377476  0.58932717  0.40874663]\n"," [-0.51521781 -0.99085274 -0.47398872]\n"," [ 0.2336745  -0.7774972  -0.68892565]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","weights_x_inputs (4, 2):\n","[[-1.11362037  2.8097376 ]\n"," [-0.22831464 -2.76627813]\n"," [-3.52900513 -0.79179505]\n"," [-1.17621695  1.13405789]]\n","\n","bias (4, 1):\n","[[-0.81178126]\n"," [ 0.8013964 ]\n"," [-0.1538711 ]\n"," [ 0.9689319 ]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-1.92540163  1.99795634]\n"," [ 0.57308176 -1.96488173]\n"," [-3.68287624 -0.94566615]\n"," [-0.20728505  2.10298979]]\n","\n","output (4, 2):\n","[[-0.95836008  0.96388291]\n"," [ 0.51761894 -0.9614606 ]\n"," [-0.9987357  -0.73781473]\n"," [-0.20436639  0.97062551]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[ 0.07606871  0.29504752  0.34950159  0.45158704]\n"," [ 0.01328539 -0.12614475  0.64581901 -0.7132814 ]\n"," [-0.20630252  0.9401353   0.22428775 -0.13232552]\n"," [ 0.59818706 -0.50898908 -0.34069425  0.84655468]]\n","\n","input (4, 2):\n","[[-0.95836008  0.96388291]\n"," [ 0.51761894 -0.9614606 ]\n"," [-0.9987357  -0.73781473]\n"," [-0.20436639  0.97062551]]\n","\n","weights_x_inputs (4, 2):\n","[[-0.36152796 -0.02990075]\n"," [-0.57725885 -1.03473513]\n"," [ 0.48738264 -1.39667585]\n"," [-0.6694848   2.13901204]]\n","\n","bias (4, 1):\n","[[-0.75725956]\n"," [-0.56568375]\n"," [-0.64429868]\n"," [ 0.51509571]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-1.11878752 -0.78716031]\n"," [-1.1429426  -1.60041888]\n"," [-0.15691604 -2.04097453]\n"," [-0.15438909  2.65410775]]\n","\n","output (4, 2):\n","[[-0.80714676 -0.65679735]\n"," [-0.81540258 -0.92173158]\n"," [-0.15564071 -0.96681096]\n"," [-0.153174    0.99014727]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[-0.58668888 -0.7141357   0.21065358 -0.66785998]]\n","\n","input (4, 2):\n","[[-0.80714676 -0.65679735]\n"," [-0.81540258 -0.92173158]\n"," [-0.15564071 -0.96681096]\n"," [-0.153174    0.99014727]]\n","\n","weights_x_inputs (1, 2):\n","[[1.12536464 0.1786352 ]]\n","\n","bias (1, 1):\n","[[0.11807517]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[1.24343981 0.29671037]]\n","\n","output (1, 2):\n","[[0.8464338  0.28829927]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [0.8464338  0.28829927]\n","desired output:     [1.0, -1.0]\n","loss:               1.6832975984082486\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [0.8464337963960112, 0.2882992740504464]\n","ys:                 [1.0, -1.0]\n","loss_data:          1.683297598408249\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0      0.6238185157    -0.0124288075           0.05000         0.6244399560\n","  1     -0.4846932912     0.0158936995           0.05000        -0.4854879762\n","  2      0.9071775278    -0.0063444729           0.05000         0.9074947514\n","  3     -0.8117812646    -0.0030746846           0.05000        -0.8116275304\n","  4     -0.7937747591    -0.0981017864           0.05000        -0.7888696697\n","  5      0.5893271688    -0.0830280560           0.05000         0.5934785716\n","  6      0.4087466311     0.0257328483           0.05000         0.4074599887\n","  7      0.8013963993    -0.0432213820           0.05000         0.8035574684\n","  8     -0.5152178102    -0.5761658345           0.05000        -0.4864095185\n","  9     -0.9908527433     0.1920211614           0.05000        -1.0004538014\n"," 10     -0.4739887170    -0.0960152330           0.05000        -0.4691879554\n"," 11     -0.1538711023    -0.1920583797           0.05000        -0.1442681833\n"," 12      0.2336745039     0.0479406091           0.05000         0.2312774734\n"," 13     -0.7774972043     0.1373015606           0.05000        -0.7843622823\n"," 14     -0.6889256532    -0.0477487216           0.05000        -0.6865382171\n"," 15      0.9689319009     0.0299149088           0.05000         0.9674361555\n"," 16      0.0760687108    -0.7767144323           0.05000         0.1149044324\n"," 17      0.2950475212     0.7669572326           0.05000         0.2566996596\n"," 18      0.3495015897     0.5636976266           0.05000         0.3213167083\n"," 19      0.4515870428    -0.7686022323           0.05000         0.4900171545\n"," 20     -0.7572595599    -0.7703068876           0.05000        -0.7187442155\n"," 21      0.0132853893    -0.2645675907           0.05000         0.0265137689\n"," 22     -0.1261447451     0.2547670874           0.05000        -0.1388830994\n"," 23      0.6458190055     0.1664115042           0.05000         0.6374984302\n"," 24     -0.7132813979    -0.2505640298           0.05000        -0.7007531964\n"," 25     -0.5656837482    -0.2329169501           0.05000        -0.5540379007\n"," 26     -0.2063025166     0.0484675124           0.05000        -0.2087258923\n"," 27      0.9401353047    -0.0404991755           0.05000         0.9421602635\n"," 28      0.2242877537    -0.0060899106           0.05000         0.2245922492\n"," 29     -0.1323255191     0.0351894091           0.05000        -0.1340849895\n"," 30     -0.6442986804     0.0145844494           0.05000        -0.6450279029\n"," 31      0.5981870641    -0.0842528131           0.05000         0.6023997047\n"," 32     -0.5089890830     0.0591448626           0.05000        -0.5119463261\n"," 33     -0.3406942542    -0.0338994313           0.05000        -0.3389992827\n"," 34      0.8465546797    -0.0416364317           0.05000         0.8486365013\n"," 35      0.5150957131     0.0258598300           0.05000         0.5138027217\n"," 36     -0.5866888834    -1.4813525729           0.05000        -0.5126212548\n"," 37     -0.7141357036    -2.1065250255           0.05000        -0.6088094523\n"," 38      0.2106535828    -2.2704792963           0.05000         0.3241775477\n"," 39     -0.6678599824     2.3525037973           0.05000        -0.7854851722\n"," 40      0.1180751661     2.2753534272           0.05000         0.0043074947\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.770792030386018), Value(data = -0.25365871269513124)]\n","step: 0, loss: 0.6095616104704527\n","-------\n","ypred: [Value(data = 0.7652311658640878), Value(data = -0.5159740468753516)]\n","step: 1, loss: 0.28939752877975977\n","-------\n","ypred: [Value(data = 0.7943944415338288), Value(data = -0.6173521186326665)]\n","step: 2, loss: 0.18869304678709511\n","-------\n","ypred: [Value(data = 0.8159373227067621), Value(data = -0.6753645491421034)]\n","step: 3, loss: 0.13926724512606442\n","-------\n","ypred: [Value(data = 0.8316025210690254), Value(data = -0.7140265208609187)]\n","step: 4, loss: 0.1101385416812186\n","-------\n","ypred: [Value(data = 0.8435885893198782), Value(data = -0.7420715940119759)]\n","step: 5, loss: 0.09099159200646871\n","-------\n","ypred: [Value(data = 0.8531528511880074), Value(data = -0.7635700387981491)]\n","step: 6, loss: 0.0774632116681202\n","-------\n","ypred: [Value(data = 0.8610302001439198), Value(data = -0.7807047494008471)]\n","step: 7, loss: 0.06740301220738425\n","-------\n","ypred: [Value(data = 0.8676768265204678), Value(data = -0.794763089048124)]\n","step: 8, loss: 0.05963161185676265\n","-------\n","ypred: [Value(data = 0.8733918926162709), Value(data = -0.8065587967170401)]\n","step: 9, loss: 0.05344911198284928\n","-------\n","ypred: [Value(data = 0.8783807398751011), Value(data = -0.8166342518029264)]\n","step: 10, loss: 0.04841424204520043\n","-------\n","ypred: [Value(data = 0.882789698892697), Value(data = -0.8253664510421753)]\n","step: 11, loss: 0.044235131107269586\n","-------\n","ypred: [Value(data = 0.8867263377490422), Value(data = -0.8330265079444339)]\n","step: 12, loss: 0.04071106960897426\n","-------\n","ypred: [Value(data = 0.8902718142024262), Value(data = -0.8398149627464832)]\n","step: 13, loss: 0.03769952091833746\n","-------\n","ypred: [Value(data = 0.8934887249121886), Value(data = -0.845883713946818)]\n","step: 14, loss: 0.035096481347657645\n","-------\n","ypred: [Value(data = 0.8964262711102986), Value(data = -0.8513501699011413)]\n","step: 15, loss: 0.032824289304536916\n","-------\n","ypred: [Value(data = 0.8991237626134857), Value(data = -0.8563066811963208)]\n","step: 16, loss: 0.030823785138076168\n","-------\n","ypred: [Value(data = 0.9016130580519784), Value(data = -0.8608270054716354)]\n","step: 17, loss: 0.029049112751875574\n","-------\n","ypred: [Value(data = 0.9039203038960179), Value(data = -0.8649708472380229)]\n","step: 18, loss: 0.02746418009905092\n","-------\n","ypred: [Value(data = 0.9060671992361123), Value(data = -0.8687871146230901)]\n","step: 19, loss: 0.02604019234828231\n","-------\n","ypred: [Value(data = 0.9080719324171704), Value(data = -0.8723163002159058)]\n","step: 20, loss: 0.02475389680006798\n","-------\n","ypred: [Value(data = 0.9099498859535142), Value(data = -0.8755922511474366)]\n","step: 21, loss: 0.023586311014347598\n","-------\n","ypred: [Value(data = 0.9117141748050619), Value(data = -0.8786435051302569)]\n","step: 22, loss: 0.02252178577742115\n","-------\n","ypred: [Value(data = 0.913376062830354), Value(data = -0.8814943127396505)]\n","step: 23, loss: 0.021547304403818537\n","-------\n","ypred: [Value(data = 0.914945288843379), Value(data = -0.8841654293597309)]\n","step: 24, loss: 0.020651951645351732\n","-------\n","ypred: [Value(data = 0.9164303246790225), Value(data = -0.886674735650257)]\n","step: 25, loss: 0.01982650617319272\n","-------\n","ypred: [Value(data = 0.917838581482086), Value(data = -0.8890377287070076)]\n","step: 26, loss: 0.019063124343375475\n","-------\n","ypred: [Value(data = 0.9191765761178268), Value(data = -0.8912679145666985)]\n","step: 27, loss: 0.018355092250712222\n","-------\n","ypred: [Value(data = 0.9204500665454814), Value(data = -0.8933771246255765)]\n","step: 28, loss: 0.017696629465728173\n","-------\n","ypred: [Value(data = 0.9216641628020279), Value(data = -0.8953757727906028)]\n","step: 29, loss: 0.017082732308670766\n","-------\n","ypred: [Value(data = 0.9228234186479558), Value(data = -0.8972730660367686)]\n","step: 30, loss: 0.0165090476706748\n","-------\n","ypred: [Value(data = 0.9239319077528726), Value(data = -0.8990771780191255)]\n","step: 31, loss: 0.01597177065470078\n","-------\n","ypred: [Value(data = 0.9249932874254341), Value(data = -0.9007953931556822)]\n","step: 32, loss: 0.015467560950379204\n","-------\n","ypred: [Value(data = 0.9260108522357018), Value(data = -0.9024342269324389)]\n","step: 33, loss: 0.014993474061157985\n","-------\n","ypred: [Value(data = 0.9269875793799554), Value(data = -0.9039995269279516)]\n","step: 34, loss: 0.01454690439485541\n","-------\n","ypred: [Value(data = 0.9279261672566618), Value(data = -0.9054965581018356)]\n","step: 35, loss: 0.014125537896914429\n","-------\n","ypred: [Value(data = 0.9288290684278938), Value(data = -0.906930075161055)]\n","step: 36, loss: 0.013727312410368302\n","-------\n","ypred: [Value(data = 0.9296985179113963), Value(data = -0.9083043842532857)]\n","step: 37, loss: 0.01335038433102333\n","-------\n","ypred: [Value(data = 0.9305365575689089), Value(data = -0.9096233957970183)]\n","step: 38, loss: 0.012993100421639916\n","-------\n","ypred: [Value(data = 0.9313450572145733), Value(data = -0.91089066991331)]\n","step: 39, loss: 0.012653973877368888\n","-------\n","ypred: [Value(data = 0.9321257329545997), Value(data = -0.9121094556518683)]\n","step: 40, loss: 0.012331663912761224\n","-------\n","ypred: [Value(data = 0.9328801631792806), Value(data = -0.9132827249878491)]\n","step: 41, loss: 0.012024958280373008\n","-------\n","ypred: [Value(data = 0.9336098025559855), Value(data = -0.9144132023928252)]\n","step: 42, loss: 0.011732758241306736\n","-------\n","ypred: [Value(data = 0.9343159943131628), Value(data = -0.9155033906443476)]\n","step: 43, loss: 0.011454065595670184\n","-------\n","ypred: [Value(data = 0.9349999810577225), Value(data = -0.9165555934261149)]\n","step: 44, loss: 0.011187971450964266\n","-------\n","ypred: [Value(data = 0.93566291432922), Value(data = -0.9175719351794415)]\n","step: 45, loss: 0.010933646462671486\n","-------\n","ypred: [Value(data = 0.9363058630622821), Value(data = -0.9185543785921086)]\n","step: 46, loss: 0.010690332326758326\n","-------\n","ypred: [Value(data = 0.9369298211023267), Value(data = -0.9195047400494943)]\n","step: 47, loss: 0.010457334340683994\n","-------\n","ypred: [Value(data = 0.9375357138977624), Value(data = -0.9204247033224339)]\n","step: 48, loss: 0.010234014879584866\n","-------\n","ypred: [Value(data = 0.9381244044736637), Value(data = -0.9213158317245067)]\n","step: 49, loss: 0.010019787658944908\n","-------\n","ypred: [Value(data = 0.9386966987767159), Value(data = -0.9221795789367562)]\n","step: 50, loss: 0.009814112675333268\n","-------\n","ypred: [Value(data = 0.9392533504684796), Value(data = -0.923017298668884)]\n","step: 51, loss: 0.009616491733541174\n","-------\n","ypred: [Value(data = 0.9397950652332974), Value(data = -0.9238302533017199)]\n","step: 52, loss: 0.009426464482343078\n","-------\n","ypred: [Value(data = 0.9403225046580993), Value(data = -0.9246196216353759)]\n","step: 53, loss: 0.009243604892676466\n","-------\n","ypred: [Value(data = 0.9408362897336907), Value(data = -0.9253865058503102)]\n","step: 54, loss: 0.00906751812170158\n","-------\n","ypred: [Value(data = 0.9413370040205664), Value(data = -0.9261319377739609)]\n","step: 55, loss: 0.008897837714313031\n","-------\n","ypred: [Value(data = 0.9418251965167371), Value(data = -0.9268568845332705)]\n","step: 56, loss: 0.008734223100495573\n","-------\n","ypred: [Value(data = 0.9423013842602782), Value(data = -0.9275622536628922)]\n","step: 57, loss: 0.00857635735267924\n","-------\n","ypred: [Value(data = 0.9427660546952293), Value(data = -0.9282488977298794)]\n","step: 58, loss: 0.008423945172126785\n","-------\n","ypred: [Value(data = 0.9432196678259573), Value(data = -0.9289176185279724)]\n","step: 59, loss: 0.008276711077529478\n","-------\n","ypred: [Value(data = 0.9436626581820527), Value(data = -0.9295691708879703)]\n","step: 60, loss: 0.008134397772520157\n","-------\n","ypred: [Value(data = 0.9440954366132188), Value(data = -0.9302042661449919)]\n","step: 61, loss: 0.007996764671825765\n","-------\n","ypred: [Value(data = 0.9445183919313223), Value(data = -0.9308235752985009)]\n","step: 62, loss: 0.007863586568368542\n","-------\n","ypred: [Value(data = 0.9449318924148109), Value(data = -0.9314277318967262)]\n","step: 63, loss: 0.007734652425841218\n","-------\n","ypred: [Value(data = 0.9453362871889702), Value(data = -0.9320173346733995)]\n","step: 64, loss: 0.007609764283195306\n","-------\n","ypred: [Value(data = 0.9457319074939933), Value(data = -0.9325929499615324)]\n","step: 65, loss: 0.007488736259128976\n","-------\n","ypred: [Value(data = 0.9461190678515186), Value(data = -0.9331551139061459)]\n","step: 66, loss: 0.0073713936460895885\n","-------\n","ypred: [Value(data = 0.9464980671391371), Value(data = -0.933704334495426)]\n","step: 67, loss: 0.007257572084542646\n","-------\n","ypred: [Value(data = 0.9468691895813572), Value(data = -0.9342410934276282)]\n","step: 68, loss: 0.007147116809335684\n","-------\n","ypred: [Value(data = 0.94723270566462), Value(data = -0.9347658478291855)]\n","step: 69, loss: 0.0070398819609216075\n","-------\n","ypred: [Value(data = 0.9475888729831725), Value(data = -0.9352790318378138)]\n","step: 70, loss: 0.006935729955024743\n","-------\n","ypred: [Value(data = 0.9479379370219095), Value(data = -0.9357810580629617)]\n","step: 71, loss: 0.006834530905047348\n","-------\n","ypred: [Value(data = 0.9482801318816768), Value(data = -0.9362723189346628)]\n","step: 72, loss: 0.0067361620921420826\n","-------\n","ypred: [Value(data = 0.9486156809519845), Value(data = -0.9367531879507184)]\n","step: 73, loss: 0.006640507478425409\n","-------\n","ypred: [Value(data = 0.9489447975355912), Value(data = -0.9372240208311344)]\n","step: 74, loss: 0.006547457259291614\n","-------\n","ypred: [Value(data = 0.9492676854289888), Value(data = -0.9376851565878405)]\n","step: 75, loss: 0.006456907451214002\n","-------\n","ypred: [Value(data = 0.9495845394624315), Value(data = -0.938136918516936)]\n","step: 76, loss: 0.0063687595117953395\n","-------\n","ypred: [Value(data = 0.9498955460028102), Value(data = -0.9385796151199982)]\n","step: 77, loss: 0.006282919989164058\n","-------\n","ypred: [Value(data = 0.9502008834223635), Value(data = -0.9390135409603584)]\n","step: 78, loss: 0.006199300198106912\n","-------\n","ypred: [Value(data = 0.9505007225359442), Value(data = -0.9394389774597002)]\n","step: 79, loss: 0.006117815920590286\n","-------\n","ypred: [Value(data = 0.9507952270093089), Value(data = -0.9398561936398221)]\n","step: 80, loss: 0.006038387128556026\n","-------\n","ypred: [Value(data = 0.9510845537406826), Value(data = -0.9402654468139692)]\n","step: 81, loss: 0.005960937727082917\n","-------\n","ypred: [Value(data = 0.9513688532176445), Value(data = -0.940666983231727)]\n","step: 82, loss: 0.005885395316191163\n","-------\n","ypred: [Value(data = 0.9516482698512059), Value(data = -0.9410610386811173)]\n","step: 83, loss: 0.005811690969730552\n","-------\n","ypred: [Value(data = 0.9519229422887867), Value(data = -0.941447839051207)]\n","step: 84, loss: 0.0057397590299406945\n","-------\n","ypred: [Value(data = 0.9521930037076531), Value(data = -0.9418276008582489)]\n","step: 85, loss: 0.00566953691640367\n","-------\n","ypred: [Value(data = 0.9524585820902457), Value(data = -0.942200531738116)]\n","step: 86, loss: 0.00560096494822644\n","-------\n","ypred: [Value(data = 0.9527198004827052), Value(data = -0.942566830907543)]\n","step: 87, loss: 0.005533986178397959\n","-------\n","ypred: [Value(data = 0.9529767772378018), Value(data = -0.9429266895964838)]\n","step: 88, loss: 0.005468546239359417\n","-------\n","ypred: [Value(data = 0.9532296262433662), Value(data = -0.9432802914536951)]\n","step: 89, loss: 0.005404593198913\n","-------\n","ypred: [Value(data = 0.9534784571372399), Value(data = -0.9436278129274807)]\n","step: 90, loss: 0.005342077425670743\n","-------\n","ypred: [Value(data = 0.9537233755096759), Value(data = -0.9439694236233707)]\n","step: 91, loss: 0.005280951463315758\n","-------\n","ypred: [Value(data = 0.9539644830940487), Value(data = -0.9443052866403704)]\n","step: 92, loss: 0.005221169913009435\n","-------\n","ypred: [Value(data = 0.9542018779466658), Value(data = -0.9446355588872728)]\n","step: 93, loss: 0.0051626893233367365\n","-------\n","ypred: [Value(data = 0.9544356546164102), Value(data = -0.9449603913804199)]\n","step: 94, loss: 0.005105468087231618\n","-------\n","ypred: [Value(data = 0.9546659043048898), Value(data = -0.945279929524178)]\n","step: 95, loss: 0.005049466345372325\n","-------\n","ypred: [Value(data = 0.954892715017714), Value(data = -0.9455943133753049)]\n","step: 96, loss: 0.004994645895577696\n","-------\n","ypred: [Value(data = 0.955116171707477), Value(data = -0.9459036778922829)]\n","step: 97, loss: 0.00494097010777457\n","-------\n","ypred: [Value(data = 0.9553363564089786), Value(data = -0.9462081531706233)]\n","step: 98, loss: 0.004888403844140904\n","-------\n","ypred: [Value(data = 0.9555533483671794), Value(data = -0.9465078646650569)]\n","step: 99, loss: 0.004836913384061183\n","-------\n","ypred: [Value(data = 0.9557672241583443), Value(data = -0.9468029333994712)]\n","step: 100, loss: 0.004786466353559261\n","-------\n","ypred: [Value(data = 0.9559780578048045), Value(data = -0.9470934761653793)]\n","step: 101, loss: 0.004737031658900421\n","-------\n","ypred: [Value(data = 0.9561859208837291), Value(data = -0.9473796057096548)]\n","step: 102, loss: 0.0046885794240782335\n","-------\n","ypred: [Value(data = 0.9563908826302739), Value(data = -0.9476614309122102)]\n","step: 103, loss: 0.004641080931923887\n","-------\n","ypred: [Value(data = 0.9565930100354494), Value(data = -0.9479390569542477)]\n","step: 104, loss: 0.004594508568595658\n","-------\n","ypred: [Value(data = 0.9567923679390236), Value(data = -0.9482125854776704)]\n","step: 105, loss: 0.004548835771224314\n","-------\n","ypred: [Value(data = 0.9569890191177568), Value(data = -0.9484821147361914)]\n","step: 106, loss: 0.00450403697850764\n","-------\n","ypred: [Value(data = 0.9571830243692431), Value(data = -0.9487477397386513)]\n","step: 107, loss: 0.004460087584061851\n","-------\n","ypred: [Value(data = 0.9573744425916141), Value(data = -0.9490095523850111)]\n","step: 108, loss: 0.004416963892352535\n","-------\n","ypred: [Value(data = 0.9575633308593473), Value(data = -0.9492676415954577)]\n","step: 109, loss: 0.004374643077040155\n","-------\n","ypred: [Value(data = 0.9577497444953993), Value(data = -0.9495220934330333)]\n","step: 110, loss: 0.004333103141587464\n","-------\n","ypred: [Value(data = 0.9579337371398757), Value(data = -0.9497729912201642)]\n","step: 111, loss: 0.0042923228819867695\n","-------\n","ypred: [Value(data = 0.9581153608154312), Value(data = -0.9500204156494464)]\n","step: 112, loss: 0.004252281851475615\n","-------\n","ypred: [Value(data = 0.9582946659895836), Value(data = -0.9502644448890194)]\n","step: 113, loss: 0.004212960327117786\n","-------\n","ypred: [Value(data = 0.9584717016341118), Value(data = -0.9505051546828357)]\n","step: 114, loss: 0.004174339278136256\n","-------\n","ypred: [Value(data = 0.9586465152816994), Value(data = -0.950742618446117)]\n","step: 115, loss: 0.004136400335891537\n","-------\n","ypred: [Value(data = 0.9588191530799726), Value(data = -0.9509769073562654)]\n","step: 116, loss: 0.00409912576540692\n","-------\n","ypred: [Value(data = 0.9589896598430727), Value(data = -0.9512080904394857)]\n","step: 117, loss: 0.004062498438348286\n","-------\n","ypred: [Value(data = 0.9591580791008981), Value(data = -0.9514362346533517)]\n","step: 118, loss: 0.004026501807372814\n","-------\n","ypred: [Value(data = 0.9593244531461356), Value(data = -0.9516614049655427)]\n","step: 119, loss: 0.003991119881766176\n","-------\n","ypred: [Value(data = 0.9594888230792006), Value(data = -0.9518836644289549)]\n","step: 120, loss: 0.003956337204293733\n","-------\n","ypred: [Value(data = 0.9596512288511935), Value(data = -0.9521030742533839)]\n","step: 121, loss: 0.003922138829195616\n","-------\n","ypred: [Value(data = 0.959811709304977), Value(data = -0.9523196938739644)]\n","step: 122, loss: 0.0038885103012601422\n","-------\n","ypred: [Value(data = 0.9599703022144686), Value(data = -0.9525335810165326)]\n","step: 123, loss: 0.0038554376359150534\n","-------\n","ypred: [Value(data = 0.9601270443222412), Value(data = -0.95274479176008)]\n","step: 124, loss: 0.003822907300278726\n","-------\n","ypred: [Value(data = 0.9602819713755165), Value(data = -0.9529533805964453)]\n","step: 125, loss: 0.003790906195118219\n","-------\n","ypred: [Value(data = 0.9604351181606325), Value(data = -0.9531594004873893)]\n","step: 126, loss: 0.003759421637663891\n","-------\n","ypred: [Value(data = 0.9605865185360603), Value(data = -0.9533629029191885)]\n","step: 127, loss: 0.0037284413452333626\n","-------\n","ypred: [Value(data = 0.9607362054640417), Value(data = -0.9535639379548684)]\n","step: 128, loss: 0.003697953419621262\n","-------\n","ypred: [Value(data = 0.9608842110409185), Value(data = -0.953762554284205)]\n","step: 129, loss: 0.0036679463322124954\n","-------\n","ypred: [Value(data = 0.9610305665262109), Value(data = -0.9539587992715952)]\n","step: 130, loss: 0.003638408909781337\n","-------\n","ypred: [Value(data = 0.9611753023705121), Value(data = -0.9541527190019123)]\n","step: 131, loss: 0.0036093303209387747\n","-------\n","ypred: [Value(data = 0.9613184482422512), Value(data = -0.9543443583244401)]\n","step: 132, loss: 0.0035807000631945213\n","-------\n","ypred: [Value(data = 0.9614600330533813), Value(data = -0.9545337608949808)]\n","step: 133, loss: 0.0035525079506012382\n","-------\n","ypred: [Value(data = 0.9616000849840407), Value(data = -0.9547209692162271)]\n","step: 134, loss: 0.003524744101950753\n","-------\n","ypred: [Value(data = 0.9617386315062383), Value(data = -0.9549060246764821)]\n","step: 135, loss: 0.003497398929493462\n","-------\n","ypred: [Value(data = 0.9618756994066062), Value(data = -0.9550889675868042)]\n","step: 136, loss: 0.0034704631281545716\n","-------\n","ypred: [Value(data = 0.9620113148082647), Value(data = -0.9552698372166539)]\n","step: 137, loss: 0.0034439276652214043\n","-------\n","ypred: [Value(data = 0.9621455031918388), Value(data = -0.9554486718281138)]\n","step: 138, loss: 0.0034177837704781914\n","-------\n","ypred: [Value(data = 0.9622782894156648), Value(data = -0.9556255087087456)]\n","step: 139, loss: 0.003392022926765953\n","-------\n","ypred: [Value(data = 0.9624096977352264), Value(data = -0.9558003842031519)]\n","step: 140, loss: 0.0033666368609460203\n","-------\n","ypred: [Value(data = 0.9625397518218505), Value(data = -0.9559733337432994)]\n","step: 141, loss: 0.003341617535247454\n","-------\n","ypred: [Value(data = 0.9626684747806994), Value(data = -0.9561443918776614)]\n","step: 142, loss: 0.0033169571389794067\n","-------\n","ypred: [Value(data = 0.9627958891680898), Value(data = -0.9563135922992343)]\n","step: 143, loss: 0.0032926480805905807\n","-------\n","ypred: [Value(data = 0.9629220170081659), Value(data = -0.9564809678724776)]\n","step: 144, loss: 0.0032686829800590627\n","-------\n","ypred: [Value(data = 0.9630468798089586), Value(data = -0.9566465506592281)]\n","step: 145, loss: 0.0032450546615964237\n","-------\n","ypred: [Value(data = 0.9631704985778543), Value(data = -0.9568103719436302)]\n","step: 146, loss: 0.0032217561466514\n","-------\n","ypred: [Value(data = 0.9632928938365002), Value(data = -0.9569724622561313)]\n","step: 147, loss: 0.003198780647198491\n","-------\n","ypred: [Value(data = 0.9634140856351702), Value(data = -0.9571328513965774)]\n","step: 148, loss: 0.003176121559298574\n","-------\n","ypred: [Value(data = 0.9635340935666145), Value(data = -0.9572915684564557)]\n","step: 149, loss: 0.00315377245691804\n","-------\n","ypred: [Value(data = 0.9636529367794133), Value(data = -0.9574486418403135)]\n","step: 150, loss: 0.0031317270859952404\n","-------\n","ypred: [Value(data = 0.9637706339908584), Value(data = -0.9576040992863933)]\n","step: 151, loss: 0.0031099793587423398\n","-------\n","ypred: [Value(data = 0.9638872034993777), Value(data = -0.9577579678865195)]\n","step: 152, loss: 0.0030885233481716774\n","-------\n","ypred: [Value(data = 0.9640026631965276), Value(data = -0.9579102741052634)]\n","step: 153, loss: 0.0030673532828366873\n","-------\n","ypred: [Value(data = 0.9641170305785658), Value(data = -0.9580610437984209)]\n","step: 154, loss: 0.003046463541777549\n","-------\n","ypred: [Value(data = 0.964230322757624), Value(data = -0.9582103022308319)]\n","step: 155, loss: 0.0030258486496621637\n","-------\n","ypred: [Value(data = 0.9643425564724974), Value(data = -0.9583580740935653)]\n","step: 156, loss: 0.003005503272114029\n","-------\n","ypred: [Value(data = 0.964453748099065), Value(data = -0.9585043835205023)]\n","step: 157, loss: 0.0029854222112182865\n","-------\n","ypred: [Value(data = 0.9645639136603558), Value(data = -0.9586492541043344)]\n","step: 158, loss: 0.0029656004011986235\n","-------\n","ypred: [Value(data = 0.9646730688362762), Value(data = -0.9587927089120096)]\n","step: 159, loss: 0.0029460329042568477\n","-------\n","ypred: [Value(data = 0.9647812289730116), Value(data = -0.9589347704996412)]\n","step: 160, loss: 0.0029267149065685836\n","-------\n","ypred: [Value(data = 0.9648884090921124), Value(data = -0.9590754609269051)]\n","step: 161, loss: 0.0029076417144281285\n","-------\n","ypred: [Value(data = 0.9649946238992821), Value(data = -0.9592148017709469)]\n","step: 162, loss: 0.0028888087505358675\n","-------\n","ypred: [Value(data = 0.9650998877928741), Value(data = -0.9593528141398143)]\n","step: 163, loss: 0.0028702115504224534\n","-------\n","ypred: [Value(data = 0.9652042148721108), Value(data = -0.9594895186854395)]\n","step: 164, loss: 0.0028518457590035865\n","-------\n","ypred: [Value(data = 0.9653076189450355), Value(data = -0.9596249356161828)]\n","step: 165, loss: 0.002833707127260247\n","-------\n","ypred: [Value(data = 0.9654101135362074), Value(data = -0.9597590847089614)]\n","step: 166, loss: 0.0028157915090386027\n","-------\n","ypred: [Value(data = 0.9655117118941489), Value(data = -0.9598919853209741)]\n","step: 167, loss: 0.0027980948579651513\n","-------\n","ypred: [Value(data = 0.9656124269985549), Value(data = -0.9600236564010406)]\n","step: 168, loss: 0.0027806132244717805\n","-------\n","ypred: [Value(data = 0.9657122715672729), Value(data = -0.9601541165005695)]\n","step: 169, loss: 0.002763342752926632\n","-------\n","ypred: [Value(data = 0.9658112580630633), Value(data = -0.9602833837841696)]\n","step: 170, loss: 0.002746279678866018\n","-------\n","ypred: [Value(data = 0.9659093987001465), Value(data = -0.9604114760399164)]\n","step: 171, loss: 0.002729420326323689\n","-------\n","ypred: [Value(data = 0.9660067054505459), Value(data = -0.9605384106892912)]\n","step: 172, loss: 0.0027127611052529896\n","-------\n","ypred: [Value(data = 0.9661031900502343), Value(data = -0.9606642047967984)]\n","step: 173, loss: 0.002696298509038758\n","-------\n","ypred: [Value(data = 0.9661988640050899), Value(data = -0.96078887507928)]\n","step: 174, loss: 0.002680029112094714\n","-------\n","ypred: [Value(data = 0.9662937385966702), Value(data = -0.9609124379149336)]\n","step: 175, loss: 0.0026639495675435157\n","-------\n","ypred: [Value(data = 0.9663878248878086), Value(data = -0.9610349093520489)]\n","step: 176, loss: 0.0026480566049756618\n","-------\n","ypred: [Value(data = 0.9664811337280419), Value(data = -0.9611563051174681)]\n","step: 177, loss: 0.002632347028284647\n","-------\n","ypred: [Value(data = 0.9665736757588741), Value(data = -0.9612766406247863)]\n","step: 178, loss: 0.002616817713574828\n","-------\n","ypred: [Value(data = 0.9666654614188811), Value(data = -0.961395930982296)]\n","step: 179, loss: 0.002601465607139756\n","-------\n","ypred: [Value(data = 0.9667565009486643), Value(data = -0.9615141910006878)]\n","step: 180, loss: 0.0025862877235076987\n","-------\n","ypred: [Value(data = 0.9668468043956556), Value(data = -0.961631435200517)]\n","step: 181, loss: 0.0025712811435520473\n","-------\n","ypred: [Value(data = 0.9669363816187823), Value(data = -0.9617476778194417)]\n","step: 182, loss: 0.0025564430126640315\n","-------\n","ypred: [Value(data = 0.9670252422929929), Value(data = -0.9618629328192411)]\n","step: 183, loss: 0.0025417705389855433\n","-------\n","ypred: [Value(data = 0.9671133959136522), Value(data = -0.961977213892626)]\n","step: 184, loss: 0.002527260991699299\n","-------\n","ypred: [Value(data = 0.9672008518008081), Value(data = -0.9620905344698412)]\n","step: 185, loss: 0.002512911699374857\n","-------\n","ypred: [Value(data = 0.9672876191033348), Value(data = -0.9622029077250751)]\n","step: 186, loss: 0.002498720048367691\n","-------\n","ypred: [Value(data = 0.9673737068029573), Value(data = -0.9623143465826766)]\n","step: 187, loss: 0.0024846834812700157\n","-------\n","ypred: [Value(data = 0.9674591237181602), Value(data = -0.9624248637231916)]\n","step: 188, loss: 0.0024707994954107264\n","-------\n","ypred: [Value(data = 0.9675438785079862), Value(data = -0.9625344715892206)]\n","step: 189, loss: 0.0024570656414032825\n","-------\n","ypred: [Value(data = 0.9676279796757254), Value(data = -0.9626431823911061)]\n","step: 190, loss: 0.002443479521739408\n","-------\n","ypred: [Value(data = 0.9677114355725027), Value(data = -0.9627510081124561)]\n","step: 191, loss: 0.0024300387894269568\n","-------\n","ypred: [Value(data = 0.967794254400763), Value(data = -0.9628579605155065)]\n","step: 192, loss: 0.002416741146670441\n","-------\n","ypred: [Value(data = 0.9678764442176604), Value(data = -0.9629640511463325)]\n","step: 193, loss: 0.002403584343592556\n","-------\n","ypred: [Value(data = 0.9679580129383522), Value(data = -0.9630692913399075)]\n","step: 194, loss: 0.0023905661769954402\n","-------\n","ypred: [Value(data = 0.9680389683392032), Value(data = -0.9631736922250212)]\n","step: 195, loss: 0.0023776844891599145\n","-------\n","ypred: [Value(data = 0.9681193180609015), Value(data = -0.963277264729059)]\n","step: 196, loss: 0.002364937166681582\n","-------\n","ypred: [Value(data = 0.9681990696114883), Value(data = -0.9633800195826432)]\n","step: 197, loss: 0.002352322139342565\n","-------\n","ypred: [Value(data = 0.9682782303693073), Value(data = -0.9634819673241501)]\n","step: 198, loss: 0.002339837379017177\n","-------\n","ypred: [Value(data = 0.9683568075858723), Value(data = -0.9635831183040966)]\n","step: 199, loss: 0.0023274808986109346\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 1.1391258239746094\n","Epoch 10 loss: 0.4605425000190735\n","Epoch 20 loss: 0.05065038800239563\n","Epoch 30 loss: 0.0016020669136196375\n","\n","Prediction:\n","tensor([[ 0.9940],\n","        [-0.9940]])\n","Loss: 5.3315689001465216e-05\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[-0.5252,  0.1233,  0.0581],\n","        [ 0.2622,  0.5055,  0.1071],\n","        [-0.3412, -0.4693,  0.3877],\n","        [ 0.4759, -0.6939,  0.3442]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.7386, -1.6699],\n","        [ 1.9339,  0.3347],\n","        [-2.4781, -0.3605],\n","        [-1.4741,  2.2936]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[ 0.1440],\n","        [-0.5113],\n","        [ 0.0463],\n","        [ 0.0045]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.5946, -1.5259],\n","        [ 1.4226, -0.1766],\n","        [-2.4318, -0.3142],\n","        [-1.4696,  2.2981]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.5332, -0.9097],\n","        [ 0.8901, -0.1748],\n","        [-0.9847, -0.3042],\n","        [-0.8995,  0.9800]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[ 0.0639,  0.2084,  0.3430,  0.5569],\n","        [-0.4494,  0.4168, -0.0222,  0.3800],\n","        [-0.2272, -0.3290, -0.1282,  0.5970],\n","        [ 0.1309, -0.3683, -0.0435,  0.6635]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.5332, -0.9097],\n","        [ 0.8901, -0.1748],\n","        [-0.9847, -0.3042],\n","        [-0.8995,  0.9800]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.6872,  0.3468],\n","        [ 0.2906,  0.7152],\n","        [-0.5825,  0.8883],\n","        [-0.9517,  0.6088]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.0377],\n","        [-0.2932],\n","        [-0.3692],\n","        [-0.1525]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.7249,  0.3091],\n","        [-0.0026,  0.4220],\n","        [-0.9517,  0.5191],\n","        [-1.1041,  0.4563]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.6199,  0.2996],\n","        [-0.0026,  0.3986],\n","        [-0.7406,  0.4770],\n","        [-0.8020,  0.4271]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[-0.3984, -0.2570, -0.6540, -0.5876]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.6199,  0.2996],\n","        [-0.0026,  0.3986],\n","        [-0.7406,  0.4770],\n","        [-0.8020,  0.4271]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 1.2032, -0.7847]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[-0.2093]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9940, -0.9940]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9940, -0.9940]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9940, -0.9940]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(7.2735e-05)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9940, -0.9940]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-0.0060,  0.0060]])\n","squared_difference: tensor([[3.6405e-05, 3.6330e-05]])\n"]},{"data":{"text/plain":["tensor(3.6368e-05)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference) / 2\n","loss"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["difference\n","len(squared_difference)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(3.6368e-05)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(7.2735e-05)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
