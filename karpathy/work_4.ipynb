{"cells":[{"cell_type":"markdown","metadata":{},"source":["## [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import random\n","\n","class Neuron:\n","    \n","    def __init__(self, nin):\n","#### my add ##########################################        \n","        # random.seed(12345)  # WARNING: all neurons will have the same weights and bias\n","######################################################        \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["### Neuron in Neural Network\n","<!-- ### Simple Neural Network: Single Neuron with 3 Inputs -->\n","<img src=\"..\\karpathy\\img\\Nertual_Network_1_Neuron_3_Inputs.png\">"]},{"cell_type":"markdown","metadata":{},"source":["#### Activation Function: Tanh"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 400x200 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["_num = np.arange(-5, 5, 0.2)\n","plt.figure(figsize=(4, 2))\n","plt.plot(_num, np.tanh(_num))\n","plt.title(\"Tanh Activation\")\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Output with Forward Pass"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["neuron output: -0.7071067801767762\n"]}],"source":["# inputs\n","x0 = Value(-3.0, label='x0')\n","x1 = Value(0.0, label='x1')\n","x2 = Value(0.5, label='x2')\n","\n","# weights\n","w0 = Value(2.0, label='w0')\n","w1 = Value(1.0, label='w1')\n","w2 = Value(1.0, label='w2')\n","\n","# bias\n","b = Value(4.618626415, label='b')\n","\n","# forward pass\n","# x0*w0 + x1*w1 + x2*w2 + b\n","x0w0 = x0*w0; x0w0.label = 'x0*w0'\n","x1w1 = x1*w1; x1w1.label = 'x1*w1'\n","x2w2 = x2*w2; x2w2.label = 'x2*w2'\n","n_sum = x0w0 + x1w1 + x2w2; n_sum.label = 'x0w0 + x1w1 + x2w2'\n","n = n_sum + b; n.label = 'n'\n","out_0 = n.tanh(); out_0.label = 'out_0'\n","print(f'neuron output: {out_0.data}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Gradient with Backward Pass"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w0.grad(i.e. d(output)/d(w0)): -1.500000004284097\n"]},{"data":{"image/svg+xml":["<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n","<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n"," \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n","<!-- Generated by graphviz version 2.46.0 (20210118.1747)\n"," -->\n","<!-- Pages: 1 -->\n","<svg width=\"1995pt\" height=\"265pt\"\n"," viewBox=\"0.00 0.00 1995.00 265.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n","<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 261)\">\n","<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-261 1991,-261 1991,4 -4,4\"/>\n","<!-- 2657282669584 -->\n","<g id=\"node1\" class=\"node\">\n","<title>2657282669584</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"3.5,-220.5 3.5,-256.5 197.5,-256.5 197.5,-220.5 3.5,-220.5\"/>\n","<text text-anchor=\"middle\" x=\"18.5\" y=\"-234.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x1</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"33.5,-220.5 33.5,-256.5 \"/>\n","<text text-anchor=\"middle\" x=\"74\" y=\"-234.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"114.5,-220.5 114.5,-256.5 \"/>\n","<text text-anchor=\"middle\" x=\"156\" y=\"-234.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282670112* -->\n","<g id=\"node3\" class=\"node\">\n","<title>2657282670112*</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"264\" cy=\"-183.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"264\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n","</g>\n","<!-- 2657282669584&#45;&gt;2657282670112* -->\n","<g id=\"edge17\" class=\"edge\">\n","<title>2657282669584&#45;&gt;2657282670112*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M173.4,-220.46C182.75,-217.68 192.16,-214.67 201,-211.5 211.53,-207.72 222.76,-202.9 232.72,-198.34\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"234.25,-201.48 241.83,-194.07 231.29,-195.14 234.25,-201.48\"/>\n","</g>\n","<!-- 2657282670112 -->\n","<g id=\"node2\" class=\"node\">\n","<title>2657282670112</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"329.5,-165.5 329.5,-201.5 546.5,-201.5 546.5,-165.5 329.5,-165.5\"/>\n","<text text-anchor=\"middle\" x=\"356\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x1*w1</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"382.5,-165.5 382.5,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"423\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"463.5,-165.5 463.5,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"505\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282601072+ -->\n","<g id=\"node8\" class=\"node\">\n","<title>2657282601072+</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"612\" cy=\"-128.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"612\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n","</g>\n","<!-- 2657282670112&#45;&gt;2657282601072+ -->\n","<g id=\"edge10\" class=\"edge\">\n","<title>2657282670112&#45;&gt;2657282601072+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M520,-165.42C529.85,-162.69 539.71,-159.71 549,-156.5 559.58,-152.85 570.81,-148.07 580.78,-143.49\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"582.32,-146.63 589.87,-139.21 579.33,-140.3 582.32,-146.63\"/>\n","</g>\n","<!-- 2657282670112*&#45;&gt;2657282670112 -->\n","<g id=\"edge1\" class=\"edge\">\n","<title>2657282670112*&#45;&gt;2657282670112</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M291.12,-183.5C299.31,-183.5 308.94,-183.5 319.28,-183.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"319.39,-187 329.39,-183.5 319.39,-180 319.39,-187\"/>\n","</g>\n","<!-- 2657282669632 -->\n","<g id=\"node4\" class=\"node\">\n","<title>2657282669632</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"0,-110.5 0,-146.5 201,-146.5 201,-110.5 0,-110.5\"/>\n","<text text-anchor=\"middle\" x=\"16.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"33,-110.5 33,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"73.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 2.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"114,-110.5 114,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"157.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad &#45;1.5000</text>\n","</g>\n","<!-- 2657282670208* -->\n","<g id=\"node10\" class=\"node\">\n","<title>2657282670208*</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"264\" cy=\"-128.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"264\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n","</g>\n","<!-- 2657282669632&#45;&gt;2657282670208* -->\n","<g id=\"edge14\" class=\"edge\">\n","<title>2657282669632&#45;&gt;2657282670208*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M201.02,-128.5C210.04,-128.5 218.72,-128.5 226.59,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"226.77,-132 236.77,-128.5 226.77,-125 226.77,-132\"/>\n","</g>\n","<!-- 2657282601552 -->\n","<g id=\"node5\" class=\"node\">\n","<title>2657282601552</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1451,-54.5 1451,-90.5 1643,-90.5 1643,-54.5 1451,-54.5\"/>\n","<text text-anchor=\"middle\" x=\"1462.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">n</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1474,-54.5 1474,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1517\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;0.8814</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1560,-54.5 1560,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1601.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282601696tanh -->\n","<g id=\"node17\" class=\"node\">\n","<title>2657282601696tanh</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"1706\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"1706\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">tanh</text>\n","</g>\n","<!-- 2657282601552&#45;&gt;2657282601696tanh -->\n","<g id=\"edge16\" class=\"edge\">\n","<title>2657282601552&#45;&gt;2657282601696tanh</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1643.4,-72.5C1652.32,-72.5 1660.93,-72.5 1668.75,-72.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1668.86,-76 1678.86,-72.5 1668.86,-69 1668.86,-76\"/>\n","</g>\n","<!-- 2657282601552+ -->\n","<g id=\"node6\" class=\"node\">\n","<title>2657282601552+</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"1388\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"1388\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n","</g>\n","<!-- 2657282601552+&#45;&gt;2657282601552 -->\n","<g id=\"edge2\" class=\"edge\">\n","<title>2657282601552+&#45;&gt;2657282601552</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1415.28,-72.5C1422.78,-72.5 1431.44,-72.5 1440.67,-72.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1440.87,-76 1450.87,-72.5 1440.87,-69 1440.87,-76\"/>\n","</g>\n","<!-- 2657282601072 -->\n","<g id=\"node7\" class=\"node\">\n","<title>2657282601072</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"689,-110.5 689,-146.5 878,-146.5 878,-110.5 689,-110.5\"/>\n","<text text-anchor=\"middle\" x=\"699\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"709,-110.5 709,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"752\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"795,-110.5 795,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"836.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282601408+ -->\n","<g id=\"node21\" class=\"node\">\n","<title>2657282601408+</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"955\" cy=\"-100.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"955\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">+</text>\n","</g>\n","<!-- 2657282601072&#45;&gt;2657282601408+ -->\n","<g id=\"edge8\" class=\"edge\">\n","<title>2657282601072&#45;&gt;2657282601408+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M878.17,-113.03C892.5,-110.66 906.49,-108.35 918.47,-106.37\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"919.14,-109.81 928.44,-104.72 918,-102.9 919.14,-109.81\"/>\n","</g>\n","<!-- 2657282601072+&#45;&gt;2657282601072 -->\n","<g id=\"edge3\" class=\"edge\">\n","<title>2657282601072+&#45;&gt;2657282601072</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M639.11,-128.5C650.28,-128.5 664.11,-128.5 678.74,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"678.97,-132 688.97,-128.5 678.97,-125 678.97,-132\"/>\n","</g>\n","<!-- 2657282670208 -->\n","<g id=\"node9\" class=\"node\">\n","<title>2657282670208</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"327,-110.5 327,-146.5 549,-146.5 549,-110.5 327,-110.5\"/>\n","<text text-anchor=\"middle\" x=\"353.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x0*w0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"380,-110.5 380,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"423\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"466,-110.5 466,-146.5 \"/>\n","<text text-anchor=\"middle\" x=\"507.5\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282670208&#45;&gt;2657282601072+ -->\n","<g id=\"edge18\" class=\"edge\">\n","<title>2657282670208&#45;&gt;2657282601072+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M549.39,-128.5C558.39,-128.5 566.99,-128.5 574.77,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"574.82,-132 584.82,-128.5 574.82,-125 574.82,-132\"/>\n","</g>\n","<!-- 2657282670208*&#45;&gt;2657282670208 -->\n","<g id=\"edge4\" class=\"edge\">\n","<title>2657282670208*&#45;&gt;2657282670208</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M291.12,-128.5C298.66,-128.5 307.41,-128.5 316.81,-128.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"316.82,-132 326.81,-128.5 316.81,-125 316.82,-132\"/>\n","</g>\n","<!-- 2657282669680 -->\n","<g id=\"node11\" class=\"node\">\n","<title>2657282669680</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1,-55.5 1,-91.5 200,-91.5 200,-55.5 1,-55.5\"/>\n","<text text-anchor=\"middle\" x=\"16\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"31,-55.5 31,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"74\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"117,-55.5 117,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"158.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 1.0000</text>\n","</g>\n","<!-- 2657282669680&#45;&gt;2657282670208* -->\n","<g id=\"edge15\" class=\"edge\">\n","<title>2657282669680&#45;&gt;2657282670208*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M170.4,-91.57C180.73,-94.66 191.2,-98 201,-101.5 211.28,-105.17 222.28,-109.73 232.11,-114.06\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"230.93,-117.37 241.48,-118.27 233.79,-110.98 230.93,-117.37\"/>\n","</g>\n","<!-- 2657282601120 -->\n","<g id=\"node12\" class=\"node\">\n","<title>2657282601120</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"675,-55.5 675,-91.5 892,-91.5 892,-55.5 675,-55.5\"/>\n","<text text-anchor=\"middle\" x=\"701.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x2*w2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"728,-55.5 728,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"768.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.5000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"809,-55.5 809,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"850.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282601120&#45;&gt;2657282601408+ -->\n","<g id=\"edge12\" class=\"edge\">\n","<title>2657282601120&#45;&gt;2657282601408+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M892.33,-90.68C901.68,-92.17 910.63,-93.59 918.66,-94.87\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"918.23,-98.35 928.66,-96.46 919.33,-91.43 918.23,-98.35\"/>\n","</g>\n","<!-- 2657282601120* -->\n","<g id=\"node13\" class=\"node\">\n","<title>2657282601120*</title>\n","<ellipse fill=\"none\" stroke=\"black\" cx=\"612\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n","<text text-anchor=\"middle\" x=\"612\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">*</text>\n","</g>\n","<!-- 2657282601120*&#45;&gt;2657282601120 -->\n","<g id=\"edge5\" class=\"edge\">\n","<title>2657282601120*&#45;&gt;2657282601120</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M639.11,-73.5C646.55,-73.5 655.16,-73.5 664.4,-73.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"664.66,-77 674.66,-73.5 664.66,-70 664.66,-77\"/>\n","</g>\n","<!-- 2657282669728 -->\n","<g id=\"node14\" class=\"node\">\n","<title>2657282669728</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"341,-55.5 341,-91.5 535,-91.5 535,-55.5 341,-55.5\"/>\n","<text text-anchor=\"middle\" x=\"356\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"371,-55.5 371,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"411.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 0.5000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"452,-55.5 452,-91.5 \"/>\n","<text text-anchor=\"middle\" x=\"493.5\" y=\"-69.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282669728&#45;&gt;2657282601120* -->\n","<g id=\"edge19\" class=\"edge\">\n","<title>2657282669728&#45;&gt;2657282601120*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M535.06,-73.5C549.08,-73.5 562.74,-73.5 574.52,-73.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"574.77,-77 584.77,-73.5 574.77,-70 574.77,-77\"/>\n","</g>\n","<!-- 2657282669776 -->\n","<g id=\"node15\" class=\"node\">\n","<title>2657282669776</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"2,-165.5 2,-201.5 199,-201.5 199,-165.5 2,-165.5\"/>\n","<text text-anchor=\"middle\" x=\"18.5\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w1</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"35,-165.5 35,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"75.5\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 1.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"116,-165.5 116,-201.5 \"/>\n","<text text-anchor=\"middle\" x=\"157.5\" y=\"-179.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.0000</text>\n","</g>\n","<!-- 2657282669776&#45;&gt;2657282670112* -->\n","<g id=\"edge11\" class=\"edge\">\n","<title>2657282669776&#45;&gt;2657282670112*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M199.17,-183.5C208.89,-183.5 218.26,-183.5 226.69,-183.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"226.83,-187 236.83,-183.5 226.83,-180 226.83,-187\"/>\n","</g>\n","<!-- 2657282601696 -->\n","<g id=\"node16\" class=\"node\">\n","<title>2657282601696</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1769,-54.5 1769,-90.5 1987,-90.5 1987,-54.5 1769,-54.5\"/>\n","<text text-anchor=\"middle\" x=\"1793.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">out_0</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1818,-54.5 1818,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1861\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;0.7071</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1904,-54.5 1904,-90.5 \"/>\n","<text text-anchor=\"middle\" x=\"1945.5\" y=\"-68.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 1.0000</text>\n","</g>\n","<!-- 2657282601696tanh&#45;&gt;2657282601696 -->\n","<g id=\"edge6\" class=\"edge\">\n","<title>2657282601696tanh&#45;&gt;2657282601696</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1733.19,-72.5C1740.65,-72.5 1749.28,-72.5 1758.56,-72.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1758.84,-76 1768.84,-72.5 1758.84,-69 1758.84,-76\"/>\n","</g>\n","<!-- 2657282669824 -->\n","<g id=\"node18\" class=\"node\">\n","<title>2657282669824</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"339.5,-0.5 339.5,-36.5 536.5,-36.5 536.5,-0.5 339.5,-0.5\"/>\n","<text text-anchor=\"middle\" x=\"356\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">w2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"372.5,-0.5 372.5,-36.5 \"/>\n","<text text-anchor=\"middle\" x=\"413\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 1.0000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"453.5,-0.5 453.5,-36.5 \"/>\n","<text text-anchor=\"middle\" x=\"495\" y=\"-14.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.2500</text>\n","</g>\n","<!-- 2657282669824&#45;&gt;2657282601120* -->\n","<g id=\"edge20\" class=\"edge\">\n","<title>2657282669824&#45;&gt;2657282601120*</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M516.4,-36.52C527.42,-39.57 538.56,-42.92 549,-46.5 559.33,-50.04 570.34,-54.57 580.16,-58.9\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"578.98,-62.21 589.53,-63.13 581.86,-55.83 578.98,-62.21\"/>\n","</g>\n","<!-- 2657282669872 -->\n","<g id=\"node19\" class=\"node\">\n","<title>2657282669872</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1077.5,-27.5 1077.5,-63.5 1265.5,-63.5 1265.5,-27.5 1077.5,-27.5\"/>\n","<text text-anchor=\"middle\" x=\"1089.5\" y=\"-41.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1101.5,-27.5 1101.5,-63.5 \"/>\n","<text text-anchor=\"middle\" x=\"1142\" y=\"-41.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data 4.6186</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1182.5,-27.5 1182.5,-63.5 \"/>\n","<text text-anchor=\"middle\" x=\"1224\" y=\"-41.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282669872&#45;&gt;2657282601552+ -->\n","<g id=\"edge13\" class=\"edge\">\n","<title>2657282669872&#45;&gt;2657282601552+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1265.65,-57.22C1295.85,-61.03 1327.56,-65.02 1350.99,-67.97\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1350.78,-71.47 1361.14,-69.25 1351.66,-64.52 1350.78,-71.47\"/>\n","</g>\n","<!-- 2657282601408 -->\n","<g id=\"node20\" class=\"node\">\n","<title>2657282601408</title>\n","<polygon fill=\"none\" stroke=\"black\" points=\"1018,-82.5 1018,-118.5 1325,-118.5 1325,-82.5 1018,-82.5\"/>\n","<text text-anchor=\"middle\" x=\"1087\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x0w0 + x1w1 + x2w2</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1156,-82.5 1156,-118.5 \"/>\n","<text text-anchor=\"middle\" x=\"1199\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">data &#45;5.5000</text>\n","<polyline fill=\"none\" stroke=\"black\" points=\"1242,-82.5 1242,-118.5 \"/>\n","<text text-anchor=\"middle\" x=\"1283.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">grad 0.5000</text>\n","</g>\n","<!-- 2657282601408&#45;&gt;2657282601552+ -->\n","<g id=\"edge9\" class=\"edge\">\n","<title>2657282601408&#45;&gt;2657282601552+</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M1310.47,-82.49C1325.29,-80.56 1339.25,-78.73 1351.09,-77.19\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1351.87,-80.62 1361.34,-75.85 1350.97,-73.67 1351.87,-80.62\"/>\n","</g>\n","<!-- 2657282601408+&#45;&gt;2657282601408 -->\n","<g id=\"edge7\" class=\"edge\">\n","<title>2657282601408+&#45;&gt;2657282601408</title>\n","<path fill=\"none\" stroke=\"black\" d=\"M982.3,-100.5C989.69,-100.5 998.31,-100.5 1007.71,-100.5\"/>\n","<polygon fill=\"black\" stroke=\"black\" points=\"1007.77,-104 1017.77,-100.5 1007.77,-97 1007.77,-104\"/>\n","</g>\n","</g>\n","</svg>\n"],"text/plain":["<graphviz.graphs.Digraph at 0x26ab042a580>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# backward pass to calculate gradient\n","out_0.backward()\n","  \n","out_0_grad = w0.grad  # store w0.grad, further calculation with w0 will reset w0.grad to zero\n","print(f'w0.grad(i.e. d(output)/d(w0)): {w0.grad}')\n","draw_dot(out_0)"]},{"cell_type":"markdown","metadata":{},"source":["##### Check Backward Pass Gradient Calculation: d(output) / d(x0)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- w0.grad from backward pass is same as d(out)/d(w0) calculation ----\n","out_1: -0.7071082802, out_0: -0.7071067802, d(out): -0.0000015000, d(w0): 0.0000010000, d(out)/d(w0): -1.4999968224\n","d(out) / d(w0):             -1.4999968224\n","w0.grad from backward pass: -1.5000000043\n"]}],"source":["h = 0.000001\n","w0 += h  # increment x0 by h\n","\n","# x0*w0 + x1*w1 + x2*w2 + b\n","x0w0 = x0*w0; x0w0.label = 'x0*w0'\n","x1w1 = x1*w1; x1w1.label = 'x1*w1'\n","x2w2 = x2*w2; x2w2.label = 'x2*w2'\n","n_sum = x0w0 + x1w1 + x2w2; n_sum.label = 'x0w0 + x1w1 + x2w2'\n","n = n_sum + b; n.label = 'n'\n","out_1 = n.tanh(); out_1.label = 'out_1'\n","out_grad = (out_1 - out_0) / h \n","\n","print(f'---- w0.grad from backward pass is same as d(out)/d(w0) calculation ----')\n","print(f'out_1: {out_1.data:<12.10f}, out_0: {out_0.data:<12.10f}, d(out): {out_1.data-out_0.data:<12.10f}, d(w0): {h:<12.10f}, d(out)/d(w0): {(out_1.data-out_0.data)/h:<12.10f}')\n","print(f'd(out) / d(w0):             {out_grad.data:<12.10f}')\n","print(f'w0.grad from backward pass: {out_0_grad:<12.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- torch results matched backward pass results ----\n","x0.data.item()  = -3.000000\n","x0.grad.item()  =  1.000000\n","w0.data.item()  =  2.000000\n","w0.grad.item()  = -1.500000 <--\n","---\n","x1.data.item()  =  0.000000\n","x1.grad.item()  =  0.500000\n","w1.data.item()  =  1.000000\n","w1.grad.item()  =  0.000000\n","---\n","x2.data.item()  =  0.500000\n","x2.grad.item()  =  0.500000\n","w2.data.item()  =  1.000000\n","w2.grad.item()  =  0.250000\n","---\n","out.data.item() = -0.707107 <--\n"]}],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <--')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <--')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["len(n.parameters()): 41\n","i:  0,  -0.3945857432\n","i:  1,  -0.1409964153\n","i:  2,   0.6226040127\n","i:  3,  -0.3181809031\n","i:  4,  -0.4004778897\n","i:  5,  -0.8961484041\n","i:  6,   0.1601359721\n","i:  7,   0.7082223884\n","i:  8,  -0.0626700326\n","i:  9,  -0.3151682468\n","i: 10,   0.0002613487\n","i: 11,  -0.9951387663\n","i: 12,  -0.8348298522\n","i: 13,  -0.3751195466\n","i: 14,   0.1025825745\n","i: 15,  -0.5674827860\n","i: 16,   0.5996578973\n","i: 17,   0.5629428207\n","i: 18,   0.0957309016\n","i: 19,   0.1049703976\n","i: 20,  -0.8963125101\n","i: 21,   0.4700064321\n","i: 22,  -0.3382759187\n","i: 23,   0.2544191880\n","i: 24,   0.3684811965\n","i: 25,   0.2396385888\n","i: 26,  -0.3862843855\n","i: 27,  -0.3991567509\n","i: 28,   0.9792600523\n","i: 29,   0.1860334776\n","i: 30,   0.4284671868\n","i: 31,  -0.3388513662\n","i: 32,   0.9188561337\n","i: 33,  -0.9841785583\n","i: 34,   0.9587706546\n","i: 35,  -0.5827054053\n","i: 36,   0.1790763345\n","i: 37,  -0.9123536322\n","i: 38,  -0.6528384481\n","i: 39,  -0.9676572871\n","i: 40,  -0.3552176577\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'len(n.parameters()): {len(n.parameters())}')\n","# n.parameters()\n","for i, v in enumerate(n.parameters()):\n","  print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","neuron 0\n","w0: -0.9001122,   w0.grad:  0.0000000\n","w1:  0.8099169,   w1.grad:  0.0000000\n","w2: -0.2057760,   w2.grad:  0.0000000\n","b:   0.2440071\n","b_mat:  [0.24400713649289774]\n","neuron 1\n","w0: -0.5794287,   w0.grad:  0.0000000\n","w1: -0.0084990,   w1.grad:  0.0000000\n","w2: -0.1500628,   w2.grad:  0.0000000\n","b:   0.3811022\n","b_mat:  [0.24400713649289774, 0.38110215262010105]\n","neuron 2\n","w0: -0.7223584,   w0.grad:  0.0000000\n","w1:  0.3412714,   w1.grad:  0.0000000\n","w2: -0.7498457,   w2.grad:  0.0000000\n","b:  -0.4420290\n","b_mat:  [0.24400713649289774, 0.38110215262010105, -0.442028987634671]\n","neuron 3\n","w0: -0.5543253,   w0.grad:  0.0000000\n","w1:  0.5264123,   w1.grad:  0.0000000\n","w2: -0.5791775,   w2.grad:  0.0000000\n","b:  -0.9234647\n","b_mat:  [0.24400713649289774, 0.38110215262010105, -0.442028987634671, -0.9234647328431844]\n","----\n","layer: 1, neuron_cnt: 4\n","----\n","neuron 0\n","w0:  0.3069117,   w0.grad:  0.0000000\n","w1: -0.2057734,   w1.grad:  0.0000000\n","w2:  0.4019992,   w2.grad:  0.0000000\n","w3:  0.3206547,   w3.grad:  0.0000000\n","b:   0.5223908\n","b_mat:  [0.5223907842238833]\n","neuron 1\n","w0: -0.9723543,   w0.grad:  0.0000000\n","w1: -0.0313872,   w1.grad:  0.0000000\n","w2: -0.0709579,   w2.grad:  0.0000000\n","w3: -0.9292859,   w3.grad:  0.0000000\n","b:  -0.3464085\n","b_mat:  [0.5223907842238833, -0.3464085004612567]\n","neuron 2\n","w0: -0.4797412,   w0.grad:  0.0000000\n","w1:  0.5809441,   w1.grad:  0.0000000\n","w2: -0.3910262,   w2.grad:  0.0000000\n","w3:  0.7648752,   w3.grad:  0.0000000\n","b:  -0.2285902\n","b_mat:  [0.5223907842238833, -0.3464085004612567, -0.22859016862336845]\n","neuron 3\n","w0: -0.7335904,   w0.grad:  0.0000000\n","w1:  0.3004052,   w1.grad:  0.0000000\n","w2:  0.9674347,   w2.grad:  0.0000000\n","w3:  0.7544007,   w3.grad:  0.0000000\n","b:  -0.5960795\n","b_mat:  [0.5223907842238833, -0.3464085004612567, -0.22859016862336845, -0.5960794959660793]\n","----\n","layer: 2, neuron_cnt: 1\n","----\n","neuron 0\n","w0:  0.2191480,   w0.grad:  0.0000000\n","w1: -0.7404540,   w1.grad:  0.0000000\n","w2: -0.8571609,   w2.grad:  0.0000000\n","w3: -0.4163843,   w3.grad:  0.0000000\n","b:  -0.0853317\n","b_mat:  [-0.08533173526507465]\n","----\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}')\n","        print(f'b_mat:  {b_mat}')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('----')"]},{"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat:\n","[[-0.9001122   0.8099169  -0.20577603]\n"," [-0.57942866 -0.00849905 -0.15006276]\n"," [-0.72235838  0.34127138 -0.74984566]\n"," [-0.55432526  0.52641231 -0.57917754]]\n","b_mat:\n","[[ 0.24400714  0.38110215 -0.44202899 -0.92346473]]\n","\n","i: 1\n","w_mat:\n","[[ 0.30691166 -0.20577338  0.40199922  0.3206547 ]\n"," [-0.97235425 -0.0313872  -0.07095789 -0.92928594]\n"," [-0.47974118  0.5809441  -0.39102622  0.76487519]\n"," [-0.73359038  0.30040523  0.96743474  0.75440069]]\n","b_mat:\n","[[ 0.52239078 -0.3464085  -0.22859017 -0.5960795 ]]\n","\n","i: 2\n","w_mat:\n","[[ 0.21914799 -0.74045405 -0.85716092 -0.41638428]]\n","b_mat:\n","[[-0.08533174]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat:\\n{w_n_b[0]}')\n","  print(f'b_mat:\\n{w_n_b[1]}\\n')  \n","    "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[-0.9001122   0.8099169  -0.20577603]\n"," [-0.57942866 -0.00849905 -0.15006276]\n"," [-0.72235838  0.34127138 -0.74984566]\n"," [-0.55432526  0.52641231 -0.57917754]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","bias (4, 1):\n","[[ 0.24400714]\n"," [ 0.38110215]\n"," [-0.44202899]\n"," [-0.92346473]]\n","\n","output (4, 2):\n","[[ 0.79294288 -0.99763341]\n"," [-0.57381323 -0.89037161]\n"," [-0.11260635 -0.99741684]\n"," [ 0.12563193 -0.99778575]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[ 0.30691166 -0.20577338  0.40199922  0.3206547 ]\n"," [-0.97235425 -0.0313872  -0.07095789 -0.92928594]\n"," [-0.47974118  0.5809441  -0.39102622  0.76487519]\n"," [-0.73359038  0.30040523  0.96743474  0.75440069]]\n","\n","input (4, 2):\n","[[ 0.79294288 -0.99763341]\n"," [-0.57381323 -0.89037161]\n"," [-0.11260635 -0.99741684]\n"," [ 0.12563193 -0.99778575]]\n","\n","bias (4, 1):\n","[[ 0.52239078]\n"," [-0.3464085 ]\n"," [-0.22859017]\n"," [-0.5960795 ]]\n","\n","output (4, 2):\n","[[ 0.70584097 -0.31084927]\n"," [-0.83613186  0.92880185]\n"," [-0.66527947 -0.56517586]\n"," [-0.8773898  -0.95168597]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[ 0.21914799 -0.74045405 -0.85716092 -0.41638428]]\n","\n","input (4, 2):\n","[[ 0.70584097 -0.31084927]\n"," [-0.83613186  0.92880185]\n"," [-0.66527947 -0.56517586]\n"," [-0.8773898  -0.95168597]]\n","\n","bias (1, 1):\n","[[-0.08533174]]\n","\n","output (1, 2):\n","[[0.92520985 0.03950436]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [0.92520985 0.03950436]\n","desired output:     [1.0, -1.0]\n","loss:               1.0861628762962363\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","  output = np.tanh(np.matmul(weights, input) + bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')    \n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')"]},{"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [0.5662633528202957, 0.43686528404847547]\n","ys:                 [1.0, -1.0]\n","loss_data:          2.2527093236103974\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0     -0.3945857432     0.9296913937           0.05000        -0.4410703129\n","  1     -0.1409964153    -0.3124238009           0.05000        -0.1253752252\n","  2      0.6226040127     0.1558673546           0.05000         0.6148106450\n","  3     -0.3181809031     0.3096674340           0.05000        -0.3336642748\n","  4     -0.4004778897    -2.4280365719           0.05000        -0.2790760611\n","  5     -0.8961484041     0.8040084102           0.05000        -0.9363488246\n","  6      0.1601359721    -0.4027319933           0.05000         0.1802725718\n","  7      0.7082223884    -0.8098307161           0.05000         0.7487139242\n","  8     -0.0626700326     0.8438263414           0.05000        -0.1048613497\n","  9     -0.3151682468    -0.2122568470           0.05000        -0.3045554045\n"," 10      0.0002613487     0.1155400508           0.05000        -0.0055156538\n"," 11     -0.9951387663     0.2875498653           0.05000        -1.0095162595\n"," 12     -0.8348298522    -0.1624232138           0.05000        -0.8267086915\n"," 13     -0.3751195466     0.0599001051           0.05000        -0.3781145518\n"," 14      0.1025825745    -0.0291647298           0.05000         0.1040408109\n"," 15     -0.5674827860    -0.0536175227           0.05000        -0.5648019099\n"," 16      0.5996578973    -0.0816339177           0.05000         0.6037395931\n"," 17      0.5629428207     0.0541581288           0.05000         0.5602349143\n"," 18      0.0957309016    -0.0726810484           0.05000         0.0993649540\n"," 19      0.1049703976    -0.1044929663           0.05000         0.1101950459\n"," 20     -0.8963125101     0.1055890903           0.05000        -0.9015919646\n"," 21      0.4700064321     0.4863088206           0.05000         0.4456909911\n"," 22     -0.3382759187    -0.9385138323           0.05000        -0.2913502271\n"," 23      0.2544191880     0.3947531883           0.05000         0.2346815286\n"," 24      0.3684811965     0.7155689311           0.05000         0.3327027500\n"," 25      0.2396385888    -0.7262727589           0.05000         0.2759522267\n"," 26     -0.3862843855     0.7010658681           0.05000        -0.4213376789\n"," 27     -0.3991567509    -0.9972083717           0.05000        -0.3492963323\n"," 28      0.9792600523     0.5911570709           0.05000         0.9497021988\n"," 29      0.1860334776     0.9777993998           0.05000         0.1371435077\n"," 30      0.4284671868    -0.9908190305           0.05000         0.4780081383\n"," 31     -0.3388513662     1.5336440235           0.05000        -0.4155335674\n"," 32      0.9188561337    -1.1639835126           0.05000         0.9770553093\n"," 33     -0.9841785583     1.3563546226           0.05000        -1.0519962894\n"," 34      0.9587706546     1.9852389249           0.05000         0.8595087083\n"," 35     -0.5827054053    -2.0068246650           0.05000        -0.4823641720\n"," 36      0.1790763345    -1.4176523528           0.05000         0.2499589522\n"," 37     -0.9123536322    -1.3025506617           0.05000        -0.8472260992\n"," 38     -0.6528384481    -0.7563902227           0.05000        -0.6150189370\n"," 39     -0.9676572871     0.1043502734           0.05000        -0.9728748008\n"," 40     -0.3552176577     1.7359610688           0.05000        -0.4420157112\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.23354024105373464), Value(data = -0.36669129703610936)]\n","step: 0, loss: 0.9885404753337728\n","-------\n","ypred: [Value(data = 0.5038038630991538), Value(data = -0.41793670083479467)]\n","step: 1, loss: 0.5850082905104066\n","-------\n","ypred: [Value(data = 0.5495470888583313), Value(data = -0.5453746375986428)]\n","step: 2, loss: 0.4095920452945695\n","-------\n","ypred: [Value(data = 0.6116207046846927), Value(data = -0.5958094447101566)]\n","step: 3, loss: 0.31420848201512663\n","-------\n","ypred: [Value(data = 0.6512032682386646), Value(data = -0.6367279563507654)]\n","step: 4, loss: 0.2536257377844804\n","-------\n","ypred: [Value(data = 0.681129370484843), Value(data = -0.6681064945099735)]\n","step: 5, loss: 0.21183177735385075\n","-------\n","ypred: [Value(data = 0.7048017368388435), Value(data = -0.6930751305326899)]\n","step: 6, loss: 0.18134489007088878\n","-------\n","ypred: [Value(data = 0.7241254979279003), Value(data = -0.7135251681873724)]\n","step: 7, loss: 0.1581745701556022\n","-------\n","ypred: [Value(data = 0.74027908233464), Value(data = -0.7306553833069142)]\n","step: 8, loss: 0.14000147761448203\n","-------\n","ypred: [Value(data = 0.7540376400204087), Value(data = -0.7452646868062963)]\n","step: 9, loss: 0.12538756231462433\n","-------\n","ypred: [Value(data = 0.7659350187218844), Value(data = -0.7579079799806342)]\n","step: 10, loss: 0.11339496161778162\n","-------\n","ypred: [Value(data = 0.7763524953511017), Value(data = -0.7689839637048002)]\n","step: 11, loss: 0.10338661536122407\n","-------\n","ypred: [Value(data = 0.7855705857107471), Value(data = -0.7787872827594229)]\n","step: 12, loss: 0.09491503998139159\n","-------\n","ypred: [Value(data = 0.7938008481030507), Value(data = -0.7875409088197232)]\n","step: 13, loss: 0.08765695566817033\n","-------\n","ypred: [Value(data = 0.8012062214647283), Value(data = -0.7954170276737416)]\n","step: 14, loss: 0.08137315895017727\n","-------\n","ypred: [Value(data = 0.807914485111695), Value(data = -0.8025509542248767)]\n","step: 15, loss: 0.07588297070741201\n","-------\n","ypred: [Value(data = 0.8140274306371055), Value(data = -0.8090506645972003)]\n","step: 16, loss: 0.0710474452462075\n","-------\n","ypred: [Value(data = 0.8196272718483648), Value(data = -0.8150034862271376)]\n","step: 17, loss: 0.06675803116897655\n","-------\n","ypred: [Value(data = 0.8247812244429076), Value(data = -0.8204808944570284)]\n","step: 18, loss: 0.0629287285626753\n","-------\n","ypred: [Value(data = 0.8295448423394579), Value(data = -0.825542017083199)]\n","step: 19, loss: 0.059490548576479105\n","-------\n","ypred: [Value(data = 0.8339644906399186), Value(data = -0.8302362383677271)]\n","step: 20, loss: 0.05638752513200085\n","-------\n","ypred: [Value(data = 0.8380792072481182), Value(data = -0.8346051633775722)]\n","step: 21, loss: 0.05357379510675743\n","-------\n","ypred: [Value(data = 0.8419221239547866), Value(data = -0.8386841201355658)]\n","step: 22, loss: 0.05101142799140242\n","-------\n","ypred: [Value(data = 0.8455215650197783), Value(data = -0.8425033226340788)]\n","step: 23, loss: 0.04866879025524368\n","-------\n","ypred: [Value(data = 0.8489019062528096), Value(data = -0.8460887814905629)]\n","step: 24, loss: 0.04651929711709444\n","-------\n","ypred: [Value(data = 0.8520842538958838), Value(data = -0.849463024401788)]\n","step: 25, loss: 0.044540448967794036\n","-------\n","ypred: [Value(data = 0.8550869862976704), Value(data = -0.8526456715537544)]\n","step: 26, loss: 0.04271307965213559\n","-------\n","ypred: [Value(data = 0.8579261899613965), Value(data = -0.8556538992239344)]\n","step: 27, loss: 0.04102076430813928\n","-------\n","ypred: [Value(data = 0.8606160134519834), Value(data = -0.8585028163341001)]\n","step: 28, loss: 0.0394493486913991\n","-------\n","ypred: [Value(data = 0.8631689568258025), Value(data = -0.8612057726019487)]\n","step: 29, loss: 0.037986571935161056\n","-------\n","ypred: [Value(data = 0.8655961100069277), Value(data = -0.8637746124827105)]\n","step: 30, loss: 0.03662176184950558\n","-------\n","ypred: [Value(data = 0.8679073504112483), Value(data = -0.8662198858038072)]\n","step: 31, loss: 0.03534558702972313\n","-------\n","ypred: [Value(data = 0.8701115077953153), Value(data = -0.8685510235443132)]\n","step: 32, loss: 0.034149853818454125\n","-------\n","ypred: [Value(data = 0.8722165025591199), Value(data = -0.8707764853655244)]\n","step: 33, loss: 0.03302733895270994\n","-------\n","ypred: [Value(data = 0.8742294624057494), Value(data = -0.8729038840968216)]\n","step: 34, loss: 0.03197165080442095\n","-------\n","ypred: [Value(data = 0.8761568212456425), Value(data = -0.874940091306604)]\n","step: 35, loss: 0.030977113686384283\n","-------\n","ypred: [Value(data = 0.8780044034496526), Value(data = -0.8768913272580376)]\n","step: 36, loss: 0.03003867088196272\n","-------\n","ypred: [Value(data = 0.8797774959458042), Value(data = -0.8787632379033308)]\n","step: 37, loss: 0.02915180296474549\n","-------\n","ypred: [Value(data = 0.8814809101773181), Value(data = -0.8805609610637605)]\n","step: 38, loss: 0.02831245867440948\n","-------\n","ypred: [Value(data = 0.8831190355630282), Value(data = -0.882289183542932)]\n","step: 39, loss: 0.027516996158706234\n","-------\n","ypred: [Value(data = 0.8846958858024973), Value(data = -0.8839521906032289)]\n","step: 40, loss: 0.02676213281666006\n","-------\n","ypred: [Value(data = 0.8862151391294794), Value(data = -0.8855539089816665)]\n","step: 41, loss: 0.02604490231270041\n","-------\n","ypred: [Value(data = 0.8876801734256737), Value(data = -0.887097944417426)]\n","step: 42, loss: 0.025362617596457367\n","-------\n","ypred: [Value(data = 0.8890940969518687), Value(data = -0.8885876144985322)]\n","step: 43, loss: 0.024712838974049176\n","-------\n","ypred: [Value(data = 0.8904597753278489), Value(data = -0.8900259775012699)]\n","step: 44, loss: 0.024093346445776535\n","-------\n","ypred: [Value(data = 0.8917798552898537), Value(data = -0.8914158577866366)]\n","step: 45, loss: 0.023502115661296938\n","-------\n","ypred: [Value(data = 0.8930567856702604), Value(data = -0.8927598682284991)]\n","step: 46, loss: 0.022937296953545513\n","-------\n","ypred: [Value(data = 0.8942928359748974), Value(data = -0.8940604300742759)]\n","step: 47, loss: 0.022397197002277325\n","-------\n","ypred: [Value(data = 0.8954901128760913), Value(data = -0.8953197905778535)]\n","step: 48, loss: 0.021880262751316597\n","-------\n","ypred: [Value(data = 0.8966505748919639), Value(data = -0.8965400386936816)]\n","step: 49, loss: 0.02138506726366647\n","-------\n","ypred: [Value(data = 0.8977760454828118), Value(data = -0.8977231190786531)]\n","step: 50, loss: 0.020910297248131544\n","-------\n","ypred: [Value(data = 0.8988682247621781), Value(data = -0.8988708446129109)]\n","step: 51, loss: 0.020454742032059343\n","-------\n","ypred: [Value(data = 0.899928699992329), Value(data = -0.8999849076209435)]\n","step: 52, loss: 0.020017283788816485\n","-------\n","ypred: [Value(data = 0.9009589550103159), Value(data = -0.9010668899492039)]\n","step: 53, loss: 0.019596888856971563\n","-------\n","ypred: [Value(data = 0.9019603787109091), Value(data = -0.902118272035237)]\n","step: 54, loss: 0.019192600011876244\n","-------\n","ypred: [Value(data = 0.9029342726958036), Value(data = -0.9031404410852716)]\n","step: 55, loss: 0.018803529570248362\n","-------\n","ypred: [Value(data = 0.9038818581841237), Value(data = -0.904134698461862)]\n","step: 56, loss: 0.01842885322513503\n","-------\n","ypred: [Value(data = 0.9048042822669804), Value(data = -0.9051022663700685)]\n","step: 57, loss: 0.01806780452280217\n","-------\n","ypred: [Value(data = 0.905702623578332), Value(data = -0.9060442939194258)]\n","step: 58, loss: 0.01771966990510901\n","-------\n","ypred: [Value(data = 0.9065778974453746), Value(data = -0.9069618626293127)]\n","step: 59, loss: 0.017383784251133825\n","-------\n","ypred: [Value(data = 0.9074310605739236), Value(data = -0.9078559914370337)]\n","step: 60, loss: 0.017059526860520608\n","-------\n","ypred: [Value(data = 0.9082630153175446), Value(data = -0.908727641260755)]\n","step: 61, loss: 0.01674631782845449\n","-------\n","ypred: [Value(data = 0.9090746135733896), Value(data = -0.909577719163238)]\n","step: 62, loss: 0.016443614768550677\n","-------\n","ypred: [Value(data = 0.9098666603426688), Value(data = -0.9104070821569366)]\n","step: 63, loss: 0.016150909845417744\n","-------\n","ypred: [Value(data = 0.9106399169893059), Value(data = -0.9112165406863392)]\n","step: 64, loss: 0.015867727083378608\n","-------\n","ypred: [Value(data = 0.9113951042265218), Value(data = -0.9120068618193725)]\n","step: 65, loss: 0.015593619921903935\n","-------\n","ypred: [Value(data = 0.9121329048577598), Value(data = -0.9127787721761154)]\n","step: 66, loss: 0.015328168991841484\n","-------\n","ypred: [Value(data = 0.9128539662954496), Value(data = -0.9135329606199567)]\n","step: 67, loss: 0.015070980089584598\n","-------\n","ypred: [Value(data = 0.9135589028785609), Value(data = -0.9142700807336092)]\n","step: 68, loss: 0.01482168232897996\n","-------\n","ypred: [Value(data = 0.914248298007647), Value(data = -0.9149907530999742)]\n","step: 69, loss: 0.014579926453094864\n","-------\n","ypred: [Value(data = 0.9149227061141009), Value(data = -0.9156955674057564)]\n","step: 70, loss: 0.014345383289985008\n","-------\n","ypred: [Value(data = 0.9155826544786164), Value(data = -0.9163850843838494)]\n","step: 71, loss: 0.014117742338372652\n","-------\n","ypred: [Value(data = 0.916228644912289), Value(data = -0.917059837608875)]\n","step: 72, loss: 0.01389671047069756\n","-------\n","ypred: [Value(data = 0.9168611553124434), Value(data = -0.9177203351587981)]\n","step: 73, loss: 0.01368201074236217\n","-------\n","ypred: [Value(data = 0.9174806411040584), Value(data = -0.9183670611542424)]\n","step: 74, loss: 0.013473381297192426\n","-------\n","ypred: [Value(data = 0.9180875365765863), Value(data = -0.9190004771859918)]\n","step: 75, loss: 0.013270574360189136\n","-------\n","ypred: [Value(data = 0.918682256125014), Value(data = -0.919621023640133)]\n","step: 76, loss: 0.013073355309577888\n","-------\n","ypred: [Value(data = 0.9192651954031554), Value(data = -0.9202291209293892)]\n","step: 77, loss: 0.01288150182098868\n","-------\n","ypred: [Value(data = 0.9198367323964116), Value(data = -0.9208251706383812)]\n","step: 78, loss: 0.012694803077325977\n","-------\n","ypred: [Value(data = 0.920397228420551), Value(data = -0.9214095565898244)]\n","step: 79, loss: 0.012513059038537944\n","-------\n","ypred: [Value(data = 0.9209470290524595), Value(data = -0.9219826458380238)]\n","step: 80, loss: 0.012336079766067903\n","-------\n","ypred: [Value(data = 0.9214864649982611), Value(data = -0.9225447895954451)]\n","step: 81, loss: 0.012163684797283148\n","-------\n","ypred: [Value(data = 0.922015852903726), Value(data = -0.9230963240976129)]\n","step: 82, loss: 0.011995702565632696\n","-------\n","ypred: [Value(data = 0.9225354961114384), Value(data = -0.9236375714111269)]\n","step: 83, loss: 0.011831969862691725\n","-------\n","ypred: [Value(data = 0.9230456853688053), Value(data = -0.9241688401891544)]\n","step: 84, loss: 0.011672331338614925\n","-------\n","ypred: [Value(data = 0.9235466994906298), Value(data = -0.9246904263783815)]\n","step: 85, loss: 0.011516639037846037\n","-------\n","ypred: [Value(data = 0.9240388059796508), Value(data = -0.9252026138810593)]\n","step: 86, loss: 0.011364751967223037\n","-------\n","ypred: [Value(data = 0.92452226160816), Value(data = -0.9257056751754753)]\n","step: 87, loss: 0.011216535693879027\n","-------\n","ypred: [Value(data = 0.9249973129635467), Value(data = -0.926199871897892)]\n","step: 88, loss: 0.011071861970575712\n","-------\n","ypred: [Value(data = 0.9254641969603821), Value(data = -0.9266854553887466)]\n","step: 89, loss: 0.010930608386316185\n","-------\n","ypred: [Value(data = 0.9259231413214363), Value(data = -0.9271626672056725)]\n","step: 90, loss: 0.010792658040275516\n","-------\n","ypred: [Value(data = 0.926374365029832), Value(data = -0.9276317396056956)]\n","step: 91, loss: 0.010657899237258284\n","-------\n","ypred: [Value(data = 0.9268180787543547), Value(data = -0.9280928959987637)]\n","step: 92, loss: 0.010526225203048446\n","-------\n","ypred: [Value(data = 0.9272544852497818), Value(data = -0.9285463513746041)]\n","step: 93, loss: 0.010397533818155757\n","-------\n","ypred: [Value(data = 0.9276837797339426), Value(data = -0.9289923127047333)]\n","step: 94, loss: 0.010271727368591322\n","-------\n","ypred: [Value(data = 0.9281061502430917), Value(data = -0.9294309793213152)]\n","step: 95, loss: 0.010148712312417535\n","-------\n","ypred: [Value(data = 0.9285217779670503), Value(data = -0.9298625432744188)]\n","step: 96, loss: 0.010028399060924443\n","-------\n","ypred: [Value(data = 0.9289308375654625), Value(data = -0.9302871896691145)]\n","step: 97, loss: 0.009910701773376697\n","-------\n","ypred: [Value(data = 0.9293334974664093), Value(data = -0.9307050969837425)]\n","step: 98, loss: 0.009795538164362523\n","-------\n","ypred: [Value(data = 0.9297299201485307), Value(data = -0.9311164373705743)]\n","step: 99, loss: 0.009682829322853886\n","-------\n","ypred: [Value(data = 0.9301202624077216), Value(data = -0.9315213769400108)]\n","step: 100, loss: 0.00957249954215777\n","-------\n","ypred: [Value(data = 0.930504675609383), Value(data = -0.9319200760293681)]\n","step: 101, loss: 0.009464476160004107\n","-------\n","ypred: [Value(data = 0.9308833059271494), Value(data = -0.9323126894572267)]\n","step: 102, loss: 0.009358689408073852\n","-------\n","ypred: [Value(data = 0.9312562945689342), Value(data = -0.9326993667642516)]\n","step: 103, loss: 0.009255072270325869\n","-------\n","ypred: [Value(data = 0.9316237779910838), Value(data = -0.9330802524413239)]\n","step: 104, loss: 0.009153560349529542\n","-------\n","ypred: [Value(data = 0.9319858881013717), Value(data = -0.9334554861457642)]\n","step: 105, loss: 0.009054091741455703\n","-------\n","ypred: [Value(data = 0.9323427524515111), Value(data = -0.9338252029063754)]\n","step: 106, loss: 0.008956606916219896\n","-------\n","ypred: [Value(data = 0.9326944944198196), Value(data = -0.9341895333179805)]\n","step: 107, loss: 0.0088610486063089\n","-------\n","ypred: [Value(data = 0.9330412333846267), Value(data = -0.9345486037260843)]\n","step: 108, loss: 0.008767361700857172\n","-------\n","ypred: [Value(data = 0.9333830848889704), Value(data = -0.9349025364022449)]\n","step: 109, loss: 0.008675493145771183\n","-------\n","ypred: [Value(data = 0.9337201607970951), Value(data = -0.9352514497107007)]\n","step: 110, loss: 0.008585391849328848\n","-------\n","ypred: [Value(data = 0.9340525694432313), Value(data = -0.9355954582667676)]\n","step: 111, loss: 0.008497008592907498\n","-------\n","ypred: [Value(data = 0.9343804157730969), Value(data = -0.9359346730874719)]\n","step: 112, loss: 0.008410295946520721\n","-------\n","ypred: [Value(data = 0.9347038014785429), Value(data = -0.9362692017348757)]\n","step: 113, loss: 0.008325208188863502\n","-------\n","ypred: [Value(data = 0.9350228251257264), Value(data = -0.9365991484524996)]\n","step: 114, loss: 0.008241701231590114\n","-------\n","ypred: [Value(data = 0.935337582277179), Value(data = -0.9369246142952374)]\n","step: 115, loss: 0.008159732547565161\n","-------\n","ypred: [Value(data = 0.9356481656081098), Value(data = -0.9372456972531242)]\n","step: 116, loss: 0.0080792611028478\n","-------\n","ypred: [Value(data = 0.9359546650172603), Value(data = -0.9375624923692967)]\n","step: 117, loss: 0.008000247292185476\n","-------\n","ypred: [Value(data = 0.9362571677326114), Value(data = -0.9378750918524662)]\n","step: 118, loss: 0.007922652877807954\n","-------\n","ypred: [Value(data = 0.936555758412222), Value(data = -0.9381835851842045)]\n","step: 119, loss: 0.007846440931326831\n","-------\n","ypred: [Value(data = 0.9368505192404621), Value(data = -0.9384880592213168)]\n","step: 120, loss: 0.007771575778559466\n","-------\n","ypred: [Value(data = 0.9371415300198842), Value(data = -0.9387885982935718)]\n","step: 121, loss: 0.007698022947106846\n","-------\n","ypred: [Value(data = 0.9374288682589682), Value(data = -0.9390852842970319)]\n","step: 122, loss: 0.007625749116526989\n","-------\n","ypred: [Value(data = 0.9377126092559542), Value(data = -0.9393781967832133)]\n","step: 123, loss: 0.007554722070956246\n","-------\n","ypred: [Value(data = 0.9379928261789682), Value(data = -0.9396674130442991)]\n","step: 124, loss: 0.007484910654038862\n","-------\n","ypred: [Value(data = 0.9382695901426316), Value(data = -0.9399530081946061)]\n","step: 125, loss: 0.007416284726035728\n","-------\n","ypred: [Value(data = 0.9385429702813383), Value(data = -0.940235055248499)]\n","step: 126, loss: 0.007348815122990434\n","-------\n","ypred: [Value(data = 0.9388130338193648), Value(data = -0.9405136251949336)]\n","step: 127, loss: 0.007282473617839034\n","-------\n","ypred: [Value(data = 0.9390798461379771), Value(data = -0.9407887870687999)]\n","step: 128, loss: 0.007217232883356462\n","-------\n","ypred: [Value(data = 0.9393434708396838), Value(data = -0.9410606080192249)]\n","step: 129, loss: 0.007153066456839745\n","-------\n","ypred: [Value(data = 0.9396039698097762), Value(data = -0.9413291533749899)]\n","step: 130, loss: 0.007089948706433881\n","-------\n","ypred: [Value(data = 0.9398614032752929), Value(data = -0.9415944867071985)]\n","step: 131, loss: 0.00702785479901257\n","-------\n","ypred: [Value(data = 0.9401158298615315), Value(data = -0.9418566698893394)]\n","step: 132, loss: 0.006966760669530296\n","-------\n","ypred: [Value(data = 0.9403673066462295), Value(data = -0.9421157631548618)]\n","step: 133, loss: 0.006906642991768884\n","-------\n","ypred: [Value(data = 0.9406158892115258), Value(data = -0.9423718251523923)]\n","step: 134, loss: 0.006847479150404223\n","-------\n","ypred: [Value(data = 0.9408616316938102), Value(data = -0.9426249129986994)]\n","step: 135, loss: 0.006789247214325362\n","-------\n","ypred: [Value(data = 0.9411045868315598), Value(data = -0.9428750823295187)]\n","step: 136, loss: 0.006731925911140545\n","-------\n","ypred: [Value(data = 0.9413448060112596), Value(data = -0.9431223873483352)]\n","step: 137, loss: 0.006675494602809589\n","-------\n","ypred: [Value(data = 0.9415823393114967), Value(data = -0.9433668808732237)]\n","step: 138, loss: 0.006619933262344734\n","-------\n","ypred: [Value(data = 0.9418172355453108), Value(data = -0.9436086143818341)]\n","step: 139, loss: 0.006565222451526532\n","-------\n","ypred: [Value(data = 0.9420495423008868), Value(data = -0.9438476380546081)]\n","step: 140, loss: 0.006511343299583003\n","-------\n","ypred: [Value(data = 0.9422793059806598), Value(data = -0.9440840008163132)]\n","step: 141, loss: 0.006458277482784364\n","-------\n","ypred: [Value(data = 0.9425065718389095), Value(data = -0.9443177503759612)]\n","step: 142, loss: 0.00640600720490824\n","-------\n","ypred: [Value(data = 0.9427313840179107), Value(data = -0.9445489332651978)]\n","step: 143, loss: 0.006354515178531498\n","-------\n","ypred: [Value(data = 0.9429537855827037), Value(data = -0.9447775948752188)]\n","step: 144, loss: 0.0063037846071096\n","-------\n","ypred: [Value(data = 0.9431738185545497), Value(data = -0.9450037794922895)]\n","step: 145, loss: 0.0062537991678039495\n","-------\n","ypred: [Value(data = 0.9433915239431254), Value(data = -0.9452275303319218)]\n","step: 146, loss: 0.006204542995022294\n","-------\n","ypred: [Value(data = 0.9436069417775159), Value(data = -0.9454488895717744)]\n","step: 147, loss: 0.006156000664636941\n","-------\n","ypred: [Value(data = 0.9438201111360566), Value(data = -0.945667898383327)]\n","step: 148, loss: 0.006108157178849517\n","-------\n","ypred: [Value(data = 0.9440310701750743), Value(data = -0.9458845969623859)]\n","step: 149, loss: 0.006060997951670874\n","-------\n","ypred: [Value(data = 0.944239856156577), Value(data = -0.9460990245584681)]\n","step: 150, loss: 0.006014508794987852\n","-------\n","ypred: [Value(data = 0.9444465054749343), Value(data = -0.9463112195031156)]\n","step: 151, loss: 0.005968675905189146\n","-------\n","ypred: [Value(data = 0.944651053682594), Value(data = -0.9465212192371831)]\n","step: 152, loss: 0.0059234858503245166\n","-------\n","ypred: [Value(data = 0.9448535355148759), Value(data = -0.9467290603371435)]\n","step: 153, loss: 0.005878925557772742\n","-------\n","ypred: [Value(data = 0.9450539849138787), Value(data = -0.9469347785404534)]\n","step: 154, loss: 0.005834982302394995\n","-------\n","ypred: [Value(data = 0.9452524350515411), Value(data = -0.947138408770016)]\n","step: 155, loss: 0.005791643695151648\n","-------\n","ypred: [Value(data = 0.9454489183518897), Value(data = -0.9473399851577813)]\n","step: 156, loss: 0.005748897672161491\n","-------\n","ypred: [Value(data = 0.9456434665125077), Value(data = -0.9475395410675183)]\n","step: 157, loss: 0.00570673248418347\n","-------\n","ypred: [Value(data = 0.9458361105252565), Value(data = -0.9477371091167922)]\n","step: 158, loss: 0.005665136686502313\n","-------\n","ypred: [Value(data = 0.9460268806962822), Value(data = -0.9479327211981816)]\n","step: 159, loss: 0.005624099129199645\n","-------\n","ypred: [Value(data = 0.9462158066653321), Value(data = -0.9481264084997634)]\n","step: 160, loss: 0.005583608947794355\n","-------\n","ypred: [Value(data = 0.9464029174244147), Value(data = -0.948318201524898)]\n","step: 161, loss: 0.005543655554235163\n","-------\n","ypred: [Value(data = 0.946588241335824), Value(data = -0.9485081301113413)]\n","step: 162, loss: 0.005504228628230742\n","-------\n","ypred: [Value(data = 0.9467718061495558), Value(data = -0.9486962234497093)]\n","step: 163, loss: 0.005465318108902625\n","-------\n","ypred: [Value(data = 0.9469536390201422), Value(data = -0.9488825101013246)]\n","step: 164, loss: 0.005426914186746561\n","-------\n","ypred: [Value(data = 0.9471337665229221), Value(data = -0.9490670180154643)]\n","step: 165, loss: 0.005389007295889942\n","-------\n","ypred: [Value(data = 0.9473122146697753), Value(data = -0.9492497745460368)]\n","step: 166, loss: 0.005351588106631936\n","-------\n","ypred: [Value(data = 0.9474890089243362), Value(data = -0.9494308064677073)]\n","step: 167, loss: 0.005314647518254921\n","-------\n","ypred: [Value(data = 0.9476641742167107), Value(data = -0.9496101399914952)]\n","step: 168, loss: 0.005278176652095519\n","-------\n","ypred: [Value(data = 0.9478377349577141), Value(data = -0.9497878007798615)]\n","step: 169, loss: 0.005242166844864567\n","-------\n","ypred: [Value(data = 0.9480097150526479), Value(data = -0.9499638139613081)]\n","step: 170, loss: 0.005206609642205453\n","-------\n","ypred: [Value(data = 0.948180137914633), Value(data = -0.9501382041445052)]\n","step: 171, loss: 0.0051714967924814965\n","-------\n","ypred: [Value(data = 0.9483490264775174), Value(data = -0.9503109954319665)]\n","step: 172, loss: 0.005136820240782252\n","-------\n","ypred: [Value(data = 0.9485164032083729), Value(data = -0.9504822114332891)]\n","step: 173, loss: 0.005102572123140312\n","-------\n","ypred: [Value(data = 0.9486822901195968), Value(data = -0.9506518752779727)]\n","step: 174, loss: 0.005068744760949994\n","-------\n","ypred: [Value(data = 0.9488467087806325), Value(data = -0.9508200096278345)]\n","step: 175, loss: 0.00503533065557971\n","-------\n","ypred: [Value(data = 0.9490096803293243), Value(data = -0.9509866366890379)]\n","step: 176, loss: 0.005002322483170058\n","-------\n","ypred: [Value(data = 0.9491712254829198), Value(data = -0.9511517782237422)]\n","step: 177, loss: 0.004969713089610653\n","-------\n","ypred: [Value(data = 0.9493313645487303), Value(data = -0.9513154555613949)]\n","step: 178, loss: 0.004937495485688179\n","-------\n","ypred: [Value(data = 0.9494901174344655), Value(data = -0.9514776896096738)]\n","step: 179, loss: 0.004905662842399237\n","-------\n","ypred: [Value(data = 0.9496475036582506), Value(data = -0.9516385008650955)]\n","step: 180, loss: 0.004874208486421259\n","-------\n","ypred: [Value(data = 0.9498035423583396), Value(data = -0.9517979094232989)]\n","step: 181, loss: 0.004843125895735501\n","-------\n","ypred: [Value(data = 0.9499582523025353), Value(data = -0.9519559349890171)]\n","step: 182, loss: 0.004812408695396267\n","-------\n","ypred: [Value(data = 0.9501116518973246), Value(data = -0.9521125968857516)]\n","step: 183, loss: 0.004782050653440248\n","-------\n","ypred: [Value(data = 0.9502637591967418), Value(data = -0.9522679140651514)]\n","step: 184, loss: 0.004752045676931457\n","-------\n","ypred: [Value(data = 0.9504145919109674), Value(data = -0.952421905116117)]\n","step: 185, loss: 0.0047223878081356675\n","-------\n","ypred: [Value(data = 0.9505641674146729), Value(data = -0.9525745882736318)]\n","step: 186, loss: 0.004693071220820031\n","-------\n","ypred: [Value(data = 0.9507125027551195), Value(data = -0.9527259814273331)]\n","step: 187, loss: 0.004664090216672953\n","-------\n","ypred: [Value(data = 0.95085961466002), Value(data = -0.9528761021298325)]\n","step: 188, loss: 0.004635439221839694\n","-------\n","ypred: [Value(data = 0.9510055195451712), Value(data = -0.9530249676047926)]\n","step: 189, loss: 0.004607112783569385\n","-------\n","ypred: [Value(data = 0.951150233521866), Value(data = -0.9531725947547688)]\n","step: 190, loss: 0.004579105566969331\n","-------\n","ypred: [Value(data = 0.9512937724040905), Value(data = -0.9533190001688256)]\n","step: 191, loss: 0.004551412351862634\n","-------\n","ypred: [Value(data = 0.9514361517155161), Value(data = -0.953464200129933)]\n","step: 192, loss: 0.004524028029745293\n","-------\n","ypred: [Value(data = 0.9515773866962911), Value(data = -0.9536082106221512)]\n","step: 193, loss: 0.004496947600839213\n","-------\n","ypred: [Value(data = 0.9517174923096398), Value(data = -0.9537510473376122)]\n","step: 194, loss: 0.004470166171237485\n","-------\n","ypred: [Value(data = 0.9518564832482755), Value(data = -0.9538927256833022)]\n","step: 195, loss: 0.004443678950138795\n","-------\n","ypred: [Value(data = 0.9519943739406332), Value(data = -0.9540332607876535)]\n","step: 196, loss: 0.004417481247167627\n","-------\n","ypred: [Value(data = 0.9521311785569274), Value(data = -0.9541726675069522)]\n","step: 197, loss: 0.004391568469777122\n","-------\n","ypred: [Value(data = 0.9522669110150423), Value(data = -0.954310960431567)]\n","step: 198, loss: 0.004365936120731717\n","-------\n","ypred: [Value(data = 0.9524015849862589), Value(data = -0.9544481538920045)]\n","step: 199, loss: 0.004340579795666832\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["Text(0, 0.5, 'Loss')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","# Create a list of iterations\n","iterations = range(len(losses))\n","\n","# Plot the loss as a function of iteration\n","plt.plot(iterations, losses)\n","\n","# Add a title to the plot\n","plt.title('Loss vs. Iteration')\n","\n","# Add labels to the x-axis and y-axis\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# TODO build same model with pyTorch "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5],\n","  [5.0, -3.0, 1.5]  \n","]\n","\n","# desired targets\n","ys = [1.0, -1.0, -.5]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
