{"cells":[{"cell_type":"markdown","metadata":{},"source":["### [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[],"source":["import math, random, torch\n","import numpy as np\n","# import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[],"source":["def plot_losses(losses):\n","  # import matplotlib.pyplot as plt\n","  \n","  # Create a list of iterations\n","  iterations = range(len(losses))\n","\n","  # Plot the loss as a function of iteration\n","  plt.plot(iterations, losses)\n","\n","  # Add a title to the plot\n","  plt.title('Loss vs. Iteration')\n","\n","  # Add labels to the x-axis and y-axis\n","  plt.xlabel('Iteration')\n","  plt.ylabel('Loss')"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[],"source":["def print_parameters(parameters):\n","  # number of parameters (e.g sum (weights + bias to each neuron and output))\n","  # MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","  # print(f'Number of parameters in MLP(2, [3, 3, 1]): {len(parameters())}\\n')\n","  print(f'Total parameters: {len(parameters())}\\n')  \n","\n","  # print first 5 parameters\n","  for i, v in enumerate(parameters()):\n","    if i < 5:\n","      print(f'i: {i:>2}, {v.data:>14.10f}')\n","  \n","  print('---')\n","\n","  # print last 5 parameters   \n","  for i, v in enumerate(parameters()):\n","    if i >= len(parameters()) - 5:\n","      print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[],"source":["def get_wt_n_b_mats(layers, verbose=False):\n","  ''' Get neuron's weights and bias for each layer.\n","  Inputs: If n = MLP(2, [3, 3, 1]), input is n.layers.\n","\n","  return: two lists of np.arrays. The first list is weight matrix for each layer\n","          The second list is the bias matrix for each layer \n","  '''\n","  layer_cnt = len(layers)  # number of layers\n","  w_mats = []  # list of weights matrix for each layer \n","  b_mats = []  # list of bias matrix for each layer\n","  if verbose:\n","    print(f'layer_cnt: {layer_cnt}\\n')\n","  for i, layer in enumerate(layers):\n","      neuron_cnt = len(layer.neurons)  # numbers of neurons in the layer\n","      if verbose: \n","        print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","        print('----')\n","      b_mat = []  # accumulate neuon's bias for each row     \n","      for j, neuron in enumerate(layer.neurons):\n","          if verbose:\n","            print(f'layer: {i}, neuron {j}')\n","          b = neuron.b.data  # bias of neuron \n","          w_row = []  # accumulate neuon's weights for each row\n","          b_row = []  # accumulate neuon's bias for each row\n","          for k, w in enumerate(neuron.w):\n","              w_row.append(w.data)\n","              if verbose:\n","                print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","          if j == 0:            \n","              w_mat = np.array([w_row])\n","          else:\n","              w_mat = np.vstack((w_mat, w_row))\n","          \n","          b_mat.append(b)\n","          if verbose:\n","            print(f'b:  {b:10.7f}\\n')\n","            print(f'b:  {b:10.7f}')        \n","            print(f'b_mat:  {b_mat}\\n')\n","      w_mats.append(w_mat)  \n","      b_mats.append(np.array([b_mat]))        \n","      if verbose:\n","          print('------')\n","\n","  zipped_w_n_b = zip(w_mats, b_mats)\n","  if verbose:\n","    for i, w_n_b in enumerate(zipped_w_n_b):\n","      print(f'layer: {i}')  # 1st layer is 0    \n","      print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","      print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","\n","  return w_mats, b_mats"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions<br>* limited to neural network with one output, e.g. MLP(2, [3, 1])<br>* neural network with multiple outputs, e.g.  MLP(2, [3, 3]), will produce errors in backward pass "]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[],"source":["class Neuron:\n","    \n","    def __init__(self, nin):\n","        # random numbers evenly distributed between -1 and 1    \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]  \n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["#   &nbsp;\n","# -- Human Brain and Artificial Neural Network -- "]},{"cell_type":"markdown","metadata":{},"source":["### Neurons in Human Brain\n","![](..\\karpathy\\img\\neuron_of_human_brain.png)"]},{"cell_type":"markdown","metadata":{},"source":["### Simple Artificial Neural Network<br>* input layer: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 nodes<br>* hidden layer 1: &nbsp;3 nodes<br>* hidden layer 2:&nbsp;&nbsp;3 nodes<br>*  output layer: &nbsp;&nbsp;&nbsp; 1 node<br>* node's bias and activation function are not shown\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\MLP (2, [3, 3, 1]).png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Artificial Neuron Function\n","\n","<img src=\"..\\karpathy\\img\\Artificial Neuron Function.png\">"]},{"cell_type":"markdown","metadata":{},"source":["#   &nbsp;\n","# -- Maths in Artificial Neural Network --"]},{"cell_type":"markdown","metadata":{},"source":["### Hidden Layer Matrix Operations<br>* Hidden layer with two inputs (X1, X2), and three neurons (b1, b2, b3)<br>* Two sets of inputs (X1, X2) are shown in different shades of gray<br>* Two sets of outputs (Y1, Y2, Y3) are shown in corresponding shades of gray<br>* Multiple sets of inputs are processed in one matrix operation \n","\n","<img src=\"..\\karpathy\\img\\Hidden Layer Matrix Operations.png\">"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[],"source":["# verbose = True   # print calculation output and weights and bias matrices \n","verbose = False  # print calculation output only"]},{"cell_type":"markdown","metadata":{},"source":["#   &nbsp;\n","# -- How Artificial Neural Network Learns --"]},{"cell_type":"markdown","metadata":{},"source":["##### ---- Create Simple Neural Network MLP(2, [3, 3, 1]) ----<br>* 2 input nodes<br>* 3 neurons in hidden layer 1<br>* 3 neurons in hidden layer 2<br>* 1 output node\n","##### ---- Parameters ----<br>* initialize neuron parameters with random numbers<br>* parameters in layer 1: 3 neurons * (2 inputs + 1 bias) = &nbsp;&nbsp;&nbsp;&nbsp;  9<br>* parameters in layer 2: 3 neurons * (3 neurons + 1 bias) = 12<br>* parameters in layer 3: 1 output * (3 neurons + 1 bias) = &nbsp;&nbsp;&nbsp; 4<br>*  total parameters: 25\n","##### ---- Inputs ----<br>* 1st set: [2.0, 3.0]<br>* 2nd set: [3.0, -1.0]\n","##### ---- Desired Output ----<br>* [1.0, -1.0] for all inputs\n","##### ---- Learning Rate ----<br>* 0.05"]},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(2, [3, 3, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0],\n","  [3.0, -1.0]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"markdown","metadata":{},"source":["##### ---- Initialize Neural Network Parameters ----"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initalize neuron weights and bias with random numbers\n","Total parameters: 25\n","\n","i:  0,  -0.6474679893\n","i:  1,   0.4823423121\n","i:  2,  -0.5713372895\n","i:  3,   0.5009876398\n","i:  4,   0.0172884853\n","---\n","i: 20,  -0.3399812553\n","i: 21,   0.4706668407\n","i: 22,   0.2657449318\n","i: 23,   0.1806394417\n","i: 24,  -0.0249901679\n"]}],"source":["# if verbose:\n","if True:\n","  print(\"Initalize neuron weights and bias with random numbers\")\n","  print_parameters(n.parameters)"]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[],"source":["# if True:\n","if verbose:\n","\t# print weights and bias of each layer\n","\tfor i, layer in enumerate(n.layers):\n","\t\tneuron_cnt = len(layer.neurons)  # numbers of neurons in the layer \n","\t\tprint(f'layer: {i}, neuron_cnt: {neuron_cnt}, layer: {layer}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 2)\n","xs_mats:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n","\n","xs_mats_T[0].shape: (2, 2)\n","xs_mats_T:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### ---- Start: Manual Calculation of Neural Network Output and Prediction Error ----<br>* calculate neural network output, (a.k.a) Forward Pass<br>* calculate prediction error, (a.k.a) Loss"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[],"source":["def forward_pass(layers, verbose=verbose):\n","  # Get Neural Network's Weights and Biases Matrices\n","  # w_mats, b_mats = get_wt_n_b_mats(n.layers, verbose=verbose)\n","  w_mats, b_mats = get_wt_n_b_mats(layers, verbose=verbose)\n","\n","  # Calculate Neural Network Output and Loss with Matrix Multiplication\n","  for layer in range(len(layers)):\n","    if layer == 0:  # first layer, use given inputs xs as inputs\n","      input = xs_mats_T[layer]\n","    else:  # after first layer, use outputs from preceding layers as inputs\n","      input = output\n","\n","    weights = w_mats[layer]\n","    bias = np.transpose(b_mats[layer])\n","\n","    weights_x_input = np.matmul(weights, input)\n","    weights_x_input_plus_bias = weights_x_input + bias\n","\n","    # output = np.tanh(np.matmul(weights, input) + bias)\n","    output = np.tanh(weights_x_input_plus_bias)\n","\n","    print(f'{\"-\"*50}')\n","    print(f'Calculate Output of Layer: {layer}')    \n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_+_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')    \n","\n","    # print(f'output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n","    print(f'Layer {layer} Output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n","\n","  yout = output[0]\n","  err_sq = ((yout - ys)**2)\n","  loss_sum = err_sq.sum()\n","  loss_mean = err_sq.mean()\n","\n","  print(f'-- Manual calculation results of neural network output and prediction error --')\n","  print(f'yout:           {yout}')   \n","  print(f'desired output: {ys}')   \n","  print(f'err_sq:         {err_sq}')\n","  print(f'loss_mean:      {loss_mean}')\n","  print(f'loss_sum:       {loss_sum}')\n","\n","  return yout, err_sq, loss_sum, loss_mean, w_mats, b_mats\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Manual calculation results of neural network output and prediction error "]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","Calculate Output of Layer: 0\n","weights (3, 2):\n","[[-0.64746799  0.48234231]\n"," [ 0.50098764  0.01728849]\n"," [-0.94314767  0.55695818]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[ 0.15209096 -2.42474628]\n"," [ 1.05384074  1.48567443]\n"," [-0.2154208  -3.38640118]]\n","\n","bias (3, 1):\n","[[-0.57133729]\n"," [ 0.05480635]\n"," [-0.42091217]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[-0.41924633 -2.99608357]\n"," [ 1.10864709  1.54048079]\n"," [-0.63633297 -3.80731335]]\n","\n","Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[-0.39629532 -0.99501596]\n"," [ 0.80358364  0.91220112]\n"," [-0.56239754 -0.99901412]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 1\n","weights (3, 3):\n","[[-0.80413864 -0.98756385  0.35988607]\n"," [ 0.08120237  0.7352709  -0.09870426]\n"," [ 0.5220309   0.18492608 -0.23336591]]\n","\n","input (3, 2):\n","[[-0.39629532 -0.99501596]\n"," [ 0.80358364  0.91220112]\n"," [-0.56239754 -0.99901412]]\n","\n","weights_x_inputs (3, 2):\n","[[-0.67731281 -0.46025733]\n"," [ 0.61418258  0.68852423]\n"," [ 0.07296958 -0.11760347]]\n","\n","bias (3, 1):\n","[[-0.93955421]\n"," [-0.00573073]\n"," [-0.33998126]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[-1.61686702 -1.39981154]\n"," [ 0.60845185  0.6827935 ]\n"," [-0.26701167 -0.45758472]]\n","\n","Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[-0.9241684  -0.8853109 ]\n"," [ 0.5430364   0.59333246]\n"," [-0.26084201 -0.42811365]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 2\n","weights (1, 3):\n","[[0.47066684 0.26574493 0.18063944]]\n","\n","input (3, 2):\n","[[-0.9241684  -0.8853109 ]\n"," [ 0.5430364   0.59333246]\n"," [-0.26084201 -0.42811365]]\n","\n","weights_x_inputs (1, 2):\n","[[-0.3377846 -0.3363456]]\n","\n","bias (1, 1):\n","[[-0.02499017]]\n","\n","weights_x_inputs_+_bias (1, 2):\n","[[-0.36277477 -0.36133577]]\n","\n","Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n","[[-0.34765578 -0.34639007]]\n","\n","-- Manual calculation results of neural network output and prediction error --\n","yout:           [-0.34765578 -0.34639007]\n","desired output: [1.0, -1.0]\n","err_sq:         [1.81617611 0.42720594]\n","loss_mean:      1.1216910246429408\n","loss_sum:       2.2433820492858816\n"]}],"source":["yout, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- manual calculation results of neural network output and prediction error --\n","yout:           [-0.34765578 -0.34639007] <-- neural network output\n","desired output: [1.0, -1.0]\n","err_sq:         [1.81617611 0.42720594]\n","loss_mean:      1.1216910246429408\n","loss_sum:       2.2433820492858816 <-- sum(prediction_error)^2\n"]}],"source":["print(f'-- manual calculation results of neural network output and prediction error --')\n","print(f'yout:           {yout} <-- neural network output')   \n","print(f'desired output: {ys}')   \n","print(f'err_sq:         {err_sq}')\n","print(f'loss_mean:      {loss_mean}')\n","print(f'loss_sum:       {loss_sum} <-- sum(prediction_error)^2')\n"]},{"cell_type":"markdown","metadata":{},"source":["##### ---- End: Manual Calculation of Neural Network Output and Prediction Error ----"]},{"cell_type":"markdown","metadata":{},"source":["##### How Neural Network Learns<br>* calculate changes in prediction errors (loss) w.r.t. changes in each parameter (a.k.a. gradients)<br>* adjust parameters in direction of less loss with gradients<br>* repeat the process "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate gradient for parameter W0<br>* increase W0 by small amount, e.g. 0.00001<br>* recalculate output and loss<br>* calculate changes in loss w.r.t. changes in W0"]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss_sum_before:   2.2433820 <-- loss_sum before increase W1\n","W0_before:        -0.6474680\n","W0_after:         -0.6474580\n","W0_dif:            0.0000100 <-- increased W1 by a small amount\n"]}],"source":["# Increase W1 by h\n","h = .00001\n","loss_sum_before = loss_sum\n","print(f'loss_sum_before:  {loss_sum_before:10.7f} <-- loss_sum before increase W1')\n","W0_before = n.parameters()[0].data  # W1\n","print(f'W0_before:        {W0_before:10.7f}')\n","n.parameters()[0].data += h\n","W0_after = n.parameters()[0].data\n","print(f'W0_after:         {W0_after:10.7f}') \n","W0_dif = W0_after - W0_before\n","print(f'W0_dif:           {W0_dif:10.7f} <-- increased W1 by a small amount') "]},{"cell_type":"markdown","metadata":{},"source":["##### Recalculate output and Loss with small changes in W0"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","Calculate Output of Layer: 0\n","weights (3, 2):\n","[[-0.64745799  0.48234231]\n"," [ 0.50098764  0.01728849]\n"," [-0.94314767  0.55695818]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[ 0.15211096 -2.42471628]\n"," [ 1.05384074  1.48567443]\n"," [-0.2154208  -3.38640118]]\n","\n","bias (3, 1):\n","[[-0.57133729]\n"," [ 0.05480635]\n"," [-0.42091217]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[-0.41922633 -2.99605357]\n"," [ 1.10864709  1.54048079]\n"," [-0.63633297 -3.80731335]]\n","\n","Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[-0.39627846 -0.99501566]\n"," [ 0.80358364  0.91220112]\n"," [-0.56239754 -0.99901412]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 1\n","weights (3, 3):\n","[[-0.80413864 -0.98756385  0.35988607]\n"," [ 0.08120237  0.7352709  -0.09870426]\n"," [ 0.5220309   0.18492608 -0.23336591]]\n","\n","input (3, 2):\n","[[-0.39627846 -0.99501566]\n"," [ 0.80358364  0.91220112]\n"," [-0.56239754 -0.99901412]]\n","\n","weights_x_inputs (3, 2):\n","[[-0.67732637 -0.46025757]\n"," [ 0.61418395  0.68852426]\n"," [ 0.07297838 -0.11760331]]\n","\n","bias (3, 1):\n","[[-0.93955421]\n"," [-0.00573073]\n"," [-0.33998126]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[-1.61688058 -1.39981178]\n"," [ 0.60845322  0.68279353]\n"," [-0.26700287 -0.45758457]]\n","\n","Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[-0.92417038 -0.88531096]\n"," [ 0.54303736  0.59333248]\n"," [-0.26083381 -0.42811352]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 2\n","weights (1, 3):\n","[[0.47066684 0.26574493 0.18063944]]\n","\n","input (3, 2):\n","[[-0.92417038 -0.88531096]\n"," [ 0.54303736  0.59333248]\n"," [-0.26083381 -0.42811352]]\n","\n","weights_x_inputs (1, 2):\n","[[-0.3377838 -0.3363456]]\n","\n","bias (1, 1):\n","[[-0.02499017]]\n","\n","weights_x_inputs_+_bias (1, 2):\n","[[-0.36277397 -0.36133577]]\n","\n","Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n","[[-0.34765507 -0.34639007]]\n","\n","-- Manual calculation results of neural network output and prediction error --\n","yout:           [-0.34765507 -0.34639007]\n","desired output: [1.0, -1.0]\n","err_sq:         [1.8161742  0.42720594]\n","loss_mean:      1.1216900699550878\n","loss_sum:       2.2433801399101756\n"]}],"source":["yout, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- recaluclate neural network output and loss with small change in W1 --\n","yout:             [-0.34765507 -0.34639007]\n","desired output:   [1.0, -1.0]\n","err_sq:           [1.8161742  0.42720594]\n","loss_sum_before:  2.2433820492858816\n","loss_sum_after:   2.2433801399101756\n","\n","loss_sum_dif:     -1.909375705988481e-06 <-- change in loss_sum\n","W0_dif:           9.99999999995449e-06 <-- change in W0\n","W0_grad:          -0.19093757059971705 <-- changes in loss_sum w.r.t. changes in W0, manual calculation\n"]}],"source":["loss_sum_after = loss_sum\n","loss_sum_dif = loss_sum_after - loss_sum_before\n","W0_grad = loss_sum_dif / W0_dif\n","\n","# print(f'-- manual forward pass calculation --')\n","print(f'-- recaluclate neural network output and loss with small change in W1 --')\n","print(f'yout:             {yout}')   \n","print(f'desired output:   {ys}')   \n","print(f'err_sq:           {err_sq}')\n","print(f'loss_sum_before:  {loss_sum_before}')\n","print(f'loss_sum_after:   {loss_sum_after}\\n')\n","print(f'loss_sum_dif:     {loss_sum_dif} <-- change in loss_sum')\n","print(f'W0_dif:           {W0_dif} <-- change in W0')\n","print(f'W0_grad:          {W0_grad} <-- changes in loss_sum w.r.t. changes in W0, manual calculation')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate output and Loss with Micrograd<br>* change W1 back to initial value<br>* compare manual calculation vs Micrograd "]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- Calculate neural network loss and gradient using Micrograd --\n","ypred_data:         [-0.3476557845463746, -0.3463900737713836]\n","ys:                 [1.0, -1.0]\n","err_sq:             [1.8161761136213044, 0.4272059356645773]\n","loss_mean:          1.1216910246429408\n","loss_sum:           2.2433820492858816 <-- loss_sum, Micrograd calculation same as manual calc.\n","W0_before:          -0.6474679893320889\n"]}],"source":["# change W1 back before Micrograd calculation\n","n.parameters()[0].data = W0_before\n","\n","ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","# loss.backward()\n","err_sq_ = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","err_sq = [l.data for l in err_sq_]\n","loss_sum = sum(err_sq)\n","loss_len = len(err_sq)\n","loss_mean = loss_sum / loss_len\n","\n","# print(f'-- Micrograd forward pass and backward pass --')\n","print(f'-- Calculate neural network loss and gradient using Micrograd --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'err_sq:             {err_sq}')\n","print(f'loss_mean:          {loss_mean}')\n","print(f'loss_sum:           {loss_sum} <-- loss_sum, Micrograd calculation same as manual calc.')\n","print(f'W0_before:          {n.parameters()[0].data}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate gradients and adjust parameters"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== adjust parameters ===\n","  i  parameter before         gradient     learning rate   parameter adjusted\n","  0     -0.6474679893    -0.1909325368           0.05000        -0.6379213625 <-- gradient same as manual calc. W0_grad -0.1909375706\n","  1      0.4823423121    -0.2869751607           0.05000         0.4966910701\n","  2     -0.5713372895    -0.0955186643           0.05000        -0.5665613563\n","  3      0.5009876398    -0.1387731805           0.05000         0.5079262989\n","  4      0.0172884853    -0.2648680595           0.05000         0.0305318883\n","  5      0.0548063510    -0.0745418892           0.05000         0.0585334455\n","  6     -0.9431476670     0.1070398570           0.05000        -0.9484996599\n","  7      0.5569581781     0.1607443230           0.05000         0.5489209620\n","  8     -0.4209121659     0.0535367046           0.05000        -0.4235890011\n","  9     -0.8041386412    -0.0519998013           0.05000        -0.8015386511\n"," 10     -0.9875638493    -0.0239742308           0.05000        -0.9863651378\n"," 11      0.3598860661    -0.0254378103           0.05000         0.3611579566\n"," 12     -0.9395542090    -0.0456584826           0.05000        -0.9372712848\n"," 13      0.0812023740    -0.0211396224           0.05000         0.0822593551\n"," 14      0.7352709033    -0.1761025784           0.05000         0.7440760322\n"," 15     -0.0987042603     0.0518185710           0.05000        -0.1012951888\n"," 16     -0.0057307331    -0.2459208214           0.05000         0.0065653080\n"," 17      0.5220309005    -0.0107840102           0.05000         0.5225701010\n"," 18      0.1849260776    -0.1657425334           0.05000         0.1932132043\n"," 19     -0.2333659095     0.0547973333           0.05000        -0.2361057762\n"," 20     -0.3399812553    -0.2291941619           0.05000        -0.3285215472\n"," 21      0.4706668407     1.1714210729           0.05000         0.4120957871\n"," 22      0.2657449318    -0.6041957621           0.05000         0.2959547199\n"," 23      0.1806394417     0.1255867940           0.05000         0.1743601020\n"," 24     -0.0249901679    -1.2191723015           0.05000         0.0359684471\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","print('=== adjust parameters ===')\n","print(f'  i  parameter before         gradient     learning rate   parameter adjusted')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","\n","  if i == 0:  \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f} <-- gradient same as manual calc. W0_grad {W0_grad:13.10f}')\n","  else:\n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = -0.212105784570171), Value(data = -0.21710232819303488)]\n","step: 0, loss: 2.082129197509236\n","-------\n","ypred: [Value(data = -0.1091509225091515), Value(data = -0.1289426911462331)]\n","step: 1, loss: 1.9889566042104687\n","-------\n","ypred: [Value(data = -0.03727481949129607), Value(data = -0.08261369126579785)]\n","step: 2, loss: 1.9175366906036655\n","-------\n","ypred: [Value(data = 0.01921925078059775), Value(data = -0.0669934806161847)]\n","step: 3, loss: 1.8324320432520738\n","-------\n","ypred: [Value(data = 0.07481806757242354), Value(data = -0.07400792210554612)]\n","step: 4, loss: 1.7134229364137132\n","-------\n","ypred: [Value(data = 0.13578391606859394), Value(data = -0.10105335500459842)]\n","step: 5, loss: 1.5549745102742234\n","-------\n","ypred: [Value(data = 0.1993146774360382), Value(data = -0.14729765630357658)]\n","step: 6, loss: 1.368198272714729\n","-------\n","ypred: [Value(data = 0.2614881476775563), Value(data = -0.2086139566569741)]\n","step: 7, loss: 1.1716916256188563\n","-------\n","ypred: [Value(data = 0.32193693694442826), Value(data = -0.27729954619059916)]\n","step: 8, loss: 0.9820654634166182\n","-------\n","ypred: [Value(data = 0.38050579656087036), Value(data = -0.34604881176907926)]\n","step: 9, loss: 0.8114252246833149\n","-------\n","ypred: [Value(data = 0.4357434402321784), Value(data = -0.41018972905190104)]\n","step: 10, loss: 0.6662616209568871\n","-------\n","ypred: [Value(data = 0.48606459737774027), Value(data = -0.4675183819389892)]\n","step: 11, loss: 0.5476662716413765\n","-------\n","ypred: [Value(data = 0.5306458838982391), Value(data = -0.5174743219966771)]\n","step: 12, loss: 0.45312431623423177\n","-------\n","ypred: [Value(data = 0.5694638060091244), Value(data = -0.5604202054415149)]\n","step: 13, loss: 0.3785918101202288\n","-------\n","ypred: [Value(data = 0.6029853825438037), Value(data = -0.5971397024774974)]\n","step: 14, loss: 0.3199170257938092\n","-------\n","ypred: [Value(data = 0.6318780104796373), Value(data = -0.628532942728515)]\n","step: 15, loss: 0.27350157380636675\n","-------\n","ypred: [Value(data = 0.6568328568103965), Value(data = -0.6554596198010139)]\n","step: 16, loss: 0.23647176175257575\n","-------\n","ypred: [Value(data = 0.6784815027737061), Value(data = -0.6786745428272762)]\n","step: 17, loss: 0.20662419348591426\n","-------\n","ypred: [Value(data = 0.697367416028442), Value(data = -0.6988125390803945)]\n","step: 18, loss: 0.18230036749650097\n","-------\n","ypred: [Value(data = 0.713944259156013), Value(data = -0.7163958593156129)]\n","step: 19, loss: 0.1622591954831319\n","-------\n","ypred: [Value(data = 0.7285851898979836), Value(data = -0.7318497049310804)]\n","step: 20, loss: 0.14557057988826227\n","-------\n","ypred: [Value(data = 0.7415953307386626), Value(data = -0.7455190546766083)]\n","step: 21, loss: 0.1315335246287483\n","-------\n","ypred: [Value(data = 0.7532239738883241), Value(data = -0.7576839793008637)]\n","step: 22, loss: 0.1196154609509348\n","-------\n","ypred: [Value(data = 0.7636752532071487), Value(data = -0.7685725667784756)]\n","step: 23, loss: 0.1094080427942084\n","-------\n","ypred: [Value(data = 0.7731169981947126), Value(data = -0.7783714280045387)]\n","step: 24, loss: 0.10059512043292541\n","-------\n","ypred: [Value(data = 0.781687887524943), Value(data = -0.7872340722987666)]\n","step: 25, loss: 0.09292951844388841\n","-------\n","ypred: [Value(data = 0.7895031528686682), Value(data = -0.7952875225083303)]\n","step: 26, loss: 0.08621612109300861\n","-------\n","ypred: [Value(data = 0.7966591008053762), Value(data = -0.8026375214946805)]\n","step: 27, loss: 0.08029946920704088\n","-------\n","ypred: [Value(data = 0.8032366931009753), Value(data = -0.8093726302808455)]\n","step: 28, loss: 0.07505459302788298\n","-------\n","ypred: [Value(data = 0.8093043854765065), Value(data = -0.8155674612477193)]\n","step: 29, loss: 0.07038017874910435\n","-------\n","ypred: [Value(data = 0.8149203854487116), Value(data = -0.8212852383896885)]\n","step: 30, loss: 0.06619342973988396\n","-------\n","ypred: [Value(data = 0.8201344555857014), Value(data = -0.8265798339561471)]\n","step: 31, loss: 0.06242616805812953\n","-------\n","ypred: [Value(data = 0.8249893605864624), Value(data = -0.8314973967992979)]\n","step: 32, loss: 0.059021851193348525\n","-------\n","ypred: [Value(data = 0.8295220344942871), Value(data = -0.8360776612580416)]\n","step: 33, loss: 0.055933269861600396\n","-------\n","ypred: [Value(data = 0.8337645271601847), Value(data = -0.8403550050183893)]\n","step: 34, loss: 0.053120756852955475\n","-------\n","ypred: [Value(data = 0.8377447758089086), Value(data = -0.8443593087894616)]\n","step: 35, loss: 0.050550782537795484\n","-------\n","ypred: [Value(data = 0.8414872373674198), Value(data = -0.8481166587210129)]\n","step: 36, loss: 0.048194845275481964\n","-------\n","ypred: [Value(data = 0.8450134093882813), Value(data = -0.8516499233880425)]\n","step: 37, loss: 0.04602858850021817\n","-------\n","ypred: [Value(data = 0.8483422613822316), Value(data = -0.8549792302023587)]\n","step: 38, loss: 0.04403109335535585\n","-------\n","ypred: [Value(data = 0.8514905937351549), Value(data = -0.8581223607627433)]\n","step: 39, loss: 0.04218430826467398\n","-------\n","ypred: [Value(data = 0.8544733377966248), Value(data = -0.8610950805334849)]\n","step: 40, loss: 0.0404725860640543\n","-------\n","ypred: [Value(data = 0.8573038079410543), Value(data = -0.8639114150507514)]\n","step: 41, loss: 0.038882306181612356\n","-------\n","ypred: [Value(data = 0.8599939142296686), Value(data = -0.8665838823758106)]\n","step: 42, loss: 0.03740156449464091\n","-------\n","ypred: [Value(data = 0.8625543425993347), Value(data = -0.869123689576431)]\n","step: 43, loss: 0.03601991736838744\n","-------\n","ypred: [Value(data = 0.8649947081645888), Value(data = -0.8715408994968875)]\n","step: 44, loss: 0.0347281693256333\n","-------\n","ypred: [Value(data = 0.8673236861598599), Value(data = -0.8738445728774595)]\n","step: 45, loss: 0.033518196046677974\n","-------\n","ypred: [Value(data = 0.8695491242073007), Value(data = -0.8760428899337215)]\n","step: 46, loss: 0.03238279613106575\n","-------\n","ypred: [Value(data = 0.8716781389239422), Value(data = -0.8781432547490501)]\n","step: 47, loss: 0.031315566393177986\n","-------\n","ypred: [Value(data = 0.8737171993433469), Value(data = -0.8801523852283459)]\n","step: 48, loss: 0.03031079650814278\n","-------\n","ypred: [Value(data = 0.8756721991932026), Value(data = -0.8820763908744707)]\n","step: 49, loss: 0.02936337964264532\n","-------\n","ypred: [Value(data = 0.8775485197194695), Value(data = -0.8839208402562504)]\n","step: 50, loss: 0.028468736349708097\n","-------\n","ypred: [Value(data = 0.879351084462686), Value(data = -0.8856908197186045)]\n","step: 51, loss: 0.02762274951693451\n","-------\n","ypred: [Value(data = 0.8810844071595632), Value(data = -0.8873909846262673)]\n","step: 52, loss: 0.02682170856403412\n","-------\n","ypred: [Value(data = 0.8827526337525923), Value(data = -0.8890256042208093)]\n","step: 53, loss: 0.026062261410510212\n","-------\n","ypred: [Value(data = 0.8843595793338325), Value(data = -0.8905986009968854)]\n","step: 54, loss: 0.025341372995686878\n","-------\n","ypred: [Value(data = 0.8859087607198399), Value(data = -0.8921135853604966)]\n","step: 55, loss: 0.024656289344249602\n","-------\n","ypred: [Value(data = 0.8874034252476547), Value(data = -0.8935738862137094)]\n","step: 56, loss: 0.024004506341612965\n","-------\n","ypred: [Value(data = 0.888846576292781), Value(data = -0.8949825780120504)]\n","step: 57, loss: 0.023383742522831645\n","-------\n","ypred: [Value(data = 0.8902409959358578), Value(data = -0.896342504759034)]\n","step: 58, loss: 0.022791915292783273\n","-------\n","ypred: [Value(data = 0.8915892651425811), Value(data = -0.8976563013339434)]\n","step: 59, loss: 0.022227120088974192\n","-------\n","ypred: [Value(data = 0.8928937817692599), Value(data = -0.8989264124917055)]\n","step: 60, loss: 0.02168761207548779\n","-------\n","ypred: [Value(data = 0.8941567766624348), Value(data = -0.9001551098255435)]\n","step: 61, loss: 0.021171790020435\n","-------\n","ypred: [Value(data = 0.8953803280838408), Value(data = -0.9013445069424478)]\n","step: 62, loss: 0.02067818206227351\n","-------\n","ypred: [Value(data = 0.896566374660531), Value(data = -0.9024965730671421)]\n","step: 63, loss: 0.020205433114516787\n","-------\n","ypred: [Value(data = 0.8977167270332178), Value(data = -0.9036131452610431)]\n","step: 64, loss: 0.019752293695266057\n","-------\n","ypred: [Value(data = 0.8988330783530974), Value(data = -0.9046959394179026)]\n","step: 65, loss: 0.019317609998946612\n","-------\n","ypred: [Value(data = 0.8999170137579384), Value(data = -0.9057465601766389)]\n","step: 66, loss: 0.01890031505366464\n","-------\n","ypred: [Value(data = 0.9009700189415242), Value(data = -0.9067665098737513)]\n","step: 67, loss: 0.018499420829563393\n","-------\n","ypred: [Value(data = 0.9019934879162051), Value(data = -0.9077571966421817)]\n","step: 68, loss: 0.018114011182140163\n","-------\n","ypred: [Value(data = 0.9029887300559677), Value(data = -0.9087199417501169)]\n","step: 69, loss: 0.01774323553025595\n","-------\n","ypred: [Value(data = 0.9039569764967758), Value(data = -0.9096559862617281)]\n","step: 70, loss: 0.017386303181981956\n","-------\n","ypred: [Value(data = 0.9048993859617204), Value(data = -0.910566497091884)]\n","step: 71, loss: 0.017042478232873824\n","-------\n","ypred: [Value(data = 0.9058170500705257), Value(data = -0.9114525725182804)]\n","step: 72, loss: 0.016711074971048255\n","-------\n","ypred: [Value(data = 0.906710998186007), Value(data = -0.912315247206937)]\n","step: 73, loss: 0.016391453731831762\n","-------\n","ypred: [Value(data = 0.9075822018440394), Value(data = -0.9131554968005338)]\n","step: 74, loss: 0.016083017151957982\n","-------\n","ypred: [Value(data = 0.9084315788083106), Value(data = -0.9139742421133821)]\n","step: 75, loss: 0.015785206779505637\n","-------\n","ypred: [Value(data = 0.9092599967865216), Value(data = -0.9147723529718923)]\n","step: 76, loss: 0.015497500001129788\n","-------\n","ypred: [Value(data = 0.9100682768406588), Value(data = -0.9155506517350638)]\n","step: 77, loss: 0.015219407252780866\n","-------\n","ypred: [Value(data = 0.9108571965204093), Value(data = -0.9163099165257429)]\n","step: 78, loss: 0.014950469484129047\n","-------\n","ypred: [Value(data = 0.9116274927456759), Value(data = -0.9170508842000519)]\n","step: 79, loss: 0.01469025585040877\n","-------\n","ypred: [Value(data = 0.9123798644613995), Value(data = -0.9177742530794707)]\n","step: 80, loss: 0.014438361608441655\n","-------\n","ypred: [Value(data = 0.9131149750854706), Value(data = -0.9184806854674672)]\n","step: 81, loss: 0.014194406196252413\n","-------\n","ypred: [Value(data = 0.9138334547683692), Value(data = -0.9191708099702929)]\n","step: 82, loss: 0.013958031478013184\n","-------\n","ypred: [Value(data = 0.9145359024812764), Value(data = -0.9198452236395406)]\n","step: 83, loss: 0.013728900138085169\n","-------\n","ypred: [Value(data = 0.9152228879477129), Value(data = -0.9205044939522808)]\n","step: 84, loss: 0.013506694209708996\n","-------\n","ypred: [Value(data = 0.9158949534322727), Value(data = -0.9211491606430032)]\n","step: 85, loss: 0.013291113725462492\n","-------\n","ypred: [Value(data = 0.916552615398681), Value(data = -0.9217797374001825)]\n","step: 86, loss: 0.013081875477984855\n","-------\n","ypred: [Value(data = 0.9171963660482291), Value(data = -0.9223967134390362)]\n","step: 87, loss: 0.012878711880681938\n","-------\n","ypred: [Value(data = 0.9178266747485747), Value(data = -0.9230005549609215)]\n","step: 88, loss: 0.012681369919202603\n","-------\n","ypred: [Value(data = 0.9184439893619581), Value(data = -0.923591706508825)]\n","step: 89, loss: 0.012489610185425925\n","-------\n","ypred: [Value(data = 0.9190487374810328), Value(data = -0.9241705922275038)]\n","step: 90, loss: 0.012303205986542262\n","-------\n","ypred: [Value(data = 0.9196413275797566), Value(data = -0.9247376170360407)]\n","step: 91, loss: 0.012121942522557655\n","-------\n","ypred: [Value(data = 0.9202221500861082), Value(data = -0.9252931677198606)]\n","step: 92, loss: 0.011945616126216328\n","-------\n","ypred: [Value(data = 0.9207915783827839), Value(data = -0.9258376139486144)]\n","step: 93, loss: 0.011774033559925415\n","-------\n","ypred: [Value(data = 0.9213499697414789), Value(data = -0.9263713092257576)]\n","step: 94, loss: 0.011607011364795295\n","-------\n","ypred: [Value(data = 0.9218976661958648), Value(data = -0.9268945917751334)]\n","step: 95, loss: 0.01144437525737695\n","-------\n","ypred: [Value(data = 0.9224349953579298), Value(data = -0.9274077853694046)]\n","step: 96, loss: 0.011285959570098796\n","-------\n","ypred: [Value(data = 0.9229622711819414), Value(data = -0.9279112001047498)]\n","step: 97, loss: 0.01113160673178216\n","-------\n","ypred: [Value(data = 0.923479794679933), Value(data = -0.9284051331258708)]\n","step: 98, loss: 0.010981166784949488\n","-------\n","ypred: [Value(data = 0.9239878545922828), Value(data = -0.928889869304999)]\n","step: 99, loss: 0.01083449693694407\n","-------\n","ypred: [Value(data = 0.9244867280166569), Value(data = -0.929365681878291)]\n","step: 100, loss: 0.010691461142149136\n","-------\n","ypred: [Value(data = 0.9249766809983165), Value(data = -0.9298328330427121)]\n","step: 101, loss: 0.01055192971284027\n","-------\n","ypred: [Value(data = 0.9254579690845474), Value(data = -0.9302915745162527)]\n","step: 102, loss: 0.010415778956423451\n","-------\n","ypred: [Value(data = 0.9259308378457403), Value(data = -0.9307421480640942)]\n","step: 103, loss: 0.010282890837009877\n","-------\n","ypred: [Value(data = 0.9263955233654558), Value(data = -0.9311847859931226)]\n","step: 104, loss: 0.010153152659457494\n","-------\n","ypred: [Value(data = 0.9268522527016163), Value(data = -0.9316197116170024)]\n","step: 105, loss: 0.010026456774170113\n","-------\n","ypred: [Value(data = 0.9273012443208002), Value(data = -0.9320471396938443)]\n","step: 106, loss: 0.009902700301091891\n","-------\n","ypred: [Value(data = 0.9277427085074648), Value(data = -0.9324672768383401)]\n","step: 107, loss: 0.009781784871466604\n","-------\n","ypred: [Value(data = 0.9281768477497767), Value(data = -0.9328803219100991)]\n","step: 108, loss: 0.009663616386050681\n","-------\n","ypred: [Value(data = 0.928603857103606), Value(data = -0.9332864663797773)]\n","step: 109, loss: 0.009548104788578888\n","-------\n","ypred: [Value(data = 0.92902392453612), Value(data = -0.9336858946744775)]\n","step: 110, loss: 0.009435163853378876\n","-------\n","ypred: [Value(data = 0.9294372312503064), Value(data = -0.9340787845037811)]\n","step: 111, loss: 0.00932471098612166\n","-------\n","ypred: [Value(data = 0.9298439519916568), Value(data = -0.9344653071676786)]\n","step: 112, loss: 0.00921666703677567\n","-------\n","ypred: [Value(data = 0.9302442553381506), Value(data = -0.9348456278475598)]\n","step: 113, loss: 0.009110956123907814\n","-------\n","ypred: [Value(data = 0.9306383039745985), Value(data = -0.9352199058813534)]\n","step: 114, loss: 0.009007505469540916\n","-------\n","ypred: [Value(data = 0.9310262549523249), Value(data = -0.9355882950238148)]\n","step: 115, loss: 0.008906245243840802\n","-------\n","ypred: [Value(data = 0.9314082599351027), Value(data = -0.9359509436928982)]\n","step: 116, loss: 0.008807108418960735\n","-------\n","ypred: [Value(data = 0.9317844654321861), Value(data = -0.9363079952030751)]\n","step: 117, loss: 0.008710030631424127\n","-------\n","ypred: [Value(data = 0.9321550130192304), Value(data = -0.9366595879864035)]\n","step: 118, loss: 0.008614950052472956\n","-------\n","ypred: [Value(data = 0.9325200395478308), Value(data = -0.9370058558020997)]\n","step: 119, loss: 0.008521807265852173\n","-------\n","ypred: [Value(data = 0.9328796773443621), Value(data = -0.937346927935303)]\n","step: 120, loss: 0.008430545152541054\n","-------\n","ypred: [Value(data = 0.9332340543987578), Value(data = -0.9376829293856849)]\n","step: 121, loss: 0.008341108781977566\n","-------\n","ypred: [Value(data = 0.9335832945438185), Value(data = -0.9380139810465064)]\n","step: 122, loss: 0.008253445309356042\n","-------\n","ypred: [Value(data = 0.9339275176256047), Value(data = -0.9383401998746852)]\n","step: 123, loss: 0.008167503878608554\n","-------\n","ypred: [Value(data = 0.934266839665429), Value(data = -0.9386616990523984)]\n","step: 124, loss: 0.008083235530708964\n","-------\n","ypred: [Value(data = 0.9346013730139325), Value(data = -0.9389785881407121)]\n","step: 125, loss: 0.008000593116963634\n","-------\n","ypred: [Value(data = 0.9349312264976924), Value(data = -0.9392909732256945)]\n","step: 126, loss: 0.007919531216977938\n","-------\n","ypred: [Value(data = 0.9352565055587859), Value(data = -0.9395989570574419)]\n","step: 127, loss: 0.007840006061008266\n","-------\n","ypred: [Value(data = 0.9355773123877031), Value(data = -0.9399026391824156)]\n","step: 128, loss: 0.00776197545643052\n","-------\n","ypred: [Value(data = 0.9358937460499783), Value(data = -0.9402021160694694)]\n","step: 129, loss: 0.0076853987180738855\n","-------\n","ypred: [Value(data = 0.9362059026068862), Value(data = -0.9404974812299106)]\n","step: 130, loss: 0.00761023660218693\n","-------\n","ypred: [Value(data = 0.936513875230527), Value(data = -0.9407888253319314)]\n","step: 131, loss: 0.007536451243817621\n","-------\n","ypred: [Value(data = 0.9368177543136044), Value(data = -0.9410762363097123)]\n","step: 132, loss: 0.00746400609740493\n","-------\n","ypred: [Value(data = 0.9371176275741815), Value(data = -0.9413597994674884)]\n","step: 133, loss: 0.007392865880392516\n","-------\n","ypred: [Value(data = 0.9374135801556837), Value(data = -0.9416395975788531)]\n","step: 134, loss: 0.007322996519687247\n","-------\n","ypred: [Value(data = 0.9377056947223982), Value(data = -0.9419157109815446)]\n","step: 135, loss: 0.007254365100798511\n","-------\n","ypred: [Value(data = 0.9379940515507081), Value(data = -0.9421882176679656)]\n","step: 136, loss: 0.0071869398195027676\n","-------\n","ypred: [Value(data = 0.938278728616283), Value(data = -0.9424571933716506)]\n","step: 137, loss: 0.007120689935890054\n","-------\n","ypred: [Value(data = 0.9385598016774341), Value(data = -0.9427227116498974)]\n","step: 138, loss: 0.007055585730657024\n","-------\n","ypred: [Value(data = 0.9388373443548318), Value(data = -0.9429848439627603)]\n","step: 139, loss: 0.0069915984635202195\n","-------\n","ypred: [Value(data = 0.9391114282077706), Value(data = -0.9432436597485907)]\n","step: 140, loss: 0.006928700333631217\n","-------\n","ypred: [Value(data = 0.939382122807156), Value(data = -0.9434992264963028)]\n","step: 141, loss: 0.006866864441882802\n","-------\n","ypred: [Value(data = 0.9396494958053772), Value(data = -0.9437516098145258)]\n","step: 142, loss: 0.006806064755002543\n","-------\n","ypred: [Value(data = 0.9399136130032196), Value(data = -0.944000873497807)]\n","step: 143, loss: 0.006746276071335468\n","-------\n","ypred: [Value(data = 0.9401745384139675), Value(data = -0.944247079590003)]\n","step: 144, loss: 0.006687473988225315\n","-------\n","ypred: [Value(data = 0.9404323343248301), Value(data = -0.944490288445005)]\n","step: 145, loss: 0.006629634870907557\n","-------\n","ypred: [Value(data = 0.940687061355823), Value(data = -0.9447305587849275)]\n","step: 146, loss: 0.006572735822834255\n","-------\n","ypred: [Value(data = 0.94093877851623), Value(data = -0.9449679477558806)]\n","step: 147, loss: 0.006516754657354429\n","-------\n","ypred: [Value(data = 0.9411875432587573), Value(data = -0.945202510981449)]\n","step: 148, loss: 0.006461669870678757\n","-------\n","ypred: [Value(data = 0.9414334115314946), Value(data = -0.9454343026139823)]\n","step: 149, loss: 0.0064074606160617244\n","-------\n","ypred: [Value(data = 0.9416764378277839), Value(data = -0.9456633753838035)]\n","step: 150, loss: 0.006354106679137809\n","-------\n","ypred: [Value(data = 0.9419166752340945), Value(data = -0.9458897806464347)]\n","step: 151, loss: 0.0063015884543526045\n","-------\n","ypred: [Value(data = 0.9421541754759993), Value(data = -0.9461135684279323)]\n","step: 152, loss: 0.006249886922432616\n","-------\n","ypred: [Value(data = 0.9423889889623381), Value(data = -0.94633478746842)]\n","step: 153, loss: 0.006198983628841245\n","-------\n","ypred: [Value(data = 0.9426211648276526), Value(data = -0.946553485263904)]\n","step: 154, loss: 0.006148860663171137\n","-------\n","ypred: [Value(data = 0.9428507509729713), Value(data = -0.9467697081064488)]\n","step: 155, loss: 0.0060995006394260025\n","-------\n","ypred: [Value(data = 0.943077794105021), Value(data = -0.946983501122789)]\n","step: 156, loss: 0.006050886677147701\n","-------\n","ypred: [Value(data = 0.9433023397739326), Value(data = -0.9471949083114469)]\n","step: 157, loss: 0.006003002383347078\n","-------\n","ypred: [Value(data = 0.9435244324095124), Value(data = -0.9474039725784281)]\n","step: 158, loss: 0.005955831835198469\n","-------\n","ypred: [Value(data = 0.9437441153561402), Value(data = -0.9476107357715503)]\n","step: 159, loss: 0.005909359563461577\n","-------\n","ypred: [Value(data = 0.9439614309063553), Value(data = -0.9478152387134752)]\n","step: 160, loss: 0.00586357053659477\n","-------\n","ypred: [Value(data = 0.9441764203331895), Value(data = -0.9480175212334941)]\n","step: 161, loss: 0.005818450145526972\n","-------\n","ypred: [Value(data = 0.944389123921299), Value(data = -0.9482176221981281)]\n","step: 162, loss: 0.0057739841890564305\n","-------\n","ypred: [Value(data = 0.9445995809969507), Value(data = -0.9484155795405873)]\n","step: 163, loss: 0.005730158859846897\n","-------\n","ypred: [Value(data = 0.9448078299569089), Value(data = -0.9486114302891453)]\n","step: 164, loss: 0.005686960730992853\n","-------\n","ypred: [Value(data = 0.9450139082962725), Value(data = -0.948805210594473)]\n","step: 165, loss: 0.00564437674312699\n","-------\n","ypred: [Value(data = 0.9452178526353044), Value(data = -0.9489969557559743)]\n","step: 166, loss: 0.005602394192045262\n","-------\n","ypred: [Value(data = 0.9454196987452979), Value(data = -0.9491867002471709)]\n","step: 167, loss: 0.005561000716824903\n","-------\n","ypred: [Value(data = 0.9456194815735198), Value(data = -0.9493744777401714)]\n","step: 168, loss: 0.0055201842884131545\n","-------\n","ypred: [Value(data = 0.9458172352672689), Value(data = -0.9495603211292672)]\n","step: 169, loss: 0.005479933198665132\n","-------\n","ypred: [Value(data = 0.946012993197085), Value(data = -0.949744262553689)]\n","step: 170, loss: 0.0054402360498105335\n","-------\n","ypred: [Value(data = 0.9462067879791469), Value(data = -0.9499263334195607)]\n","step: 171, loss: 0.005401081744329458\n","-------\n","ypred: [Value(data = 0.9463986514968886), Value(data = -0.9501065644210811)]\n","step: 172, loss: 0.005362459475219731\n","-------\n","ypred: [Value(data = 0.9465886149218686), Value(data = -0.9502849855609697)]\n","step: 173, loss: 0.005324358716637421\n","-------\n","ypred: [Value(data = 0.9467767087339208), Value(data = -0.950461626170201)]\n","step: 174, loss: 0.0052867692148948105\n","-------\n","ypred: [Value(data = 0.946962962740617), Value(data = -0.9506365149270607)]\n","step: 175, loss: 0.005249680979799484\n","-------\n","ypred: [Value(data = 0.9471474060960668), Value(data = -0.9508096798755478)]\n","step: 176, loss: 0.005213084276320163\n","-------\n","ypred: [Value(data = 0.9473300673190843), Value(data = -0.9509811484431517)]\n","step: 177, loss: 0.005176969616564516\n","-------\n","ypred: [Value(data = 0.9475109743107425), Value(data = -0.9511509474580278)]\n","step: 178, loss: 0.0051413277520558995\n","-------\n","ypred: [Value(data = 0.9476901543713439), Value(data = -0.9513191031655928)]\n","step: 179, loss: 0.005106149666296037\n","-------\n","ypred: [Value(data = 0.9478676342168256), Value(data = -0.9514856412445695)]\n","step: 180, loss: 0.005071426567601309\n","-------\n","ypred: [Value(data = 0.9480434399946255), Value(data = -0.9516505868224928)]\n","step: 181, loss: 0.005037149882201388\n","-------\n","ypred: [Value(data = 0.9482175972990259), Value(data = -0.9518139644907089)]\n","step: 182, loss: 0.005003311247588517\n","-------\n","ypred: [Value(data = 0.9483901311859982), Value(data = -0.9519757983188762)]\n","step: 183, loss: 0.004969902506107722\n","-------\n","ypred: [Value(data = 0.9485610661875652), Value(data = -0.9521361118689977)]\n","step: 184, loss: 0.004936915698777151\n","-------\n","ypred: [Value(data = 0.948730426325701), Value(data = -0.9522949282089924)]\n","step: 185, loss: 0.004904343059329557\n","-------\n","ypred: [Value(data = 0.9488982351257848), Value(data = -0.9524522699258327)]\n","step: 186, loss: 0.004872177008465458\n","-------\n","ypred: [Value(data = 0.9490645156296261), Value(data = -0.9526081591382574)]\n","step: 187, loss: 0.00484041014830934\n","-------\n","ypred: [Value(data = 0.9492292904080769), Value(data = -0.9527626175090792)]\n","step: 188, loss: 0.004809035257060942\n","-------\n","ypred: [Value(data = 0.9493925815732462), Value(data = -0.9529156662571021)]\n","step: 189, loss: 0.004778045283833129\n","-------\n","ypred: [Value(data = 0.9495544107903315), Value(data = -0.953067326168662)]\n","step: 190, loss: 0.004747433343669382\n","-------\n","ypred: [Value(data = 0.949714799289082), Value(data = -0.9532176176088037)]\n","step: 191, loss: 0.004717192712733419\n","-------\n","ypred: [Value(data = 0.9498737678749054), Value(data = -0.9533665605321117)]\n","step: 192, loss: 0.004687316823664071\n","-------\n","ypred: [Value(data = 0.9500313369396332), Value(data = -0.9535141744932009)]\n","step: 193, loss: 0.004657799261089048\n","-------\n","ypred: [Value(data = 0.9501875264719548), Value(data = -0.953660478656887)]\n","step: 194, loss: 0.004628633757291031\n","-------\n","ypred: [Value(data = 0.9503423560675347), Value(data = -0.9538054918080408)]\n","step: 195, loss: 0.004599814188020498\n","-------\n","ypred: [Value(data = 0.9504958449388209), Value(data = -0.9539492323611446)]\n","step: 196, loss: 0.0045713345684491175\n","-------\n","ypred: [Value(data = 0.9506480119245592), Value(data = -0.9540917183695563)]\n","step: 197, loss: 0.004543189049258588\n","-------\n","ypred: [Value(data = 0.9507988754990205), Value(data = -0.9542329675344929)]\n","step: 198, loss: 0.004515371912859659\n","-------\n","ypred: [Value(data = 0.9509484537809527), Value(data = -0.9543729972137464)]\n","step: 199, loss: 0.004487877569736129\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 1.0667738914489746\n","Epoch 10 loss: 0.06401736289262772\n","Epoch 20 loss: 0.0020931633189320564\n","Epoch 30 loss: 5.492215495905839e-05\n","\n","Prediction:\n","tensor([[ 0.9992],\n","        [-0.9985]])\n","Loss: 2.0028644485137193e-06\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 1.0718882083892822\n","Epoch 10 loss: 0.1612805426120758\n","Epoch 20 loss: 0.008970905095338821\n","Epoch 30 loss: 0.0004123043909203261\n","\n","Prediction:\n","tensor([[ 0.9990],\n","        [-0.9940]])\n","Loss: 2.513984145480208e-05\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLUElEQVR4nO3deVhU9eIG8HcWZoZ1ENkVwX0FVFRCcylJNDO91s2Wm8Ytb5kt5q1uVmrZQnrTa6Vp2TWrX4vaTcslFUktjXIB3FJUFEFWEdn3me/vD2RyBFRg4MyceT/PMw/DmXNm3jPnXnk737MohBACRERERDKhlDoAERERkSWx3BAREZGssNwQERGRrLDcEBERkayw3BAREZGssNwQERGRrLDcEBERkayw3BAREZGssNwQERGRrLDcEBFZid27d0OhUGD37t1SRyGyaSw3RDZszZo1UCgUOHjwoNRRrE5D383WrVvx2muvSRfqig8//BBr1qyROgaRbLHcEJHd2Lp1K15//XWpYzRabkaMGIHy8nKMGDGi7UMRyQjLDRFRCwghUF5ebpH3UiqV0Ol0UCr5TzNRS/D/QUR2IDExEePGjYObmxtcXFwwevRo/Pbbb2bzVFdX4/XXX0f37t2h0+nQvn173HrrrYiNjTXNk52djejoaHTs2BFarRZ+fn6YOHEiUlNTG/3sd999FwqFAufPn6/32pw5c6DRaHD58mUAwOnTp3HPPffA19cXOp0OHTt2xP3334/CwsIWfwePPPIIli9fDgBQKBSmRx2j0YilS5eib9++0Ol08PHxweOPP27KVicoKAh33XUXtm/fjkGDBsHR0REfffQRAODTTz/F7bffDm9vb2i1WvTp0wcrVqyot/zx48exZ88eU4ZRo0YBaPyYm/Xr1yMsLAyOjo7w9PTE3/72N2RkZNRbPxcXF2RkZGDSpElwcXGBl5cXnn/+eRgMhhZ/f0S2RC11ACJqXcePH8fw4cPh5uaGF198EQ4ODvjoo48watQo7NmzB+Hh4QCA1157DTExMXjssccwZMgQFBUV4eDBg0hISMAdd9wBALjnnntw/PhxPP300wgKCkJubi5iY2ORlpaGoKCgBj//vvvuw4svvoh169bhhRdeMHtt3bp1GDNmDNq1a4eqqipERUWhsrISTz/9NHx9fZGRkYHNmzejoKAAer2+Rd/D448/jszMTMTGxuKLL75o8PU1a9YgOjoazzzzDM6dO4dly5YhMTER+/btg4ODg2ne5ORkPPDAA3j88ccxffp09OzZEwCwYsUK9O3bF3fffTfUajU2bdqEJ598EkajETNnzgQALF26FE8//TRcXFzwyiuvAAB8fHwazV2XafDgwYiJiUFOTg7ee+897Nu3D4mJiXB3dzfNazAYEBUVhfDwcLz77rvYuXMnFi9ejK5du2LGjBkt+v6IbIogIpv16aefCgDiwIEDjc4zadIkodFoREpKimlaZmamcHV1FSNGjDBNCw0NFePHj2/0fS5fviwAiH//+99NzhkRESHCwsLMpu3fv18AEJ9//rkQQojExEQBQKxfv77J79+Qhr6bmTNniob+2fvll18EAPHll1+aTd+2bVu96YGBgQKA2LZtW733KSsrqzctKipKdOnSxWxa3759xciRI+vNu2vXLgFA7Nq1SwghRFVVlfD29hb9+vUT5eXlpvk2b94sAIh58+aZpk2bNk0AEAsWLDB7zwEDBtT77onkjsNSRDJmMBiwY8cOTJo0CV26dDFN9/Pzw4MPPoi9e/eiqKgIAODu7o7jx4/j9OnTDb6Xo6MjNBoNdu/eXW+o5kamTJmCQ4cOISUlxTRt7dq10Gq1mDhxIgCY9sxs374dZWVlTXr/llq/fj30ej3uuOMO5OXlmR5hYWFwcXHBrl27zObv3LkzoqKi6r2Po6Oj6XlhYSHy8vIwcuRInD17tllDawcPHkRubi6efPJJ6HQ60/Tx48ejV69e2LJlS71lnnjiCbPfhw8fjrNnzzb5s4lsGcsNkYxdvHgRZWVlpmGTq/Xu3RtGoxHp6ekAgAULFqCgoAA9evRAcHAwXnjhBRw5csQ0v1arxcKFC/Hjjz/Cx8cHI0aMwKJFi5CdnX3DHH/961+hVCqxdu1aALUH4a5fv950HBBQWxhmz56NTz75BJ6enoiKisLy5cstcrzNjZw+fRqFhYXw9vaGl5eX2aOkpAS5ublm83fu3LnB99m3bx8iIyPh7OwMd3d3eHl54eWXXwaAZq1H3XFKDW2/Xr161TuOSafTwcvLy2xau3btmlxGiWwdyw0RAag9DTklJQWrV69Gv3798Mknn2DgwIH45JNPTPPMmjULp06dQkxMDHQ6HebOnYvevXsjMTHxuu/t7++P4cOHY926dQCA3377DWlpaZgyZYrZfIsXL8aRI0fw8ssvo7y8HM888wz69u2LCxcuWH6Fr2I0GuHt7Y3Y2NgGHwsWLDCb/+o9NHVSUlIwevRo5OXlYcmSJdiyZQtiY2Px3HPPmT6jtalUqlb/DCJbwHJDJGNeXl5wcnJCcnJyvddOnjwJpVKJgIAA0zQPDw9ER0fj66+/Rnp6OkJCQupd9K5r16745z//iR07duDYsWOoqqrC4sWLb5hlypQpOHz4MJKTk7F27Vo4OTlhwoQJ9eYLDg7Gq6++ip9//hm//PILMjIysHLlyqavfAOuPjvqal27dsWlS5cwbNgwREZG1nuEhobe8L03bdqEyspK/PDDD3j88cdx5513IjIyssEi1FiOawUGBgJAg9svOTnZ9DoRmWO5IZIxlUqFMWPG4Pvvvzc7XTsnJwdfffUVbr31VtOw0KVLl8yWdXFxQbdu3VBZWQkAKCsrQ0VFhdk8Xbt2haurq2me67nnnnugUqnw9ddfY/369bjrrrvg7Oxser2oqAg1NTVmywQHB0OpVJq9f1paGk6ePHlzX8A16j6voKDAbPp9990Hg8GAN954o94yNTU19eZvSN1eEyGEaVphYSE+/fTTBnPczHsOGjQI3t7eWLlypdl38OOPP+LEiRMYP378Dd+DyB7xVHAiGVi9ejW2bdtWb/qzzz6LN998E7Gxsbj11lvx5JNPQq1W46OPPkJlZSUWLVpkmrdPnz4YNWoUwsLC4OHhgYMHD+Lbb7/FU089BQA4deoURo8ejfvuuw99+vSBWq3Ghg0bkJOTg/vvv/+GGb29vXHbbbdhyZIlKC4urjck9dNPP+Gpp57CX//6V/To0QM1NTX44osvoFKpcM8995jmmzp1Kvbs2WNWIm5WWFgYAOCZZ55BVFQUVCoV7r//fowcORKPP/44YmJikJSUhDFjxsDBwQGnT5/G+vXr8d577+Hee++97nuPGTMGGo0GEyZMwOOPP46SkhKsWrUK3t7eyMrKqpdjxYoVePPNN9GtWzd4e3vj9ttvr/eeDg4OWLhwIaKjozFy5Eg88MADplPBg4KCTENeRHQNic/WIqIWqDvdubFHenq6EEKIhIQEERUVJVxcXISTk5O47bbbxK+//mr2Xm+++aYYMmSIcHd3F46OjqJXr17irbfeElVVVUIIIfLy8sTMmTNFr169hLOzs9Dr9SI8PFysW7fupvOuWrVKABCurq5mpzYLIcTZs2fF3//+d9G1a1eh0+mEh4eHuO2228TOnTvN5hs5cmSDp3M39t1cfSp4TU2NePrpp4WXl5dQKBT13ufjjz8WYWFhwtHRUbi6uorg4GDx4osviszMTNM8gYGBjZ4y/8MPP4iQkBCh0+lEUFCQWLhwoVi9erUAIM6dO2eaLzs7W4wfP164uroKAKbTwq89FbzO2rVrxYABA4RWqxUeHh7ioYceEhcuXDCbZ9q0acLZ2blepvnz59/U90UkJwohmvGfP0RERERWisfcEBERkayw3BAREZGssNwQERGRrLDcEBERkayw3BAREZGssNwQERGRrNjdRfyMRiMyMzPh6up605dAJyIiImkJIVBcXAx/f38oldffN2N35SYzM9PsXjpERERkO9LT09GxY8frzmN35cbV1RVA7ZdTd08dIiIism5FRUUICAgw/R2/HrsrN3VDUW5ubiw3RERENuZmDinhAcVEREQkKyw3REREJCssN0RERCQrLDdEREQkKyw3REREJCssN0RERCQrLDdEREQkKyw3REREJCssN0RERCQrLDdEREQkKyw3REREJCssN0RERCQrLDcWdKmkEieyiqSOQUREZNdYbixk27FsDH5rJ+Z8d1TqKERERHaN5cZCBnZyhwCQlF6ArMJyqeMQERHZLZYbC/F20yGsUzsAwI7jORKnISIisl8sNxY0tp8vAODHY1kSJyEiIrJfLDcWFNW3ttzsP5ePSyWVEqchIiKyTyw3FhTg4YR+HdxgFMDOExyaIiIikgLLjYWN7Vs3NJUtcRIiIiL7xHJjYXXH3ew7k4eiimqJ0xAREdkflhsL6+btim7eLqg2CPx0IlfqOERERHaH5aYV1A1NbePQFBERUZtjuWkFdUNTu0/lorzKIHEaIiIi+8Jy0wr6+ruhYztHVFQbsecUh6aIiIjaEstNK1AoFByaIiIikgjLTSsZF1xbbuJO5KKqxihxGiIiIvvBctNKBgS0g5erFsWVNdiXkid1HCIiIrvBctNKlEoFovr6AAC2c2iKiIiozbDctKJx/fwAADv+yIHBKCROQ0REZB9YblrRkM4ecHdyQH5pFfafy5c6DhERkV1guWlFDiolIntfGZo6zqEpIiKitiBpufn5558xYcIE+Pv7Q6FQYOPGjTdcZvfu3Rg4cCC0Wi26deuGNWvWtHrOlhjX789Two0cmiIiImp1kpab0tJShIaGYvny5Tc1/7lz5zB+/HjcdtttSEpKwqxZs/DYY49h+/btrZy0+YZ184SzRoXsogocvlAgdRwiIiLZU0v54ePGjcO4ceNuev6VK1eic+fOWLx4MQCgd+/e2Lt3L/7zn/8gKiqqtWK2iM5Bhdt6eWPzkSxsO56NAZ3aSR2JiIhI1mzqmJv4+HhERkaaTYuKikJ8fHyjy1RWVqKoqMjs0dbqzprafiwbQnBoioiIqDXZVLnJzs6Gj4+P2TQfHx8UFRWhvLy8wWViYmKg1+tNj4CAgLaIamZUTy9o1EqkXirDyeziNv98IiIie2JT5aY55syZg8LCQtMjPT29zTM4a9UY0d0LAO81RURE1Npsqtz4+voiJyfHbFpOTg7c3Nzg6OjY4DJarRZubm5mDylcfdYUERERtR6bKjcRERGIi4szmxYbG4uIiAiJEt28yN4+UCsVSM4pxtmLJVLHISIiki1Jy01JSQmSkpKQlJQEoPZU76SkJKSlpQGoHVKaOnWqaf4nnngCZ8+exYsvvoiTJ0/iww8/xLp16/Dcc89JEb9J9E4OiOjaHgCw/XjODeYmIiKi5pK03Bw8eBADBgzAgAEDAACzZ8/GgAEDMG/ePABAVlaWqegAQOfOnbFlyxbExsYiNDQUixcvxieffGK1p4Ffa6xpaCpL4iRERETypRB2dm5yUVER9Ho9CgsL2/z4m9ziCoS/HQchgH0v3Y4O7g0fJ0RERETmmvL326aOubF13q46DA70AADs4L2miIiIWgXLTRuLujI09SPPmiIiImoVLDdtLKpv7UUID6Tm42JxpcRpiIiI5Iflpo11bOeEkI56CAHsPMGzpoiIiCyN5UYCUX05NEVERNRaWG4kUHdK+K9n8lBYXi1xGiIiInlhuZFAVy8X9PBxQY1RYOcfHJoiIiKyJJYbiYzr5wcA2HwkU+IkRERE8sJyI5EJof4AgF9O5+FyaZXEaYiIiOSD5UYi3bxd0MfPDTVGwQOLiYiILIjlRkJ396/de/PD4QyJkxAREckHy42E7gqpPe7m93P5yC6skDgNERGRPLDcSKhjOyeEBbaDEMCWo7xTOBERkSWw3Ejs7tC6oSmeNUVERGQJLDcSuzPYD0oFcDi9AOcvlUodh4iIyOax3EjMy1WLoV09AQCbuPeGiIioxVhurEDd0NSmwzzuhoiIqKVYbqxAVD9fOKgUSM4pRnJ2sdRxiIiIbBrLjRXQOzpgZA9vALzmDRERUUux3FiJugv6bTqcBSGExGmIiIhsF8uNlYjs7Q1HBxXS8stw+EKh1HGIiIhsFsuNlXDSqBHZxwcA8EMSz5oiIiJqLpYbK1J31tTmI5kwGDk0RURE1BwsN1ZkRA9PuOnUyC2uxP5z+VLHISIiskksN1ZEq1ZhbD9fALwdAxERUXOx3FiZu0M7AAB+PJaFaoNR4jRERES2h+XGykR0bQ9PFy0Kyqqx93Se1HGIiIhsDsuNlVEpFRgfzKEpIiKi5mK5sUJ1F/TbcTwbFdUGidMQERHZFpYbKzSwUzt0cHdEaZUBP53MlToOERGRTWG5sUIKhQJ3hfoB4AX9iIiImorlxkrVXdDvp+RcFFdUS5yGiIjIdrDcWKk+fm7o6uWMqhojdhzPkToOERGRzWC5sVIKhQITruy94VlTREREN4/lxorVDU3tPZOH/NIqidMQERHZBpYbK9bFywX9OrjBYBTYejRL6jhEREQ2geXGyt3NoSkiIqImYbmxcuNDasvNgdR8ZBWWS5yGiIjI+rHcWLkO7o4YHNQOQgBbjnBoioiI6EZYbmwAh6aIiIhuHsuNDRgX7AeVUoEjFwpxLq9U6jhERERWjeXGBni6aDG0a3sAwCbuvSEiIroulhsbMbF/BwC1Q1NCCInTEBERWS+WGxsR1dcHGrUSZ3JLcCKrWOo4REREVovlxka46hxwe09vAMD3hzMkTkNERGS9WG5syN39a8+a2nw4C0Yjh6aIiIgawnJjQ27v5Q0XrRoZBeVISLssdRwiIiKrxHJjQ3QOKozp4wOA17whIiJqDMuNjakbmtp6NAs1BqPEaYiIiKwPy42NGdbNEx7OGuSVVOHXlEtSxyEiIrI6LDc2xkGlxJ3BvgCA75M4NEVERHQtlhsbdHdo7QX9dhzPRkW1QeI0RERE1kXycrN8+XIEBQVBp9MhPDwc+/fvv+78S5cuRc+ePeHo6IiAgAA899xzqKioaKO01mFQYDv46XUorqzB7uRcqeMQERFZFUnLzdq1azF79mzMnz8fCQkJCA0NRVRUFHJzG/6D/dVXX+Gll17C/PnzceLECfz3v//F2rVr8fLLL7dxcmkplQreKZyIiKgRkpabJUuWYPr06YiOjkafPn2wcuVKODk5YfXq1Q3O/+uvv2LYsGF48MEHERQUhDFjxuCBBx644d4eOZpwpdzEnchFcUW1xGmIiIish2TlpqqqCocOHUJkZOSfYZRKREZGIj4+vsFlhg4dikOHDpnKzNmzZ7F161bceeedjX5OZWUlioqKzB5y0NffDV28nFFZY0TsHzlSxyEiIrIakpWbvLw8GAwG+Pj4mE338fFBdnZ2g8s8+OCDWLBgAW699VY4ODiga9euGDVq1HWHpWJiYqDX602PgIAAi66HVBSKP4emeNYUERHRnyQ/oLgpdu/ejbfffhsffvghEhIS8N1332HLli144403Gl1mzpw5KCwsND3S09PbMHHrqis3e8/k4VJJpcRpiIiIrINaqg/29PSESqVCTo75kEpOTg58fX0bXGbu3Ll4+OGH8dhjjwEAgoODUVpain/84x945ZVXoFTW72parRZardbyK2AFuni5ILiDHkczCrH1WDYeviVQ6khERESSk2zPjUajQVhYGOLi4kzTjEYj4uLiEBER0eAyZWVl9QqMSqUCAAhhn3fJrtt7s4lDU0RERAAkHpaaPXs2Vq1ahc8++wwnTpzAjBkzUFpaiujoaADA1KlTMWfOHNP8EyZMwIoVK/DNN9/g3LlziI2Nxdy5czFhwgRTybE3d4X6QaEA9qfmI7OgXOo4REREkpNsWAoApkyZgosXL2LevHnIzs5G//79sW3bNtNBxmlpaWZ7al599VUoFAq8+uqryMjIgJeXFyZMmIC33npLqlWQnJ/eEYODPLD/XD42H8nEP0Z0lToSERGRpBTCzsZzioqKoNfrUVhYCDc3N6njWMT//XYer248hr7+btjyzHCp4xAREVlcU/5+29TZUtSwO4P9oFYqcDyzCCkXS6SOQ0REJCmWGxnwcNZgeHdPAMAPPLCYiIjsHMuNTNzd/8pZU4cz7fbMMSIiIoDlRjbu6OMLrVqJs3mlOJ4pj1tMEBERNQfLjUy4aNWI7F17lhnvFE5ERPaM5UZG6u4UvulwJoxGDk0REZF9YrmRkVE9veCqUyOrsAIHUvOljkNERCQJlhsZ0TmoMLZv7X25ODRFRET2iuVGZurOmtp6NAvVBqPEaYiIiNoey43MRHRpD08XDS6XVWPvmTyp4xAREbU5lhuZUauUGB/sB4B3CiciIvvEciNDdWdN7fgjBxXVBonTEBERtS2WGxka2Kkd/PU6lFTWYHfyRanjEBERtSmWGxlSKhUYH1I7NLX5CIemiIjIvrDcyFTd0FTciVyUVdVInIaIiKjtsNzIVHAHPTp5OKG82oC4E7lSxyEiImozLDcypVAoMCGUQ1NERGR/WG5k7K6Q2qGpXckXUVxRLXEaIiKitsFyI2O9fF3RzdsFVTVGxP6RI3UcIiKiNsFyI2MKhQJ3XTlrahPvNUVERHaC5Ubm6oamfjmdh4KyKonTEBERtT6WG5nr5u2C3n5uqDEKbD+eLXUcIiKiVsdyYwf+HJrKkjgJERFR62O5sQMTrgxN/ZqSh7ySSonTEBERtS6WGzvQqb0TQjvqYRTAj0e594aIiOSN5cZO1N2OYdMRlhsiIpI3lhs7cWdw7XE3B1LzkV1YIXEaIiKi1sNyYyf83R0xOKgdhAC2cGiKiIhkjOXGjtRd84YX9CMiIjljubEj44J9oVQASekFSM8vkzoOERFRq2C5sSPerjrc0qU9AA5NERGRfLHc2BkOTRERkdyx3NiZsf18oVYqcDyzCGcvlkgdh4iIyOJYbuyMh7MGw7p5AgA285o3REQkQyw3dqjugn6bj3BoioiI5Iflxg6N6esDjUqJUzklSM4uljoOERGRRbHc2CE3nQNG9vQCwL03REQkPyw3duqukNrbMWw6nAkhhMRpiIiILIflxk5F9vaBzkGJ1EtlOJ5ZJHUcIiIii2G5sVPOWjVG9/IBAGzi0BQREckIy40dmxBaOzS1+XAWh6aIiEg2WG7s2Kie3nDWqJBRUI7E9AKp4xAREVkEy40d0zmocEefK0NTvB0DERHJBMuNnau7oN+WI1kwGDk0RUREto/lxs4N7+4FN50aucWVOJCaL3UcIiKiFmO5sXMatRJj+/kC4AX9iIhIHlhuyDQ0tfVoNmoMRonTEBERtQzLDSGiS3u0d9Ygv7QKv6ZckjoOERFRi7DcENQqJe4M/vN2DERERLaM5YYA/HmvqW3Hs1FZY5A4DRERUfOx3BAAYHCQB3zctCiuqMHPp/KkjkNERNRsLDcEAFAqFbgrpPbAYp41RUREtkzycrN8+XIEBQVBp9MhPDwc+/fvv+78BQUFmDlzJvz8/KDVatGjRw9s3bq1jdLKW91ZU7F/5KC8ikNTRERkmyQtN2vXrsXs2bMxf/58JCQkIDQ0FFFRUcjNzW1w/qqqKtxxxx1ITU3Ft99+i+TkZKxatQodOnRo4+TyFNpRjwAPR5RVGfDTyYa3ARERkbWTtNwsWbIE06dPR3R0NPr06YOVK1fCyckJq1evbnD+1atXIz8/Hxs3bsSwYcMQFBSEkSNHIjQ0tI2Ty5NC8efQFM+aIiIiWyVZuamqqsKhQ4cQGRn5ZxilEpGRkYiPj29wmR9++AERERGYOXMmfHx80K9fP7z99tswGBofQqmsrERRUZHZgxo34Uq5+Sk5F8UV1RKnISIiajrJyk1eXh4MBgN8fHzMpvv4+CA7O7vBZc6ePYtvv/0WBoMBW7duxdy5c7F48WK8+eabjX5OTEwM9Hq96REQEGDR9ZCb3n6u6OrljKoaI2L/yJE6DhERUZNJfkBxUxiNRnh7e+Pjjz9GWFgYpkyZgldeeQUrV65sdJk5c+agsLDQ9EhPT2/DxLZHoVCYDizefCRL4jRERERNJ1m58fT0hEqlQk6O+d6BnJwc+Pr6NriMn58fevToAZVKZZrWu3dvZGdno6qqqsFltFot3NzczB50fXXH3fx86iIKyhr+XomIiKyVZOVGo9EgLCwMcXFxpmlGoxFxcXGIiIhocJlhw4bhzJkzMBr/vLnjqVOn4OfnB41G0+qZ7UU3bxf09nNDjVFg27GGhwiJiIislaTDUrNnz8aqVavw2Wef4cSJE5gxYwZKS0sRHR0NAJg6dSrmzJljmn/GjBnIz8/Hs88+i1OnTmHLli14++23MXPmTKlWQbYmhF651xQv6EdERDZGLeWHT5kyBRcvXsS8efOQnZ2N/v37Y9u2baaDjNPS0qBU/tm/AgICsH37djz33HMICQlBhw4d8Oyzz+Jf//qXVKsgWxNC/LFoWzLiUy4ht7gC3q46qSMRERHdFIUQQkgdoi0VFRVBr9ejsLCQx9/cwKTl+5CUXoDX7+6LaUODpI5DRER2rCl/v23qbClqW3+eNcWhKSIish3NKjfp6em4cOGC6ff9+/dj1qxZ+Pjjjy0WjKQ3PtgPCgVwIPUyMgvKpY5DRER0U5pVbh588EHs2rULAJCdnY077rgD+/fvxyuvvIIFCxZYNCBJx1evw+AgDwDAFl7zhoiIbESzys2xY8cwZMgQAMC6devQr18//Prrr/jyyy+xZs0aS+YjidUNTfGsKSIishXNKjfV1dXQarUAgJ07d+Luu+8GAPTq1QtZWfwvfDkZ188XKqUCRy4UIjWvVOo4REREN9SsctO3b1+sXLkSv/zyC2JjYzF27FgAQGZmJtq3b2/RgCQtTxcthnat3aZbjrK4EhGR9WtWuVm4cCE++ugjjBo1Cg888ABCQ0MB1N61u264iuSj7k7hmw5zaIqIiKxfs69zYzAYUFRUhHbt2pmmpaamwsnJCd7e3hYLaGm8zk3TFZZVY9Bbsag2COx4bgR6+LhKHYmIiOxMq1/npry8HJWVlaZic/78eSxduhTJyclWXWyoefRODhjZwwsAsJl7b4iIyMo1q9xMnDgRn3/+OQCgoKAA4eHhWLx4MSZNmoQVK1ZYNCBZhz/PmsqCnV3UmoiIbEyzyk1CQgKGDx8OAPj222/h4+OD8+fP4/PPP8f7779v0YBkHSJ7+0DnoMS5vFIczyySOg4REVGjmlVuysrK4Opae9zFjh07MHnyZCiVStxyyy04f/68RQOSdXDWqnF7r9ohR17zhoiIrFmzyk23bt2wceNGpKenY/v27RgzZgwAIDc3lwfpyljdWVObD3NoioiIrFezys28efPw/PPPIygoCEOGDEFERASA2r04AwYMsGhAsh639fKGs0aFjIJyJKQVSB2HiIioQc0qN/feey/S0tJw8OBBbN++3TR99OjR+M9//mOxcGRddA4qjOnrC4DXvCEiIuvVrHIDAL6+vhgwYAAyMzNNdwgfMmQIevXqZbFwZH3uDv3zgn7VBqPEaYiIiOprVrkxGo1YsGAB9Ho9AgMDERgYCHd3d7zxxhswGvkHT86Gd/eEp4sGl0qr8POpi1LHISIiqqdZ5eaVV17BsmXL8M477yAxMRGJiYl4++238cEHH2Du3LmWzkhWRK1S4u7QDgCA7xIyJE5DRERUX7Nuv+Dv74+VK1ea7gZe5/vvv8eTTz6JjAzr/aPH2y+03LGMQtz1wV5o1EoceDkSeicHqSMREZHMtfrtF/Lz8xs8tqZXr17Iz89vzluSDenr74aePq6oqjHyTuFERGR1mlVuQkNDsWzZsnrTly1bhpCQkBaHIuumUCgweWDt0NSGxAsSpyEiIjKnbs5CixYtwvjx47Fz507TNW7i4+ORnp6OrVu3WjQgWadJAzpg4baTOJB6GecvlSKwvbPUkYiIiAA0c8/NyJEjcerUKfzlL39BQUEBCgoKMHnyZBw/fhxffPGFpTOSFfJx02FYN08AwIZE6z3GioiI7E+zDihuzOHDhzFw4EAYDAZLvaXF8YBiy9mYmIFZa5PQycMJe14YBYVCIXUkIiKSqVY/oJgIAMb09YGzRoW0/DIcOn9Z6jhEREQAWG6oBZw0aozt5wcA+B+veUNERFaC5YZa5J4rZ01tPpKJimrrHY4kIiL70aSzpSZPnnzd1wsKClqShWzQLV3aw1+vQ2ZhBeJO5GJ8iJ/UkYiIyM41qdzo9fobvj516tQWBSLbolQqMGlAB3y4OwXfJVxguSEiIsk1qdx8+umnrZWDbNjkgbXlZvepi8grqYSni1bqSEREZMd4zA21WDdvV4R21MNgFNh0OFPqOEREZOdYbsgiJg/sCIB3CiciIumx3JBFTAj1h1qpwNGMQpzKKZY6DhER2TGWG7IID2cNbuvlDYB7b4iISFosN2QxkwfUXvNmY2IGDEaL3dWDiIioSVhuyGJu7+0NN50a2UUViE+5JHUcIiKyUyw3ZDFatQoTQv0BAN8lXJA4DRER2SuWG7KourOmfjyWjdLKGonTEBGRPWK5IYsa2MkdQe2dUF5twLZj2VLHISIiO8RyQxalUChMe282JPKsKSIianssN2Rxf7ly1tS+lDxkFZZLnIaIiOwNyw1ZXICHE4Z09oAQwMZE3o6BiIjaFssNtYp7Btbuvfku4QKE4DVviIio7bDcUKsYF+wHrVqJ07klOJZRJHUcIiKyIyw31CrcdA64o48PAOB/vOYNERG1IZYbajX3XDlr6ofDmag2GCVOQ0RE9oLlhlrN8O6e8HTRIr+0Cjv/yJE6DhER2QmWG2o1apUS9w2q3Xvzefx5idMQEZG9YLmhVvXQLYFQKoD4s5dwOqdY6jhERGQHWG6oVXVwdzQdWMy9N0RE1BZYbqjVTY0IAlB7zZviimppwxARkexZRblZvnw5goKCoNPpEB4ejv3799/Uct988w0UCgUmTZrUugGpRYZ2bY+uXs4orTLguwTeb4qIiFqX5OVm7dq1mD17NubPn4+EhASEhoYiKioKubm5110uNTUVzz//PIYPH95GSam5FAoFpg0NAgB8Fp/KKxYTEVGrkrzcLFmyBNOnT0d0dDT69OmDlStXwsnJCatXr250GYPBgIceegivv/46unTp0oZpqbn+MqADnDUqnL1Yin1nLkkdh4iIZEzSclNVVYVDhw4hMjLSNE2pVCIyMhLx8fGNLrdgwQJ4e3vj0UcfbYuYZAGuOgfcE1Z3WniqtGGIiEjWJC03eXl5MBgM8PHxMZvu4+OD7OzsBpfZu3cv/vvf/2LVqlU39RmVlZUoKioye5A0Hr4lEACw80QOLlwukzgNERHJleTDUk1RXFyMhx9+GKtWrYKnp+dNLRMTEwO9Xm96BAQEtHJKakx3H1cM7doeRgF8+Xua1HGIiEimJC03np6eUKlUyMkxvzR/Tk4OfH19682fkpKC1NRUTJgwAWq1Gmq1Gp9//jl++OEHqNVqpKSk1Ftmzpw5KCwsND3S09NbbX3oxupOC197IB0V1QZpwxARkSxJWm40Gg3CwsIQFxdnmmY0GhEXF4eIiIh68/fq1QtHjx5FUlKS6XH33XfjtttuQ1JSUoN7ZbRaLdzc3MweJJ3I3t7w1+uQX1qFLUeypI5DREQypJY6wOzZszFt2jQMGjQIQ4YMwdKlS1FaWoro6GgAwNSpU9GhQwfExMRAp9OhX79+Zsu7u7sDQL3pZJ3UKiUeuiUQ/96ejM9/O286yJiIiMhSJC83U6ZMwcWLFzFv3jxkZ2ejf//+2LZtm+kg47S0NCiVNnVoEN3AlMEBeG/naRxOL0BSegH6B7hLHYmIiGREIezsimpFRUXQ6/UoLCzkEJWEnlubhA2JGZg8sAOW3Ndf6jhERGTlmvL3m7tESBJTI2pPC998JAuXSiolTkNERHLCckOS6B/gjpCOelTVGLH2IM9gIyIiy2G5IUkoFArTRf2+/C0NBqNdjY4SEVErYrkhyUwI9Uc7JwdkFJQj7kTOjRcgIiK6CSw3JBmdgwpTBncCAHwef17iNEREJBcsNySph8I7QaEA9p7Jw5ncEqnjEBGRDLDckKQCPJwwulftNY3+7zfuvSEiopZjuSHJ1Z0W/u2hCyiprJE4DRER2TqWG5Lcrd080cXTGSWVNdiQcEHqOEREZONYbkhySqUCD1/Ze/N5/HnY2UWziYjIwlhuyCrcE9YRThoVTueWIP7sJanjEBGRDWO5IavgpnPAXwZ0AACs2ZcqbRgiIrJpLDdkNR4ZGgSFAtjxRw7+yCySOg4REdkolhuyGt19XHFnsB8A4L24UxKnISIiW8VyQ1Zl1ujuUCiA7cdzcCyjUOo4RERkg1huyKp093HFhBB/AMDSnaclTkNERLaI5YaszjOju0OpAHaeyMHRC9x7Q0RETcNyQ1anm7cLJvavPXPqPzt57A0RETUNyw1Zpadv7walAvjpZC6S0gukjkNERDaE5YasUhcvF/xlQEcAwFLuvSEioiZguSGr9czoblApFdidfBGHzl+WOg4REdkIlhuyWoHtnTH5ylWLufeGiIhuFssNWbWnb+8OtVKBX07n4WBqvtRxiIjIBrDckFXr1N4J94bVHnvDM6eIiOhmsNyQ1Zt5Wzc4qBTYd+YSfucdw4mI6AZYbsjqBXg44a+DAgBw7w0REd0Yyw3ZhLq9N7+dzUd8CvfeEBFR41huyCZ0cHfE/YM7AajdeyOEkDgRERFZK5YbshlP3tYVGpUS+8/l41fuvSEiokaw3JDN8NM74sHwK3tvYrn3hoiIGsZyQzZlxqiu0KqVOHj+MvaeyZM6DhERWSGWG7IpPm46096bJdx7Q0REDWC5IZszY1RX6ByUSEwrwJ5TF6WOQ0REVoblhmyOt6sOfwsPBMBjb4iIqD6WG7JJj4/sCkcHFQ5fKMT24zlSxyEiIivCckM2yctVi0dv7QwAWLDpOEorayRORERE1oLlhmzWzNu6IcDDEZmFFfhPLG/LQEREtVhuyGY5alR4Y2I/AMDqfedwLKNQ4kRERGQNWG7Ipo3q6Y3xIX4wCuCVDUdhMPLgYiIie8dyQzZv/l194KpV4/CFQnz5+3mp4xARkcRYbsjmebvp8MLYngCAf29LRk5RhcSJiIhISiw3JAsPhQciNMAdxZU1WLDpD6njEBGRhFhuSBZUSgXe/ks/qJQKbDmahV3JuVJHIiIiibDckGz09dcjemgQAGDuxmMorzJIG4iIiCTBckOy8twdPeCv1+HC5XK8/9NpqeMQEZEEWG5IVpy1arx+5do3q34+i+TsYokTERFRW2O5Idm5o48PxvTxQY1R4OUNR2HktW+IiOwKyw3J0mt394WzRoVD5y9j7cF0qeMQEVEbYrkhWfJ3d8Rzd/QAALzz40nklVRKnIiIiNoKyw3J1iNDg9DHzw2F5dV4a8sJqeMQEVEbYbkh2VKrlIiZHAyFAtiQmIF9Z/KkjkRERG2A5YZkLTTAHVNvCQQAvLrxGCqqee0bIiK5s4pys3z5cgQFBUGn0yE8PBz79+9vdN5Vq1Zh+PDhaNeuHdq1a4fIyMjrzk/0z6ie8HbV4lxeKT7cdUbqOERE1MokLzdr167F7NmzMX/+fCQkJCA0NBRRUVHIzW348vm7d+/GAw88gF27diE+Ph4BAQEYM2YMMjIy2jg52Qo3nQPmT+gLAFi+OwW/n70kcSIiImpNCiGEpBcBCQ8Px+DBg7Fs2TIAgNFoREBAAJ5++mm89NJLN1zeYDCgXbt2WLZsGaZOnXrD+YuKiqDX61FYWAg3N7cW5yfbIITAc2uTsDEpE16uWmx5+lZ4u+mkjkVERDepKX+/Jd1zU1VVhUOHDiEyMtI0TalUIjIyEvHx8Tf1HmVlZaiuroaHh0eDr1dWVqKoqMjsQfZHoVDg7cnB6OnjiovFlXjqq0RUG4xSxyIiolYgabnJy8uDwWCAj4+P2XQfHx9kZ2ff1Hv861//gr+/v1lBulpMTAz0er3pERAQ0OLcZJucNGqs+NtAuGjV2J+aj0XbTkodiYiIWoHkx9y0xDvvvINvvvkGGzZsgE7X8BDDnDlzUFhYaHqkp/Nqtfasi5cL3v1rCABg1S/nsPVolsSJiIjI0iQtN56enlCpVMjJyTGbnpOTA19f3+su++677+Kdd97Bjh07EBIS0uh8Wq0Wbm5uZg+yb2P7+eHxEV0AAC+sP4yUiyUSJyIiIkuStNxoNBqEhYUhLi7ONM1oNCIuLg4RERGNLrdo0SK88cYb2LZtGwYNGtQWUUlmXojqifDOHiitMuCJLw6htLJG6khERGQhkg9LzZ49G6tWrcJnn32GEydOYMaMGSgtLUV0dDQAYOrUqZgzZ45p/oULF2Lu3LlYvXo1goKCkJ2djezsbJSU8L++6eapVUp88OAAeLtqcTq3BC99dxQSnzhIREQWInm5mTJlCt59913MmzcP/fv3R1JSErZt22Y6yDgtLQ1ZWX8eF7FixQpUVVXh3nvvhZ+fn+nx7rvvSrUKZKO8XXX48KGBUCsV2HQ4E5/9mip1JCIisgDJr3PT1nidG7rWf/eewxub/4BaqcDax29BWGDDlxUgIiLp2Mx1boiswd+HBWF8iB9qjAJPfpmAvJJKqSMREVELsNyQ3VMoFFh4Twi6ejkjp6gST3+ViBpe4I+IyGax3BABcNGq8dHDYXDSqBB/9hIWx56SOhIRETUTyw3RFd28XbHo3tprJq3YnYIdx2/uKtlERGRdWG6IrnJXiD/+PqwzAOCf63iBPyIiW8RyQ3SNOXf2wqDAdiiurMFDq37H+UulUkciIqImYLkhuoaDSomVD4ehu7cLsosq8OCq35GeXyZ1LCIiukksN0QN8HTR4svp4eji6YyMgnI8+MlvyCwolzoWERHdBJYbokZ4u+rw1fRbENjeCen55Xhw1W/ILqyQOhYREd0Ayw3Rdfjqdfh6+i0I8HBE6qUyPPjJb8gtZsEhIrJmLDdEN+Dv7oivHrsFHdwdcfZiKR5a9TuvYkxEZMVYbohuQoCHE76aHg5fNx1O55bgb5/8jsulVVLHIiKiBrDcEN2kwPbO+Gp6OLxdtTiZXYy//fd3FJZVSx2LiIiuwXJD1ARdvFzw1fRweLpocDyzCFNX/46iChYcIiJrwnJD1ETdvF3x5WO3wMNZg8MXCjFt9X6UVNZIHYuIiK5guSFqhp6+rvi/R8Ohd3RAYloBoj/dj1IWHCIiq8ByQ9RMffzd8H+PhsNVp8aB1MuI/vQADzImIrICLDdELRDcUY8vHg2Hq1aN/an5mLBsL/7ILJI6FhGRXWO5IWqh/gHu+HbGUAS2d8KFy+WYvGIffjicKXUsIiK7xXJDZAE9fV3xw8xbMaKHFyqqjXjm60TEbD2BGoNR6mhERHaH5YbIQvRODvj0kcGYMaorAOCjn88ies0BFJTxOBwiorbEckNkQSqlAv8a2wvLHxwIRwcVfjmdhwnL9uJEFo/DISJqKyw3RK1gfIgfvntyKDp51N5RfPKHv2LzER6HQ0TUFlhuiFpJbz83/PDUMAzv7onyagOe+ioRMT+egMEopI5GRCRrLDdErcjdSYM10UPw+MguAICP9pzFI5/u53E4REStiOWGqJWplArMGdcbHzwwwHQczt3L9uHIhQKpoxERyRLLDVEbmRDqj//NGIqO7RyRll+GScv34bUfjqOYN94kIrIolhuiNtTH3w2bnroVE/v7wyiANb+mInLJHvx4NAtC8FgcIiJLYLkhamPtnDV47/4B+OLRIQhs74ScokrM+DIBj352EOn5ZVLHIyKyeSw3RBIZ3t0L22eNwNO3d4ODSoGfTuZizH9+xkd7UlDNKxsTETUbyw2RhHQOKvxzTE/8+OxwDOnsgfJqA2J+PIkJH+zFofOXpY5HRGSTWG6IrEA3b1es/cctWHRvCNydHHAyuxj3rvwVL284isIyHnBMRNQULDdEVkKhUOC+QQGImz0S9wzsCCGAr35Pw+gle7Ah8QIv/kdEdJMUws5O0SgqKoJer0dhYSHc3NykjkPUqPiUS3hl41GcvVgKAOji5YwnR3XDxP7+cFDxv0uIyL405e83yw2RFausMWDVz2fx8c9nUVRRAwDo4O6Ix0d2wX2DAqBzUEmckIiobbDcXAfLDdmi4opqfPl7Gj755RzySioBAJ4uWjw2vDP+dksgXLRqiRMSEbUulpvrYLkhW1ZRbcC6g+n4aM9ZZBSUAwDcdGo8MqwzoocGoZ2zRuKEREStg+XmOlhuSA6qDUZsTMzAij0ppmNynDQqPBTeCY8N7wIfN53ECYmILIvl5jpYbkhODEaB7cezseynM/gjqwgAoFEpcWewLyYP7Ihh3TyhUiokTklE1HIsN9fBckNyJITA7lMXsfynMzh41cX/vF21mNjfH5MHdkRvP/7vnYhsF8vNdbDckNwlpl3GdwkZ2HQkEwVXXQCwl68rJg/sgIn9O3DYiohsDsvNdbDckL2oqjFiV3IuNiRk4KeTuai6cr8qpQIY1s0Tkwd2QFRfXzhpeKYVEVk/lpvrYLkhe1RQVoXNR7KwITHD7J5VThoVovr6YlRPLwzv7gUPnm1FRFaK5eY6WG7I3qXmlWJDYgY2JGYgLb/MNF2hAII76DG8uydGdPfCwMB2vBIyEVkNlpvrYLkhqiWEwKHzl7Hjjxz8fOoiTmYXm73uolXjli7tMbKHJ0b08EJge2eJkhIRsdxcF8sNUcNyiirwy+k8/HzqIvaeyUN+aZXZ6508nDCihyfCO7dHaEd3BHg4QqHgaeZE1DZYbq6D5YboxoxGgeOZRfj59EX8fOoiDp2/jJpr7krezskBwR3dEdpRj5ArP715FhYRtRKWm+tguSFqupLKGsSnXMLe0xeRmF6AE1lFqDbU/6fDT69DiKnsuCO4gx56JwcJEhOR3LDcXAfLDVHLVdYYcDKrGEcuFODwhUIcuVCA07klaOhfE29XLbp6uaCrt3PtTy8XdPFyhr/eEUpePZmIbhLLzXWw3BC1jtLKGhzLKMSRC4U4fKEAhy8UID2/vNH5dQ5KdPF0QVdvF3T1qi0+ge2d4O/uiPbOGh7PQ0RmWG6ug+WGqO0UVVTj7MVSpOSW4GxeCVJyS5FysQSpl0obHNaqo1Er4a/Xwd/dEX56R3Rwv/Lcvfa5n94RzlpefJDInjTl7zf/dSCiVuOmc0D/AHf0D3A3m15jMCL9cnm90pN+uQy5xZWoqjEi9VIZUi+VNfzGAPSODvB00aC9i7b2p7MW7et+d6792d5FA09nLdwc1dwTRGRHrKLcLF++HP/+97+RnZ2N0NBQfPDBBxgyZEij869fvx5z585FamoqunfvjoULF+LOO+9sw8RE1BJqlRKdPZ3R2dMZgI/Za9UGI7ILK5BZUI6swgpkFJSbnmcWlCOjoBzFFTUoLK9GYXk1Ui6W3vDzHFQKuDtpoHd0gJtODTdHhyvPr/x0VF/1vHa6s1YFF60aTlo1nBxUPD6IyIZIXm7Wrl2L2bNnY+XKlQgPD8fSpUsRFRWF5ORkeHt715v/119/xQMPPICYmBjcdddd+OqrrzBp0iQkJCSgX79+EqwBEVmSg0qJAA8nBHg4NTpPcUU1sgsrkFdShUullbhUUoVLJZXIK61CXnElLpXW/n6ppArFlTWoNghcLK7ExeLKZudydFDBWauCk0YNJ40Kzlp17UOjgqODCloHFXQOSugcVNCpr3p+5af2qmkatRIaldL006Hu97ppaiVULFNEzSb5MTfh4eEYPHgwli1bBgAwGo0ICAjA008/jZdeeqne/FOmTEFpaSk2b95smnbLLbegf//+WLly5Q0/j8fcENmXimoDLpVWoaCsCoXl1Sgqr0FRRTWKymsfheXVKKqouep57c+ySgNKq2pglOhfSKWi9tgjB1XtQ61U1D5USqhVCjgoa3+qVUo4KBVQKRVwUNWWIvWV3+s9FAqoVQooFbXzKK9MUykVUCgUUCkBlaLued10mM2jVADKKz9rf/9zmqKBnwqFAgpc9TuuTKubB7gyH6BA7Qxm0/Hncrjm97qhxj9/r/v2Gpjnqtdr3/3q+c1fa2j61cvVn97w/Ghg/sYqa2PDpo3P38j0Bpaw1IhsU95Ho1bC29Wy172ymWNuqqqqcOjQIcyZM8c0TalUIjIyEvHx8Q0uEx8fj9mzZ5tNi4qKwsaNGxucv7KyEpWVf/7XWlFRUcuDE5HN0Dmo0MHdER3cHZu8rBAClTVGlFbWoKyqtuyUVhqu/F77vKyq9rWKaiMqagyoqK59XlltQEWNAZWm6cYrrxlQZTCiqsaIaoNAVY3R9PvVjAJXljE2ko7Ieg3s5I7vnhwm2edLWm7y8vJgMBjg42M+5u7j44OTJ082uEx2dnaD82dnZzc4f0xMDF5//XXLBCYiu6JQKK4MLanQvpU/SwiBGuOVslNjRLXBiMorP2uMAtUGIwxGgWqDQM1V02oMAjXG2t9rnwsYjEYYjIDhynRD3UMIGK7MY7zyecYrzw1GwCjqngsYBf58TdTNBwjUzieEgNG0TG3+uue1rwMCtfMI1E5H3Wuomx+m53XzCwHT+9V+L39ON82L2veq+2Gadu38ov53fNWipnnrnl/92tXTcM085u9h/iHX7ugzW+YG8147obGdho0NuDRlJ2NTx2xEk969ds+NlCQ/5qa1zZkzx2xPT1FREQICAiRMRERUn0KhgIOqdmjJWSt1GiLbJmm58fT0hEqlQk5Ojtn0nJwc+Pr6NriMr69vk+bXarXQavkvBRERkb2QdL+RRqNBWFgY4uLiTNOMRiPi4uIQERHR4DIRERFm8wNAbGxso/MTERGRfZF8WGr27NmYNm0aBg0ahCFDhmDp0qUoLS1FdHQ0AGDq1Kno0KEDYmJiAADPPvssRo4cicWLF2P8+PH45ptvcPDgQXz88cdSrgYRERFZCcnLzZQpU3Dx4kXMmzcP2dnZ6N+/P7Zt22Y6aDgtLQ1K5Z87mIYOHYqvvvoKr776Kl5++WV0794dGzdu5DVuiIiICIAVXOemrfE6N0RERLanKX+/pT1Xi4iIiMjCWG6IiIhIVlhuiIiISFZYboiIiEhWWG6IiIhIVlhuiIiISFZYboiIiEhWWG6IiIhIVlhuiIiISFYkv/1CW6u7IHNRUZHESYiIiOhm1f3dvpkbK9hduSkuLgYABAQESJyEiIiImqq4uBh6vf6689jdvaWMRiMyMzPh6uoKhUJh0fcuKipCQEAA0tPTZX3fKntYT3tYR4DrKTdcT/mwh3UEmraeQggUFxfD39/f7IbaDbG7PTdKpRIdO3Zs1c9wc3OT9f8Y69jDetrDOgJcT7nhesqHPawjcPPreaM9NnV4QDERERHJCssNERERyQrLjQVptVrMnz8fWq1W6iityh7W0x7WEeB6yg3XUz7sYR2B1ltPuzugmIiIiOSNe26IiIhIVlhuiIiISFZYboiIiEhWWG6IiIhIVlhuLGT58uUICgqCTqdDeHg49u/fL3Uki3rttdegUCjMHr169ZI6Vov9/PPPmDBhAvz9/aFQKLBx40az14UQmDdvHvz8/ODo6IjIyEicPn1amrAtcKP1fOSRR+pt37Fjx0oTtpliYmIwePBguLq6wtvbG5MmTUJycrLZPBUVFZg5cybat28PFxcX3HPPPcjJyZEocfPczHqOGjWq3vZ84oknJErcPCtWrEBISIjp4m4RERH48ccfTa/LYVsCN15POWzLa73zzjtQKBSYNWuWaZqltyfLjQWsXbsWs2fPxvz585GQkIDQ0FBERUUhNzdX6mgW1bdvX2RlZZkee/fulTpSi5WWliI0NBTLly9v8PVFixbh/fffx8qVK/H777/D2dkZUVFRqKioaOOkLXOj9QSAsWPHmm3fr7/+ug0TttyePXswc+ZM/Pbbb4iNjUV1dTXGjBmD0tJS0zzPPfccNm3ahPXr12PPnj3IzMzE5MmTJUzddDezngAwffp0s+25aNEiiRI3T8eOHfHOO+/g0KFDOHjwIG6//XZMnDgRx48fByCPbQnceD0B29+WVztw4AA++ugjhISEmE23+PYU1GJDhgwRM2fONP1uMBiEv7+/iImJkTCVZc2fP1+EhoZKHaNVARAbNmww/W40GoWvr6/497//bZpWUFAgtFqt+PrrryVIaBnXrqcQQkybNk1MnDhRkjytJTc3VwAQe/bsEULUbjsHBwexfv160zwnTpwQAER8fLxUMVvs2vUUQoiRI0eKZ599VrpQraRdu3bik08+ke22rFO3nkLIa1sWFxeL7t27i9jYWLP1ao3tyT03LVRVVYVDhw4hMjLSNE2pVCIyMhLx8fESJrO806dPw9/fH126dMFDDz2EtLQ0qSO1qnPnziE7O9ts2+r1eoSHh8tu2wLA7t274e3tjZ49e2LGjBm4dOmS1JFapLCwEADg4eEBADh06BCqq6vNtmevXr3QqVMnm96e165nnS+//BKenp7o168f5syZg7KyMiniWYTBYMA333yD0tJSREREyHZbXruedeSyLWfOnInx48ebbTegdf6/aXc3zrS0vLw8GAwG+Pj4mE338fHByZMnJUpleeHh4VizZg169uyJrKwsvP766xg+fDiOHTsGV1dXqeO1iuzsbABocNvWvSYXY8eOxeTJk9G5c2ekpKTg5Zdfxrhx4xAfHw+VSiV1vCYzGo2YNWsWhg0bhn79+gGo3Z4ajQbu7u5m89ry9mxoPQHgwQcfRGBgIPz9/XHkyBH861//QnJyMr777jsJ0zbd0aNHERERgYqKCri4uGDDhg3o06cPkpKSZLUtG1tPQD7b8ptvvkFCQgIOHDhQ77XW+P8myw3dlHHjxpmeh4SEIDw8HIGBgVi3bh0effRRCZORJdx///2m58HBwQgJCUHXrl2xe/dujB49WsJkzTNz5kwcO3ZMFseFXU9j6/mPf/zD9Dw4OBh+fn4YPXo0UlJS0LVr17aO2Ww9e/ZEUlISCgsL8e2332LatGnYs2eP1LEsrrH17NOnjyy2ZXp6Op599lnExsZCp9O1yWdyWKqFPD09oVKp6h3VnZOTA19fX4lStT53d3f06NEDZ86ckTpKq6nbfva2bQGgS5cu8PT0tMnt+9RTT2Hz5s3YtWsXOnbsaJru6+uLqqoqFBQUmM1vq9uzsfVsSHh4OADY3PbUaDTo1q0bwsLCEBMTg9DQULz33nuy25aNrWdDbHFbHjp0CLm5uRg4cCDUajXUajX27NmD999/H2q1Gj4+Phbfniw3LaTRaBAWFoa4uDjTNKPRiLi4OLMxU7kpKSlBSkoK/Pz8pI7Sajp37gxfX1+zbVtUVITff/9d1tsWAC5cuIBLly7Z1PYVQuCpp57Chg0b8NNPP6Fz585mr4eFhcHBwcFseyYnJyMtLc2mtueN1rMhSUlJAGBT27MhRqMRlZWVstmWjalbz4bY4rYcPXo0jh49iqSkJNNj0KBBeOihh0zPLb49W378M33zzTdCq9WKNWvWiD/++EP84x//EO7u7iI7O1vqaBbzz3/+U+zevVucO3dO7Nu3T0RGRgpPT0+Rm5srdbQWKS4uFomJiSIxMVEAEEuWLBGJiYni/PnzQggh3nnnHeHu7i6+//57ceTIETFx4kTRuXNnUV5eLnHyprneehYXF4vnn39exMfHi3PnzomdO3eKgQMHiu7du4uKigqpo9+0GTNmCL1eL3bv3i2ysrJMj7KyMtM8TzzxhOjUqZP46aefxMGDB0VERISIiIiQMHXT3Wg9z5w5IxYsWCAOHjwozp07J77//nvRpUsXMWLECImTN81LL70k9uzZI86dOyeOHDkiXnrpJaFQKMSOHTuEEPLYlkJcfz3lsi0bcu1ZYJbeniw3FvLBBx+ITp06CY1GI4YMGSJ+++03qSNZ1JQpU4Sfn5/QaDSiQ4cOYsqUKeLMmTNSx2qxXbt2CQD1HtOmTRNC1J4OPnfuXOHj4yO0Wq0YPXq0SE5OljZ0M1xvPcvKysSYMWOEl5eXcHBwEIGBgWL69Ok2V84bWj8A4tNPPzXNU15eLp588knRrl074eTkJP7yl7+IrKws6UI3w43WMy0tTYwYMUJ4eHgIrVYrunXrJl544QVRWFgobfAm+vvf/y4CAwOFRqMRXl5eYvTo0aZiI4Q8tqUQ119PuWzLhlxbbiy9PRVCCNG8fT5ERERE1ofH3BAREZGssNwQERGRrLDcEBERkayw3BAREZGssNwQERGRrLDcEBERkayw3BAREZGssNwQkd0JCgrC0qVLpY5BRK2E5YaIWtUjjzyCSZMmAQBGjRqFWbNmtdlnr1mzBu7u7vWmHzhwwOxuy0QkL2qpAxARNVVVVRU0Gk2zl/fy8rJgGiKyNtxzQ0Rt4pFHHsGePXvw3nvvQaFQQKFQIDU1FQBw7NgxjBs3Di4uLvDx8cHDDz+MvLw807KjRo3CU089hVmzZsHT0xNRUVEAgCVLliA4OBjOzs4ICAjAk08+iZKSEgDA7t27ER0djcLCQtPnvfbaawDqD0ulpaVh4sSJcHFxgZubG+677z7k5OSYXn/ttdfQv39/fPHFFwgKCoJer8f999+P4uLi1v3SiKhZWG6IqE289957iIiIwPTp05GVlYWsrCwEBASgoKAAt99+OwYMGICDBw9i27ZtyMnJwX333We2/GeffQaNRoN9+/Zh5cqVAAClUon3338fx48fx2effYaffvoJL774IgBg6NChWLp0Kdzc3Eyf9/zzz9fLZTQaMXHiROTn52PPnj2IjY3F2bNnMWXKFLP5UlJSsHHjRmzevBmbN2/Gnj178M4777TSt0VELcFhKSJqE3q9HhqNBk5OTvD19TVNX7ZsGQYMGIC3337bNG316tUICAjAqVOn0KNHDwBA9+7dsWjRIrP3vPr4naCgILz55pt44okn8OGHH0Kj0UCv10OhUJh93rXi4uJw9OhRnDt3DgEBAQCAzz//HH379sWBAwcwePBgALUlaM2aNXB1dQUAPPzww4iLi8Nbb73Vsi+GiCyOe26ISFKHDx/Grl274OLiYnr06tULQO3ekjphYWH1lt25cydGjx6NDh06wNXVFQ8//DAuXbqEsrKym/78EydOICAgwFRsAKBPnz5wd3fHiRMnTNOCgoJMxQYA/Pz8kJub26R1JaK2wT03RCSpkpISTJgwAQsXLqz3mp+fn+m5s7Oz2Wupqam46667MGPGDLz11lvw8PDA3r178eijj6KqqgpOTk4Wzeng4GD2u0KhgNFotOhnEJFlsNwQUZvRaDQwGAxm0wYOHIj//e9/CAoKglp98/8kHTp0CEajEYsXL4ZSWbsTet26dTf8vGv17t0b6enpSE9PN+29+eOPP1BQUIA+ffrcdB4ish4cliKiNhMUFITff/8dqampyMvLg9FoxMyZM5Gfn48HHngABw4cQEpKCrZv347o6OjrFpNu3bqhuroaH3zwAc6ePYsvvvjCdKDx1Z9XUlKCuLg45OXlNThcFRkZieDgYDz00ENISEjA/v37MXXqVIwcORKDBg2y+HdARK2P5YaI2szzzz8PlUqFPn36wMvLC2lpafD398e+fftgMBgwZswYBAcHY9asWXB3dzftkWlIaGgolixZgoULF6Jfv3748ssvERMTYzbP0KFD8cQTT2DKlCnw8vKqd0AyUDu89P3336Ndu3YYMWIEIiMj0aVLF6xdu9bi609EbUMhhBBShyAiIiKyFO65ISIiIllhuSEiIiJZYbkhIiIiWWG5ISIiIllhuSEiIiJZYbkhIiIiWWG5ISIiIllhuSEiIiJZYbkhIiIiWWG5ISIiIllhuSEiIiJZYbkhIiIiWfl/Ey8giUUULvMAAAAASUVORK5CYII=","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[ 0.1711, -0.4917,  0.3542],\n","        [ 0.0705, -0.4072, -0.2638],\n","        [-0.2197, -0.3984, -0.3755],\n","        [-0.2389,  0.3789, -0.2519]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-1.4869,  1.1821],\n","        [-0.8169,  0.4870],\n","        [-1.2591, -0.4483],\n","        [ 0.9108, -1.2217]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[ 0.3066],\n","        [-0.1112],\n","        [-0.5459],\n","        [-0.1532]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-1.1803,  1.4887],\n","        [-0.9281,  0.3758],\n","        [-1.8050, -0.9942],\n","        [ 0.7576, -1.3749]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.8275,  0.9031],\n","        [-0.7297,  0.3590],\n","        [-0.9473, -0.7591],\n","        [ 0.6397, -0.8798]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[ 0.6170,  0.2965, -0.0453, -0.5058],\n","        [-0.0169,  0.1108, -0.3873, -0.6387],\n","        [ 0.0498,  0.4687,  0.0153, -0.1835],\n","        [ 0.2367, -0.3383,  0.0424,  0.1172]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.8275,  0.9031],\n","        [-0.7297,  0.3590],\n","        [-0.9473, -0.7591],\n","        [ 0.6397, -0.8798]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-1.0076,  1.1431],\n","        [-0.1085,  0.8805],\n","        [-0.5151,  0.3630],\n","        [ 0.0858, -0.0429]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[ 0.1340],\n","        [-0.1541],\n","        [-0.3803],\n","        [ 0.0882]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.8736,  1.2771],\n","        [-0.2626,  0.7264],\n","        [-0.8954, -0.0173],\n","        [ 0.1739,  0.0453]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.7032,  0.8557],\n","        [-0.2568,  0.6208],\n","        [-0.7140, -0.0173],\n","        [ 0.1722,  0.0452]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[-0.9637, -0.5098, -0.0628, -0.0027]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.7032,  0.8557],\n","        [-0.2568,  0.6208],\n","        [-0.7140, -0.0173],\n","        [ 0.1722,  0.0452]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 0.8529, -1.1401]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[0.1461]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9990, -0.9940]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9990, -0.9940]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":189,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":190,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9990, -0.9940]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(1.8415e-05)"]},"execution_count":190,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","\n","# loss = torch.sum(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":191,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9990, -0.9940]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-0.0010,  0.0060]])\n","squared_difference: tensor([[1.0130e-06, 3.5818e-05]])\n"]},{"data":{"text/plain":["tensor(1.8415e-05)"]},"execution_count":191,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":192,"metadata":{},"outputs":[{"data":{"text/plain":["[0.9989935159683228, -0.9940152168273926]"]},"execution_count":192,"metadata":{},"output_type":"execute_result"}],"source":["# for item in output.item:\n","#   print(item)\n","# type(output)\n","output.tolist()[0]\n"]},{"cell_type":"code","execution_count":193,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.841531986457312e-05\n"]}],"source":["import numpy as np\n","\n","def mse_loss(y_true, y_pred):\n","  \"\"\"Calculates the mean squared error loss.\n","\n","  Args:\n","    y_true: The ground truth labels.\n","    y_pred: The predicted labels.\n","\n","  Returns:\n","    The mean squared error loss.\n","  \"\"\"\n","\n","  loss = np.mean((y_true - y_pred)**2)\n","  return loss\n","\n","def main():\n","  \"\"\"Main function.\"\"\"\n","\n","  # y_true = np.array([1, 2, 3, 4, 5])\n","  y_true = np.array([1.0, -1.0])\n","\n","  # y_pred = np.array([0, 1, 2, 3, 4])\n","  # y_pred = np.array([0.9997345209121704, -0.9980572462081909])\n","  y_pred = np.array(output.tolist()[0])  \n","\n","  loss = mse_loss(y_true, y_pred)\n","  print(loss)\n","\n","if __name__ == \"__main__\":\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":194,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":194,"metadata":{},"output_type":"execute_result"}],"source":["len(squared_difference)\n"]},{"cell_type":"code","execution_count":195,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.8415e-05)"]},"execution_count":195,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":196,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(3.6831e-05)"]},"execution_count":196,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":197,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- torch results matched backward pass results ----\n","x0.data.item()  = -3.000000\n","x0.grad.item()  =  1.000000\n","w0.data.item()  =  2.000000\n","w0.grad.item()  = -1.500000 <-- result matched micrograd\n","---\n","x1.data.item()  =  0.000000\n","x1.grad.item()  =  0.500000\n","w1.data.item()  =  1.000000\n","w1.grad.item()  =  0.000000\n","---\n","x2.data.item()  =  0.500000\n","x2.grad.item()  =  0.500000\n","w2.data.item()  =  1.000000\n","w2.grad.item()  =  0.250000\n","---\n","out.data.item() = -0.707107 <-- result matched micrograd\n"]}],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <-- result matched micrograd')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <-- result matched micrograd')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":198,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":199,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["parameters in MLP: 41\n","\n","i:  0,   0.8319599575\n","i:  1,   0.8609907326\n","i:  2,  -0.4257097644\n","i:  3,   0.4365094175\n","i:  4,  -0.7071191567\n","---\n","i: 36,   0.4234582019\n","i: 37,  -0.0478053349\n","i: 38,  -0.9716211876\n","i: 39,  -0.4475568742\n","i: 40,  -0.4139976786\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'parameters in MLP: {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":200,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 3)\n","xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":201,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","layer: 0, neuron 0\n","w0:  0.8319600,   w0.grad:  0.0000000\n","w1:  0.8609907,   w1.grad:  0.0000000\n","w2: -0.4257098,   w2.grad:  0.0000000\n","b:   0.4365094\n","\n","layer: 0, neuron 1\n","w0: -0.7071192,   w0.grad:  0.0000000\n","w1: -0.8268281,   w1.grad:  0.0000000\n","w2: -0.5004434,   w2.grad:  0.0000000\n","b:  -0.7521156\n","\n","layer: 0, neuron 2\n","w0: -0.7210451,   w0.grad:  0.0000000\n","w1:  0.2755378,   w1.grad:  0.0000000\n","w2:  0.6575294,   w2.grad:  0.0000000\n","b:   0.2720229\n","\n","layer: 0, neuron 3\n","w0:  0.0883634,   w0.grad:  0.0000000\n","w1: -0.3485455,   w1.grad:  0.0000000\n","w2: -0.9299397,   w2.grad:  0.0000000\n","b:  -0.6097657\n","\n","------\n","layer: 1, neuron_cnt: 4\n","----\n","layer: 1, neuron 0\n","w0: -0.8444723,   w0.grad:  0.0000000\n","w1: -0.4259739,   w1.grad:  0.0000000\n","w2: -0.0239105,   w2.grad:  0.0000000\n","w3:  0.6847080,   w3.grad:  0.0000000\n","b:   0.1879889\n","\n","layer: 1, neuron 1\n","w0:  0.1510274,   w0.grad:  0.0000000\n","w1: -0.0931277,   w1.grad:  0.0000000\n","w2:  0.2564109,   w2.grad:  0.0000000\n","w3:  0.0408543,   w3.grad:  0.0000000\n","b:  -0.4031348\n","\n","layer: 1, neuron 2\n","w0:  0.0797383,   w0.grad:  0.0000000\n","w1: -0.8679798,   w1.grad:  0.0000000\n","w2:  0.3313646,   w2.grad:  0.0000000\n","w3: -0.5912548,   w3.grad:  0.0000000\n","b:  -0.2700038\n","\n","layer: 1, neuron 3\n","w0: -0.7168359,   w0.grad:  0.0000000\n","w1:  0.1441776,   w1.grad:  0.0000000\n","w2: -0.2950260,   w2.grad:  0.0000000\n","w3: -0.4672216,   w3.grad:  0.0000000\n","b:  -0.4154805\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0:  0.4234582,   w0.grad:  0.0000000\n","w1: -0.0478053,   w1.grad:  0.0000000\n","w2: -0.9716212,   w2.grad:  0.0000000\n","w3: -0.4475569,   w3.grad:  0.0000000\n","b:  -0.4139977\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":202,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(4, 3):\n","[[ 0.83195996  0.86099073 -0.42570976]\n"," [-0.70711916 -0.82682814 -0.5004434 ]\n"," [-0.7210451   0.27553779  0.65752939]\n"," [ 0.08836338 -0.3485455  -0.92993969]]\n","b_mat(1, 4):\n","[[ 0.43650942 -0.75211556  0.2720229  -0.60976565]]\n","\n","i: 1\n","w_mat(4, 4):\n","[[-0.84447233 -0.42597391 -0.02391051  0.68470799]\n"," [ 0.15102743 -0.09312771  0.25641091  0.04085433]\n"," [ 0.0797383  -0.86797978  0.33136464 -0.59125478]\n"," [-0.71683594  0.14417762 -0.29502604 -0.46722156]]\n","b_mat(1, 4):\n","[[ 0.18798887 -0.40313485 -0.27000376 -0.41548048]]\n","\n","i: 2\n","w_mat(1, 4):\n","[[ 0.4234582  -0.04780533 -0.97162119 -0.44755687]]\n","b_mat(1, 1):\n","[[-0.41399768]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":203,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[ 0.83195996  0.86099073 -0.42570976]\n"," [-0.70711916 -0.82682814 -0.5004434 ]\n"," [-0.7210451   0.27553779  0.65752939]\n"," [ 0.08836338 -0.3485455  -0.92993969]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","weights_x_inputs (4, 2):\n","[[ 4.67260188  1.42203426]\n"," [-3.39427934 -1.54475103]\n"," [-1.27300622 -2.1099084 ]\n"," [ 0.06102995  0.14866579]]\n","\n","bias (4, 1):\n","[[ 0.43650942]\n"," [-0.75211556]\n"," [ 0.2720229 ]\n"," [-0.60976565]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[ 5.10911129  1.85854368]\n"," [-4.1463949  -2.29686659]\n"," [-1.00098332 -1.8378855 ]\n"," [-0.54873571 -0.46109986]]\n","\n","output (4, 2):\n","[[ 0.999927    0.95254409]\n"," [-0.99949949 -0.97997253]\n"," [-0.76200682 -0.95059177]\n"," [-0.49957205 -0.43098021]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[-0.84447233 -0.42597391 -0.02391051  0.68470799]\n"," [ 0.15102743 -0.09312771  0.25641091  0.04085433]\n"," [ 0.0797383  -0.86797978  0.33136464 -0.59125478]\n"," [-0.71683594  0.14417762 -0.29502604 -0.46722156]]\n","\n","input (4, 2):\n","[[ 0.999927    0.95254409]\n"," [-0.99949949 -0.97997253]\n"," [-0.76200682 -0.95059177]\n"," [-0.49957205 -0.43098021]]\n","\n","weights_x_inputs (4, 2):\n","[[-0.74249099 -0.65932086]\n"," [ 0.02830095 -0.02622663]\n"," [ 0.99015008  0.86637719]\n"," [-0.40266637 -0.34229536]]\n","\n","bias (4, 1):\n","[[ 0.18798887]\n"," [-0.40313485]\n"," [-0.27000376]\n"," [-0.41548048]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-0.55450212 -0.47133199]\n"," [-0.37483389 -0.42936148]\n"," [ 0.72014632  0.59637343]\n"," [-0.81814685 -0.75777584]]\n","\n","output (4, 2):\n","[[-0.50388685 -0.43927491]\n"," [-0.35821261 -0.40478755]\n"," [ 0.61699993  0.53446395]\n"," [-0.67405998 -0.63976502]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[ 0.4234582  -0.04780533 -0.97162119 -0.44755687]]\n","\n","input (4, 2):\n","[[-0.50388685 -0.43927491]\n"," [-0.35821261 -0.40478755]\n"," [ 0.61699993  0.53446395]\n"," [-0.67405998 -0.63976502]]\n","\n","weights_x_inputs (1, 2):\n","[[-0.49406057 -0.39962883]]\n","\n","bias (1, 1):\n","[[-0.41399768]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[-0.90805824 -0.81362651]]\n","\n","output (1, 2):\n","[[-0.72019897 -0.67158596]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [-0.72019897 -0.67158596]\n","desired output:     [1.0, -1.0]\n","loss:               3.0669402679225244\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [-0.7201989675163029, -0.6715859624218699]\n","ys:                 [1.0, -1.0]\n","loss_data:          3.066940267922524\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0      0.8319599575    -0.0160134800           0.05000         0.8327606315\n","  1      0.8609907326     0.0054672579           0.05000         0.8607173697\n","  2     -0.4257097644    -0.0027159792           0.05000        -0.4255739655\n","  3      0.4365094175    -0.0053260602           0.05000         0.4367757205\n","  4     -0.7071191567     0.0169409643           0.05000        -0.7079662050\n","  5     -0.8268281425    -0.0078124736           0.05000        -0.8264375188\n","  6     -0.5004434027     0.0036109433           0.05000        -0.5006239499\n","  7     -0.7521155598     0.0054501258           0.05000        -0.7523880661\n","  8     -0.7210451019     0.1844229530           0.05000        -0.7302662496\n","  9      0.2755377912     0.3092062062           0.05000         0.2600774809\n"," 10      0.6575293889    -0.1040557589           0.05000         0.6627321768\n"," 11      0.2720228994     0.0951725471           0.05000         0.2672642720\n"," 12      0.0883633787    -1.0262493206           0.05000         0.1396758448\n"," 13     -0.3485454990    -2.7767503707           0.05000        -0.2097079805\n"," 14     -0.9299396862     0.9630797111           0.05000        -0.9780936718\n"," 15     -0.6097656543    -0.6256134230           0.05000        -0.5784849832\n"," 16     -0.8444723299    -0.4057528643           0.05000        -0.8241846866\n"," 17     -0.4259739055     0.4021492757           0.05000        -0.4460813692\n"," 18     -0.0239105062     0.2815206541           0.05000        -0.0379865389\n"," 19      0.6847079906     0.2082528207           0.05000         0.6742953496\n"," 20      0.1879888741    -0.3999431963           0.05000         0.2079860339\n"," 21      0.1510274259     0.0552693670           0.05000         0.1482639575\n"," 22     -0.0931277085    -0.0548445350           0.05000        -0.0903854818\n"," 23      0.2564109115    -0.0388801482           0.05000         0.2583549189\n"," 24      0.0408543313    -0.0282604733           0.05000         0.0422673550\n"," 25     -0.4031348478     0.0545904112           0.05000        -0.4058643684\n"," 26      0.0797383003     0.7579540448           0.05000         0.0418405980\n"," 27     -0.8679797797    -0.7506635456           0.05000        -0.8304466025\n"," 28      0.3313646438    -0.5213741789           0.05000         0.3574333527\n"," 29     -0.5912547835    -0.3899221689           0.05000        -0.5717586750\n"," 30     -0.2700037627     0.7461499791           0.05000        -0.3073112616\n"," 31     -0.7168359357     0.3135505725           0.05000        -0.7325134644\n"," 32      0.1441776154    -0.3107630123           0.05000         0.1597157660\n"," 33     -0.2950260435    -0.2175256439           0.05000        -0.2841497613\n"," 34     -0.4672215606    -0.1609345440           0.05000        -0.4591748334\n"," 35     -0.4154804802     0.3090562408           0.05000        -0.4309332923\n"," 36      0.4234582019     0.6759972319           0.05000         0.3896583403\n"," 37     -0.0478053349     0.4472093114           0.05000        -0.0701658004\n"," 38     -0.9716211876    -0.8289789835           0.05000        -0.9301722384\n"," 39     -0.4475568742     0.8854957858           0.05000        -0.4918316635\n"," 40     -0.4139976786    -1.2953293739           0.05000        -0.3492312099\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = -0.19642314124125418), Value(data = -0.47403319043897213)]\n","step: 0, loss: 1.708069417657397\n","-------\n","ypred: [Value(data = 0.7173418496137512), Value(data = -0.10464045409359357)]\n","step: 1, loss: 0.8815643464255015\n","-------\n","ypred: [Value(data = 0.48683723066994133), Value(data = -0.7164432106470164)]\n","step: 2, loss: 0.34374048061466733\n","-------\n","ypred: [Value(data = 0.6447063401662155), Value(data = -0.684118858698428)]\n","step: 3, loss: 0.22601448014806863\n","-------\n","ypred: [Value(data = 0.6894711444360052), Value(data = -0.6995440035318992)]\n","step: 4, loss: 0.18670197595152377\n","-------\n","ypred: [Value(data = 0.7173957391671019), Value(data = -0.7189909991279243)]\n","step: 5, loss: 0.15883122681203096\n","-------\n","ypred: [Value(data = 0.7383486024784262), Value(data = -0.7371569917885209)]\n","step: 6, loss: 0.13754790079065232\n","-------\n","ypred: [Value(data = 0.7552698306060729), Value(data = -0.7532244321072192)]\n","step: 7, loss: 0.12079103672038471\n","-------\n","ypred: [Value(data = 0.7694471991164297), Value(data = -0.7672912187305243)]\n","step: 8, loss: 0.10730797087518391\n","-------\n","ypred: [Value(data = 0.781593462993773), Value(data = -0.7796297503918352)]\n","step: 9, loss: 0.09626446231941727\n","-------\n","ypred: [Value(data = 0.7921625224835901), Value(data = -0.7905126614577677)]\n","step: 10, loss: 0.08708136206989206\n","-------\n","ypred: [Value(data = 0.8014688748481201), Value(data = -0.80017438463245)]\n","step: 11, loss: 0.0793448842110914\n","-------\n","ypred: [Value(data = 0.8097423130692776), Value(data = -0.808807940691704)]\n","step: 12, loss: 0.07275239097877576\n","-------\n","ypred: [Value(data = 0.8171568540943853), Value(data = -0.8165703870175423)]\n","step: 13, loss: 0.06707803892355613\n","-------\n","ypred: [Value(data = 0.8238477249982629), Value(data = -0.8235895050462849)]\n","step: 14, loss: 0.062150286718102354\n","-------\n","ypred: [Value(data = 0.8299221122676083), Value(data = -0.8299697438617386)]\n","step: 15, loss: 0.05783677589795483\n","-------\n","ypred: [Value(data = 0.8354663416042029), Value(data = -0.8357970467763942)]\n","step: 16, loss: 0.05403393459245853\n","-------\n","ypred: [Value(data = 0.8405508687884643), Value(data = -0.8411426473766217)]\n","step: 17, loss: 0.05065968392662189\n","-------\n","ypred: [Value(data = 0.8452338563121284), Value(data = -0.8460660242376729)]\n","step: 18, loss: 0.047648228126011626\n","-------\n","ypred: [Value(data = 0.8495637990254812), Value(data = -0.8506171986851079)]\n","step: 19, loss: 0.04494627189233035\n","-------\n","ypred: [Value(data = 0.8535814899400606), Value(data = -0.854838528062718)]\n","step: 20, loss: 0.042510233023170904\n","-------\n","ypred: [Value(data = 0.8573215165055211), Value(data = -0.8587661133208755)]\n","step: 21, loss: 0.04030416039877606\n","-------\n","ypred: [Value(data = 0.8608134157626794), Value(data = -0.8624309113381637)]\n","step: 22, loss: 0.0382981593869009\n","-------\n","ypred: [Value(data = 0.8640825772679861), Value(data = -0.8658596201934808)]\n","step: 23, loss: 0.036467187296750185\n","-------\n","ypred: [Value(data = 0.8671509566742499), Value(data = -0.869075388805548)]\n","step: 24, loss: 0.03479012212898545\n","-------\n","ypred: [Value(data = 0.8700376452758163), Value(data = -0.872098389804665)]\n","step: 25, loss: 0.033249035536013966\n","-------\n","ypred: [Value(data = 0.8727593286848907), Value(data = -0.8749462851588744)]\n","step: 26, loss: 0.03182862003228523\n","-------\n","ypred: [Value(data = 0.8753306592589697), Value(data = -0.8776346071152444)]\n","step: 27, loss: 0.030515733896643707\n","-------\n","ypred: [Value(data = 0.8777645607835044), Value(data = -0.8801770718104251)]\n","step: 28, loss: 0.0292990367203736\n","-------\n","ypred: [Value(data = 0.8800724794737506), Value(data = -0.8825858389901057)]\n","step: 29, loss: 0.028168695385231368\n","-------\n","ypred: [Value(data = 0.8822645920941801), Value(data = -0.8848717283161766)]\n","step: 30, loss: 0.027116145215654072\n","-------\n","ypred: [Value(data = 0.8843499795639934), Value(data = -0.8870444004864699)]\n","step: 31, loss: 0.026133894688309733\n","-------\n","ypred: [Value(data = 0.8863367725899872), Value(data = -0.889112509664874)]\n","step: 32, loss: 0.025215364778082946\n","-------\n","ypred: [Value(data = 0.8882322744804807), Value(data = -0.8910838323875345)]\n","step: 33, loss: 0.02435475603519328\n","-------\n","ypred: [Value(data = 0.8900430652308854), Value(data = -0.8929653770767849)]\n","step: 34, loss: 0.023546938008134163\n","-------\n","ypred: [Value(data = 0.8917750901501469), Value(data = -0.894763477486618)]\n","step: 35, loss: 0.022787356782718392\n","-------\n","ypred: [Value(data = 0.8934337356578106), Value(data = -0.8964838727685875)]\n","step: 36, loss: 0.02207195729283936\n","-------\n","ypred: [Value(data = 0.895023894379956), Value(data = -0.8981317763450168)]\n","step: 37, loss: 0.021397117741772304\n","-------\n","ypred: [Value(data = 0.8965500212762785), Value(data = -0.8997119353772202)]\n","step: 38, loss: 0.0207595940037213\n","-------\n","ypred: [Value(data = 0.8980161822159465), Value(data = -0.901228682297293)]\n","step: 39, loss: 0.020156472290540108\n","-------\n","ypred: [Value(data = 0.8994260961683244), Value(data = -0.90268597961551)]\n","step: 40, loss: 0.019585128695336074\n","-------\n","ypred: [Value(data = 0.9007831719724139), Value(data = -0.9040874590081521)]\n","step: 41, loss: 0.019043194483368497\n","-------\n","ypred: [Value(data = 0.9020905404853935), Value(data = -0.9054364555223765)]\n","step: 42, loss: 0.018528526206613848\n","-------\n","ypred: [Value(data = 0.9033510827778038), Value(data = -0.906736037597556)]\n","step: 43, loss: 0.018039179883227403\n","-------\n","ypred: [Value(data = 0.9045674549344878), Value(data = -0.9079890334901446)]\n","step: 44, loss: 0.01757338861575876\n","-------\n","ypred: [Value(data = 0.9057421099314262), Value(data = -0.9091980545966517)]\n","step: 45, loss: 0.017129543129211982\n","-------\n","ypred: [Value(data = 0.9068773169853276), Value(data = -0.9103655160929451)]\n","step: 46, loss: 0.016706174797135237\n","-------\n","ypred: [Value(data = 0.907975178712182), Value(data = -0.9114936552447319)]\n","step: 47, loss: 0.016301940794993204\n","-------\n","ypred: [Value(data = 0.9090376463806075), Value(data = -0.9125845476913299)]\n","step: 48, loss: 0.01591561107830879\n","-------\n","ypred: [Value(data = 0.9100665335038082), Value(data = -0.9136401219607629)]\n","step: 49, loss: 0.015546056930973558\n","-------\n","ypred: [Value(data = 0.9110635279787864), Value(data = -0.9146621724372445)]\n","step: 50, loss: 0.015192240868710705\n","-------\n","ypred: [Value(data = 0.912030202951907), Value(data = -0.9156523709710152)]\n","step: 51, loss: 0.014853207715493928\n","-------\n","ypred: [Value(data = 0.9129680265650055), Value(data = -0.9166122772942439)]\n","step: 52, loss: 0.014528076698041657\n","-------\n","ypred: [Value(data = 0.9138783707151721), Value(data = -0.9175433483844502)]\n","step: 53, loss: 0.014216034426321486\n","-------\n","ypred: [Value(data = 0.9147625189434634), Value(data = -0.9184469468980352)]\n","step: 54, loss: 0.01391632864711533\n","-------\n","ypred: [Value(data = 0.9156216735525987), Value(data = -0.9193243487803937)]\n","step: 55, loss: 0.01362826267377178\n","-------\n","ypred: [Value(data = 0.9164569620407075), Value(data = -0.9201767501453415)]\n","step: 56, loss: 0.013351190408827028\n","-------\n","ypred: [Value(data = 0.9172694429270909), Value(data = -0.9210052735048064)]\n","step: 57, loss: 0.013084511887644307\n","-------\n","ypred: [Value(data = 0.918060111036425), Value(data = -0.9218109734196085)]\n","step: 58, loss: 0.012827669280952164\n","-------\n","ypred: [Value(data = 0.9188299022996268), Value(data = -0.9225948416334421)]\n","step: 59, loss: 0.012580143302440044\n","-------\n","ypred: [Value(data = 0.9195796981225285), Value(data = -0.9233578117446469)]\n","step: 60, loss: 0.012341449974632629\n","-------\n","ypred: [Value(data = 0.9203103293673836), Value(data = -0.9241007634638385)]\n","step: 61, loss: 0.012111137712307082\n","-------\n","ypred: [Value(data = 0.9210225799869189), Value(data = -0.9248245264998258)]\n","step: 62, loss: 0.011888784687898005\n","-------\n","ypred: [Value(data = 0.9217171903460417), Value(data = -0.9255298841113251)]\n","step: 63, loss: 0.011673996447790536\n","-------\n","ypred: [Value(data = 0.9223948602622912), Value(data = -0.9262175763577031)]\n","step: 64, loss: 0.011466403752240683\n","-------\n","ypred: [Value(data = 0.9230562517926203), Value(data = -0.9268883030782478)]\n","step: 65, loss: 0.011265660614978803\n","-------\n","ypred: [Value(data = 0.9237019917910322), Value(data = -0.9275427266261881)]\n","step: 66, loss: 0.011071442521423034\n","-------\n","ypred: [Value(data = 0.924332674258907), Value(data = -0.9281814743808279)]\n","step: 67, loss: 0.010883444806920363\n","-------\n","ypred: [Value(data = 0.9249488625074993), Value(data = -0.9288051410586312)]\n","step: 68, loss: 0.010701381178599643\n","-------\n","ypred: [Value(data = 0.9255510911500102), Value(data = -0.9294142908418878)]\n","step: 69, loss: 0.010524982366307698\n","-------\n","ypred: [Value(data = 0.9261398679388118), Value(data = -0.9300094593416308)]\n","step: 70, loss: 0.010353994889746997\n","-------\n","ypred: [Value(data = 0.9267156754617856), Value(data = -0.9305911554097517)]\n","step: 71, loss: 0.01018817993037558\n","-------\n","ypred: [Value(data = 0.9272789727103122), Value(data = -0.9311598628137363)]\n","step: 72, loss: 0.010027312297891118\n","-------\n","ypred: [Value(data = 0.9278301965301797), Value(data = -0.9317160417860868)]\n","step: 73, loss: 0.009871179482231925\n","-------\n","ypred: [Value(data = 0.9283697629655598), Value(data = -0.9322601304592943)]\n","step: 74, loss: 0.009719580783001905\n","-------\n","ypred: [Value(data = 0.9288980685052002), Value(data = -0.9327925461961668)]\n","step: 75, loss: 0.009572326509085578\n","-------\n","ypred: [Value(data = 0.9294154912390963), Value(data = -0.9333136868243516)]\n","step: 76, loss: 0.009429237241978757\n","-------\n","ypred: [Value(data = 0.9299223919331085), Value(data = -0.9338239317830602)]\n","step: 77, loss: 0.009290143157029918\n","-------\n","ypred: [Value(data = 0.9304191150282891), Value(data = -0.9343236431892308)]\n","step: 78, loss: 0.009154883397381935\n","-------\n","ypred: [Value(data = 0.930905989571045), Value(data = -0.934813166829698)]\n","step: 79, loss: 0.009023305495929338\n","-------\n","ypred: [Value(data = 0.931383330079702), Value(data = -0.9352928330853243)]\n","step: 80, loss: 0.008895264841074835\n","-------\n","ypred: [Value(data = 0.9318514373525207), Value(data = -0.9357629577925107)]\n","step: 81, loss: 0.008770624182484164\n","-------\n","ypred: [Value(data = 0.9323105992217665), Value(data = -0.9362238430470065)]\n","step: 82, loss: 0.008649253173409184\n","-------\n","ypred: [Value(data = 0.9327610912580169), Value(data = -0.9366757779545073)]\n","step: 83, loss: 0.0085310279464796\n","-------\n","ypred: [Value(data = 0.9332031774285274), Value(data = -0.9371190393321323)]\n","step: 84, loss: 0.008415830720158718\n","-------\n","ypred: [Value(data = 0.9336371107131407), Value(data = -0.9375538923645055)]\n","step: 85, loss: 0.00830354943332371\n","-------\n","ypred: [Value(data = 0.9340631336809225), Value(data = -0.9379805912178643)]\n","step: 86, loss: 0.008194077405665549\n","-------\n","ypred: [Value(data = 0.9344814790304397), Value(data = -0.938399379615302)]\n","step: 87, loss: 0.008087313021818395\n","-------\n","ypred: [Value(data = 0.9348923700963446), Value(data = -0.9388104913760111)]\n","step: 88, loss: 0.007983159437316567\n","-------\n","ypred: [Value(data = 0.9352960213247151), Value(data = -0.9392141509211399)]\n","step: 89, loss: 0.007881524304649676\n","-------\n","ypred: [Value(data = 0.9356926387193886), Value(data = -0.939610573748663)]\n","step: 90, loss: 0.007782319517840756\n","-------\n","ypred: [Value(data = 0.9360824202613535), Value(data = -0.9399999668794782)]\n","step: 91, loss: 0.007685460974109953\n","-------\n","ypred: [Value(data = 0.9364655563030907), Value(data = -0.9403825292767477)]\n","step: 92, loss: 0.007590868351313583\n","-------\n","ypred: [Value(data = 0.9368422299396063), Value(data = -0.9407584522403551)]\n","step: 93, loss: 0.007498464899959844\n","-------\n","ypred: [Value(data = 0.9372126173577604), Value(data = -0.9411279197781925)]\n","step: 94, loss: 0.007408177248705943\n","-------\n","ypred: [Value(data = 0.9375768881653712), Value(data = -0.9414911089558596)]\n","step: 95, loss: 0.0073199352223336615\n","-------\n","ypred: [Value(data = 0.9379352057014572), Value(data = -0.9418481902262364)]\n","step: 96, loss: 0.0072336716712844175\n","-------\n","ypred: [Value(data = 0.938287727328878), Value(data = -0.9421993277402751)]\n","step: 97, loss: 0.007149322311911037\n","-------\n","ypred: [Value(data = 0.9386346047105347), Value(data = -0.9425446796402562)]\n","step: 98, loss: 0.0070668255766731134\n","-------\n","ypred: [Value(data = 0.9389759840702112), Value(data = -0.9428843983366608)]\n","step: 99, loss: 0.006986122473564348\n","-------\n","ypred: [Value(data = 0.9393120064390438), Value(data = -0.9432186307697221)]\n","step: 100, loss: 0.0069071564541198106\n","-------\n","ypred: [Value(data = 0.9396428078885504), Value(data = -0.9435475186566454)]\n","step: 101, loss: 0.006829873289400233\n","-------\n","ypred: [Value(data = 0.9399685197510672), Value(data = -0.9438711987254068)]\n","step: 102, loss: 0.006754220953400782\n","-------\n","ypred: [Value(data = 0.9402892688283905), Value(data = -0.9441898029359819)]\n","step: 103, loss: 0.006680149513372759\n","-------\n","ypred: [Value(data = 0.9406051775893554), Value(data = -0.9445034586897896)]\n","step: 104, loss: 0.0066076110265879074\n","-------\n","ypred: [Value(data = 0.9409163643570393), Value(data = -0.9448122890280812)]\n","step: 105, loss: 0.0065365594431101804\n","-------\n","ypred: [Value(data = 0.9412229434862225), Value(data = -0.9451164128199555)]\n","step: 106, loss: 0.00646695051417334\n","-------\n","ypred: [Value(data = 0.9415250255316991), Value(data = -0.9454159449406301)]\n","step: 107, loss: 0.006398741705792764\n","-------\n","ypred: [Value(data = 0.9418227174079875), Value(data = -0.9457109964405578)]\n","step: 108, loss: 0.00633189211726801\n","-------\n","ypred: [Value(data = 0.9421161225409529), Value(data = -0.9460016747059364)]\n","step: 109, loss: 0.006266362404257486\n","-------\n","ypred: [Value(data = 0.9424053410118207), Value(data = -0.9462880836111189)]\n","step: 110, loss: 0.006202114706130811\n","-------\n","ypred: [Value(data = 0.9426904696940256), Value(data = -0.9465703236634035)]\n","step: 111, loss: 0.006139112577324855\n","-------\n","ypred: [Value(data = 0.9429716023833116), Value(data = -0.9468484921406465)]\n","step: 112, loss: 0.006077320922450034\n","-------\n","ypred: [Value(data = 0.9432488299214731), Value(data = -0.9471226832221098)]\n","step: 113, loss: 0.0060167059349112385\n","-------\n","ypred: [Value(data = 0.9435222403140977), Value(data = -0.9473929881129362)]\n","step: 114, loss: 0.005957235038824208\n","-------\n","ypred: [Value(data = 0.9437919188426517), Value(data = -0.947659495162606)]\n","step: 115, loss: 0.005898876834024326\n","-------\n","ypred: [Value(data = 0.9440579481712245), Value(data = -0.9479222899777224)]\n","step: 116, loss: 0.005841601043977839\n","-------\n","ypred: [Value(data = 0.9443204084482301), Value(data = -0.9481814555294376)]\n","step: 117, loss: 0.005785378466419588\n","-------\n","ypred: [Value(data = 0.9445793774033419), Value(data = -0.9484370722558143)]\n","step: 118, loss: 0.00573018092655333\n","-------\n","ypred: [Value(data = 0.9448349304399248), Value(data = -0.948689218159408)]\n","step: 119, loss: 0.005675981232660759\n","-------\n","ypred: [Value(data = 0.9450871407232051), Value(data = -0.948937968900322)]\n","step: 120, loss: 0.005622753133977563\n","-------\n","ypred: [Value(data = 0.9453360792644102), Value(data = -0.9491833978849871)]\n","step: 121, loss: 0.00557047128070238\n","-------\n","ypred: [Value(data = 0.9455818150010931), Value(data = -0.9494255763508892)]\n","step: 122, loss: 0.005519111186014998\n","-------\n","ypred: [Value(data = 0.945824414873842), Value(data = -0.9496645734474644)]\n","step: 123, loss: 0.005468649189987308\n","-------\n","ypred: [Value(data = 0.9460639438995672), Value(data = -0.9499004563133617)]\n","step: 124, loss: 0.0054190624252784154\n","-------\n","ypred: [Value(data = 0.946300465241542), Value(data = -0.9501332901502663)]\n","step: 125, loss: 0.005370328784512372\n","-------\n","ypred: [Value(data = 0.9465340402763672), Value(data = -0.9503631382934551)]\n","step: 126, loss: 0.005322426889243786\n","-------\n","ypred: [Value(data = 0.9467647286580143), Value(data = -0.9505900622792636)]\n","step: 127, loss: 0.005275336060421895\n","-------\n","ypred: [Value(data = 0.9469925883791), Value(data = -0.9508141219096123)]\n","step: 128, loss: 0.005229036290269995\n","-------\n","ypred: [Value(data = 0.9472176758295288), Value(data = -0.9510353753137476)]\n","step: 129, loss: 0.00518350821550227\n","-------\n","ypred: [Value(data = 0.9474400458526369), Value(data = -0.9512538790073355)]\n","step: 130, loss: 0.005138733091804404\n","-------\n","ypred: [Value(data = 0.9476597517989624), Value(data = -0.9514696879490414)]\n","step: 131, loss: 0.005094692769509631\n","-------\n","ypred: [Value(data = 0.9478768455777586), Value(data = -0.9516828555947191)]\n","step: 132, loss: 0.005051369670405586\n","-------\n","ypred: [Value(data = 0.94809137770636), Value(data = -0.95189343394933)]\n","step: 133, loss: 0.005008746765611255\n","-------\n","ypred: [Value(data = 0.9483033973575076), Value(data = -0.9521014736166995)]\n","step: 134, loss: 0.0049668075544674894\n","-------\n","ypred: [Value(data = 0.9485129524047309), Value(data = -0.9523070238472185)]\n","step: 135, loss: 0.0049255360443872884\n","-------\n","ypred: [Value(data = 0.9487200894658804), Value(data = -0.9525101325835895)]\n","step: 136, loss: 0.0048849167316155585\n","-------\n","ypred: [Value(data = 0.9489248539448978), Value(data = -0.9527108465047078)]\n","step: 137, loss: 0.004844934582851327\n","-------\n","ypred: [Value(data = 0.9491272900719085), Value(data = -0.9529092110677685)]\n","step: 138, loss: 0.004805575017687712\n","-------\n","ypred: [Value(data = 0.949327440941715), Value(data = -0.9531052705486851)]\n","step: 139, loss: 0.004766823891827407\n","-------\n","ypred: [Value(data = 0.9495253485507639), Value(data = -0.9532990680808936)]\n","step: 140, loss: 0.004728667481034886\n","-------\n","ypred: [Value(data = 0.949721053832661), Value(data = -0.9534906456926229)]\n","step: 141, loss: 0.004691092465787309\n","-------\n","ypred: [Value(data = 0.9499145966922975), Value(data = -0.9536800443426972)]\n","step: 142, loss: 0.004654085916589719\n","-------\n","ypred: [Value(data = 0.9501060160386523), Value(data = -0.9538673039549429)]\n","step: 143, loss: 0.004617635279920849\n","-------\n","ypred: [Value(data = 0.9502953498163317), Value(data = -0.9540524634512595)]\n","step: 144, loss: 0.004581728364778684\n","-------\n","ypred: [Value(data = 0.9504826350359002), Value(data = -0.9542355607834165)]\n","step: 145, loss: 0.004546353329796216\n","-------\n","ypred: [Value(data = 0.9506679078030614), Value(data = -0.9544166329636349)]\n","step: 146, loss: 0.004511498670899226\n","-------\n","ypred: [Value(data = 0.9508512033467333), Value(data = -0.9545957160940054)]\n","step: 147, loss: 0.004477153209480316\n","-------\n","ypred: [Value(data = 0.9510325560460748), Value(data = -0.9547728453947958)]\n","step: 148, loss: 0.004443306081063846\n","-------\n","ypred: [Value(data = 0.9512119994565023), Value(data = -0.9549480552316973)]\n","step: 149, loss: 0.004409946724438529\n","-------\n","ypred: [Value(data = 0.9513895663347449), Value(data = -0.9551213791420561)]\n","step: 150, loss: 0.004377064871235244\n","-------\n","ypred: [Value(data = 0.9515652886629783), Value(data = -0.9552928498601324)]\n","step: 151, loss: 0.004344650535929282\n","-------\n","ypred: [Value(data = 0.9517391976720768), Value(data = -0.9554624993414313)]\n","step: 152, loss: 0.004312694006246879\n","-------\n","ypred: [Value(data = 0.9519113238640217), Value(data = -0.9556303587861454)]\n","step: 153, loss: 0.004281185833957193\n","-------\n","ypred: [Value(data = 0.952081697033501), Value(data = -0.955796458661745)]\n","step: 154, loss: 0.004250116826032007\n","-------\n","ypred: [Value(data = 0.9522503462887367), Value(data = -0.9559608287247531)]\n","step: 155, loss: 0.0042194780361560925\n","-------\n","ypred: [Value(data = 0.952417300071569), Value(data = -0.9561234980417413)]\n","step: 156, loss: 0.004189260756572184\n","-------\n","ypred: [Value(data = 0.9525825861768302), Value(data = -0.9562844950095766)]\n","step: 157, loss: 0.004159456510245468\n","-------\n","ypred: [Value(data = 0.9527462317710387), Value(data = -0.9564438473749506)]\n","step: 158, loss: 0.004130057043332994\n","-------\n","ypred: [Value(data = 0.9529082634104382), Value(data = -0.9566015822532238)]\n","step: 159, loss: 0.004101054317944373\n","-------\n","ypred: [Value(data = 0.9530687070584115), Value(data = -0.9567577261466089)]\n","step: 160, loss: 0.0040724405051808625\n","-------\n","ypred: [Value(data = 0.9532275881022932), Value(data = -0.956912304961722)]\n","step: 161, loss: 0.00404420797844039\n","-------\n","ypred: [Value(data = 0.9533849313696053), Value(data = -0.9570653440265305)]\n","step: 162, loss: 0.004016349306976581\n","-------\n","ypred: [Value(data = 0.9535407611437401), Value(data = -0.9572168681067149)]\n","step: 163, loss: 0.003988857249701238\n","-------\n","ypred: [Value(data = 0.9536951011791104), Value(data = -0.9573669014214747)]\n","step: 164, loss: 0.003961724749219077\n","-------\n","ypred: [Value(data = 0.953847974715791), Value(data = -0.9575154676587985)]\n","step: 165, loss: 0.003934944926084869\n","-------\n","ypred: [Value(data = 0.9539994044936684), Value(data = -0.9576625899902169)]\n","step: 166, loss: 0.003908511073273618\n","-------\n","ypred: [Value(data = 0.954149412766121), Value(data = -0.9578082910850633)]\n","step: 167, loss: 0.003882416650854302\n","-------\n","ypred: [Value(data = 0.9542980213132449), Value(data = -0.957952593124257)]\n","step: 168, loss: 0.0038566552808588983\n","-------\n","ypred: [Value(data = 0.9544452514546468), Value(data = -0.9580955178136319)]\n","step: 169, loss: 0.003831220742338005\n","-------\n","ypred: [Value(data = 0.9545911240618161), Value(data = -0.9582370863968248)]\n","step: 170, loss: 0.00380610696659565\n","-------\n","ypred: [Value(data = 0.9547356595700972), Value(data = -0.9583773196677411)]\n","step: 171, loss: 0.0037813080325955458\n","-------\n","ypred: [Value(data = 0.9548788779902725), Value(data = -0.9585162379826163)]\n","step: 172, loss: 0.0037568181625316417\n","-------\n","ypred: [Value(data = 0.9550207989197751), Value(data = -0.9586538612716851)]\n","step: 173, loss: 0.00373263171755637\n","-------\n","ypred: [Value(data = 0.9551614415535414), Value(data = -0.9587902090504762)]\n","step: 174, loss: 0.0037087431936599358\n","-------\n","ypred: [Value(data = 0.9553008246945205), Value(data = -0.9589253004307446)]\n","step: 175, loss: 0.0036851472176945756\n","-------\n","ypred: [Value(data = 0.9554389667638495), Value(data = -0.9590591541310569)]\n","step: 176, loss: 0.0036618385435378603\n","-------\n","ypred: [Value(data = 0.9555758858107115), Value(data = -0.9591917884870397)]\n","step: 177, loss: 0.0036388120483894504\n","-------\n","ypred: [Value(data = 0.9557115995218832), Value(data = -0.9593232214613077)]\n","step: 178, loss: 0.0036160627291958753\n","-------\n","ypred: [Value(data = 0.9558461252309874), Value(data = -0.9594534706530786)]\n","step: 179, loss: 0.0035935856991984004\n","-------\n","ypred: [Value(data = 0.955979479927459), Value(data = -0.9595825533074914)]\n","step: 180, loss: 0.003571376184598756\n","-------\n","ypred: [Value(data = 0.9561116802652354), Value(data = -0.9597104863246325)]\n","step: 181, loss: 0.0035494295213385507\n","-------\n","ypred: [Value(data = 0.9562427425711812), Value(data = -0.9598372862682879)]\n","step: 182, loss: 0.003527741151987369\n","-------\n","ypred: [Value(data = 0.9563726828532573), Value(data = -0.9599629693744239)]\n","step: 183, loss: 0.0035063066227357877\n","-------\n","ypred: [Value(data = 0.9565015168084419), Value(data = -0.9600875515594111)]\n","step: 184, loss: 0.0034851215804889363\n","-------\n","ypred: [Value(data = 0.9566292598304146), Value(data = -0.9602110484279993)]\n","step: 185, loss: 0.003464181770056706\n","-------\n","ypred: [Value(data = 0.9567559270170096), Value(data = -0.9603334752810507)]\n","step: 186, loss: 0.003443483031437212\n","-------\n","ypred: [Value(data = 0.9568815331774477), Value(data = -0.9604548471230434)]\n","step: 187, loss: 0.003423021297189409\n","-------\n","ypred: [Value(data = 0.9570060928393536), Value(data = -0.9605751786693483)]\n","step: 188, loss: 0.0034027925898920965\n","-------\n","ypred: [Value(data = 0.9571296202555677), Value(data = -0.9606944843532922)]\n","step: 189, loss: 0.0033827930196854246\n","-------\n","ypred: [Value(data = 0.9572521294107568), Value(data = -0.9608127783330122)]\n","step: 190, loss: 0.0033630187818923284\n","-------\n","ypred: [Value(data = 0.9573736340278337), Value(data = -0.9609300744981077)]\n","step: 191, loss: 0.003343466154716467\n","-------\n","ypred: [Value(data = 0.9574941475741898), Value(data = -0.9610463864760994)]\n","step: 192, loss: 0.0033241314970141635\n","-------\n","ypred: [Value(data = 0.957613683267748), Value(data = -0.9611617276386993)]\n","step: 193, loss: 0.003305011246137358\n","-------\n","ypred: [Value(data = 0.9577322540828423), Value(data = -0.9612761111079008)]\n","step: 194, loss: 0.0032861019158450418\n","-------\n","ypred: [Value(data = 0.9578498727559296), Value(data = -0.961389549761892)]\n","step: 195, loss: 0.003267400094280745\n","-------\n","ypred: [Value(data = 0.9579665517911383), Value(data = -0.9615020562408011)]\n","step: 196, loss: 0.0032489024420134993\n","-------\n","ypred: [Value(data = 0.9580823034656619), Value(data = -0.9616136429522782)]\n","step: 197, loss: 0.003230605690140036\n","-------\n","ypred: [Value(data = 0.9581971398349991), Value(data = -0.9617243220769175)]\n","step: 198, loss: 0.0032125066384461644\n","-------\n","ypred: [Value(data = 0.9583110727380482), Value(data = -0.961834105573528)]\n","step: 199, loss: 0.003194602153624918\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 1.8021128177642822\n","Epoch 10 loss: 0.22280243039131165\n","Epoch 20 loss: 0.014641683548688889\n","Epoch 30 loss: 0.0005923573044128716\n","\n","Prediction:\n","tensor([[ 0.9960],\n","        [-0.9949]])\n","Loss: 2.9800092306686565e-05\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[-0.2509,  0.4789, -0.5113],\n","        [ 0.4660, -0.0379,  0.5000],\n","        [-0.2433, -0.3306, -0.4590],\n","        [ 0.0012,  0.4755, -0.4421]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 1.4462, -1.4873],\n","        [ 0.3182,  1.6858],\n","        [-1.0195, -0.6287],\n","        [ 1.8710, -0.6928]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.0677],\n","        [-0.0262],\n","        [-0.0255],\n","        [-0.5505]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 1.3785, -1.5551],\n","        [ 0.2920,  1.6597],\n","        [-1.0450, -0.6542],\n","        [ 1.3205, -1.2433]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.8806, -0.9146],\n","        [ 0.2840,  0.9302],\n","        [-0.7798, -0.5745],\n","        [ 0.8669, -0.8464]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[ 0.6676, -0.3209, -0.3009,  0.4640],\n","        [ 0.0624,  0.1638, -0.0064, -0.2677],\n","        [-0.0611, -0.4283, -0.3186,  0.0769],\n","        [-0.6747, -0.1807,  0.0012, -0.1338]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.8806, -0.9146],\n","        [ 0.2840,  0.9302],\n","        [-0.7798, -0.5745],\n","        [ 0.8669, -0.8464]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 1.1336, -1.1289],\n","        [-0.1256,  0.3255],\n","        [ 0.1397, -0.2246],\n","        [-0.7624,  0.5616]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.1712],\n","        [ 0.4067],\n","        [ 0.1040],\n","        [-0.1397]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 0.9624, -1.3001],\n","        [ 0.2811,  0.7322],\n","        [ 0.2437, -0.1206],\n","        [-0.9021,  0.4219]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.7454, -0.8617],\n","        [ 0.2740,  0.6244],\n","        [ 0.2390, -0.1201],\n","        [-0.7173,  0.3985]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[ 0.8778,  0.0081,  0.1600, -0.4710]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.7454, -0.8617],\n","        [ 0.2740,  0.6244],\n","        [ 0.2390, -0.1201],\n","        [-0.7173,  0.3985]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 1.0326, -0.9583]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[-0.0366]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9960, -0.9949]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9960, -0.9949]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":211,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9960, -0.9949]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(4.2678e-05)"]},"execution_count":212,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9960, -0.9949]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-0.0040,  0.0051]])\n","squared_difference: tensor([[1.6251e-05, 2.6427e-05]])\n"]},{"data":{"text/plain":["tensor(2.1339e-05)"]},"execution_count":213,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference) / 2\n","loss"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":214,"metadata":{},"output_type":"execute_result"}],"source":["difference\n","len(squared_difference)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":215,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(2.1339e-05)"]},"execution_count":215,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":216,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(4.2678e-05)"]},"execution_count":216,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
