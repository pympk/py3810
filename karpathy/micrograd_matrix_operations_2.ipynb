{"cells":[{"cell_type":"markdown","metadata":{},"source":["### [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import math, random, torch\n","import numpy as np\n","# import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def plot_losses(losses):\n","  # import matplotlib.pyplot as plt\n","  \n","  # Create a list of iterations\n","  iterations = range(len(losses))\n","\n","  # Plot the loss as a function of iteration\n","  plt.plot(iterations, losses)\n","\n","  # Add a title to the plot\n","  plt.title('Loss vs. Iteration')\n","\n","  # Add labels to the x-axis and y-axis\n","  plt.xlabel('Iteration')\n","  plt.ylabel('Loss')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions<br>* limited to neural network with one output, e.g. MLP(2, [3, 1])<br>* neural network with multiple outputs, e.g.  MLP(2, [3, 3]), will produce errors in backward pass "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class Neuron:\n","    \n","    def __init__(self, nin):\n","        # random numbers evenly distributed between -1 and 1    \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]  \n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["### Basic Neuron Function\n","\n","<img src=\"..\\karpathy\\img\\Basic Neuron Function.png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Simple Neural Network MLP(2, [3, 3, 1])<br>* input layer: 2 nodes<br>* hidden layer 1: 3 nodes<br>* hidden layer 2: 3 nodes<br>*  output layer: 1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\MLP (2, [3, 3, 1]).png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Hidden Layer Matrix Operations<br>* Hidden layer with two inputs (X1, X2), and three neurons (b1, b2, b3)<br>* Two sets of inputs (X1, X2) are shown in different shades of gray<br>* Two sets of outputs (Y1, Y2, Y3) are shown in corresponding shades of gray<br>* Multiple sets of inputs are processed in one matrix operation \n","\n","<img src=\"..\\karpathy\\img\\Hidden Layer Matrix Operations.png\">"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(2, [3, 3, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0],\n","  [3.0, -1.0]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"markdown","metadata":{},"source":["### Parameters in MLP(2, [3, 3, 1])<br>* parameters in layer 1: 3 neurons * (2 inputs + 1 bias) = 9<br>* parameters in layer 2: 3 neurons * (3 neurons + 1 bias) = 12<br>* parameters in layer 3: 1 output * (3 neurons + 1 bias) = 4<br>* Total 25 parameters "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in MLP(2, [3, 3, 1]): 25\n","\n","i:  0,  -0.8421551440\n","i:  1,   0.0723389503\n","i:  2,  -0.9359519026\n","i:  3,  -0.1933517182\n","i:  4,  -0.6729403344\n","---\n","i: 20,   0.6802104046\n","i: 21,   0.2043608216\n","i: 22,   0.6523593721\n","i: 23,   0.2406822492\n","i: 24,   0.2602266220\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'Number of parameters in MLP(2, [3, 3, 1]): {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"markdown","metadata":{},"source":["### ---- Start: Manual Calculation of Output and Loss ----"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 2)\n","xs_mats:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n","\n","xs_mats_T[0].shape: (2, 2)\n","xs_mats_T:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 3\n","----\n","layer: 0, neuron 0\n","w0: -0.8421551,   w0.grad:  0.0000000\n","w1:  0.0723390,   w1.grad:  0.0000000\n","b:  -0.9359519\n","\n","layer: 0, neuron 1\n","w0: -0.1933517,   w0.grad:  0.0000000\n","w1: -0.6729403,   w1.grad:  0.0000000\n","b:  -0.6104558\n","\n","layer: 0, neuron 2\n","w0:  0.7965034,   w0.grad:  0.0000000\n","w1:  0.8431053,   w1.grad:  0.0000000\n","b:   0.0062005\n","\n","------\n","layer: 1, neuron_cnt: 3\n","----\n","layer: 1, neuron 0\n","w0: -0.0892851,   w0.grad:  0.0000000\n","w1:  0.7366116,   w1.grad:  0.0000000\n","w2:  0.8746133,   w2.grad:  0.0000000\n","b:  -0.4959429\n","\n","layer: 1, neuron 1\n","w0: -0.4706892,   w0.grad:  0.0000000\n","w1: -0.2386927,   w1.grad:  0.0000000\n","w2: -0.0204109,   w2.grad:  0.0000000\n","b:   0.3729192\n","\n","layer: 1, neuron 2\n","w0: -0.7393307,   w0.grad:  0.0000000\n","w1:  0.8598633,   w1.grad:  0.0000000\n","w2:  0.3541904,   w2.grad:  0.0000000\n","b:   0.6802104\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0:  0.2043608,   w0.grad:  0.0000000\n","w1:  0.6523594,   w1.grad:  0.0000000\n","w2:  0.2406822,   w2.grad:  0.0000000\n","b:   0.2602266\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(3, 2):\n","[[-0.84215514  0.07233895]\n"," [-0.19335172 -0.67294033]\n"," [ 0.79650338  0.84310525]]\n","b_mat(1, 3):\n","[[-0.9359519  -0.61045584  0.00620051]]\n","\n","i: 1\n","w_mat(3, 3):\n","[[-0.08928511  0.73661159  0.87461327]\n"," [-0.47068916 -0.2386927  -0.02041091]\n"," [-0.73933071  0.85986331  0.35419041]]\n","b_mat(1, 3):\n","[[-0.49594288  0.37291921  0.6802104 ]]\n","\n","i: 2\n","w_mat(1, 3):\n","[[0.20436082 0.65235937 0.24068225]]\n","b_mat(1, 1):\n","[[0.26022662]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (3, 2):\n","[[-0.84215514  0.07233895]\n"," [-0.19335172 -0.67294033]\n"," [ 0.79650338  0.84310525]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[-1.46729344 -2.59880438]\n"," [-2.40552444  0.09288518]\n"," [ 4.12232253  1.5464049 ]]\n","\n","bias (3, 1):\n","[[-0.9359519 ]\n"," [-0.61045584]\n"," [ 0.00620051]]\n","\n","weights_x_inputs_plus_bias (3, 2):\n","[[-2.40324534 -3.53475628]\n"," [-3.01598028 -0.51757066]\n"," [ 4.12852303  1.55260541]]\n","\n","output (3, 2):\n","[[-0.98377962 -0.99830015]\n"," [-0.99520993 -0.47582287]\n"," [ 0.99948129  0.91421435]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (3, 3):\n","[[-0.08928511  0.73661159  0.87461327]\n"," [-0.47068916 -0.2386927  -0.02041091]\n"," [-0.73933071  0.85986331  0.35419041]]\n","\n","input (3, 2):\n","[[-0.98377962 -0.99830015]\n"," [-0.99520993 -0.47582287]\n"," [ 0.99948129  0.91421435]]\n","\n","weights_x_inputs (3, 2):\n","[[0.2289133  0.5382207 ]\n"," [0.68020343 0.56480456]\n"," [0.22560066 0.65273729]]\n","\n","bias (3, 1):\n","[[-0.49594288]\n"," [ 0.37291921]\n"," [ 0.6802104 ]]\n","\n","weights_x_inputs_plus_bias (3, 2):\n","[[-0.26702958  0.04227783]\n"," [ 1.05312265  0.93772378]\n"," [ 0.90581107  1.33294769]]\n","\n","output (3, 2):\n","[[-0.26085869  0.04225266]\n"," [ 0.78301742  0.73417469]\n"," [ 0.71911562  0.86996792]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 3):\n","[[0.20436082 0.65235937 0.24068225]]\n","\n","input (3, 2):\n","[[-0.26085869  0.04225266]\n"," [ 0.78301742  0.73417469]\n"," [ 0.71911562  0.86996792]]\n","\n","weights_x_inputs (1, 2):\n","[[0.63057782 0.69696637]]\n","\n","bias (1, 1):\n","[[0.26022662]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[0.89080444 0.95719299]]\n","\n","output (1, 2):\n","[[0.71179083 0.74302218]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [0.71179083 0.74302218]\n","desired output:     [1.0, -1.0]\n","err_sq:             [0.08306452 3.03812631]\n","loss_sum:           3.12119083216846\n","loss_mean:          1.56059541608423\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","err_sq = ((yout - ys)**2)\n","loss_sum = err_sq.sum()\n","loss_mean = err_sq.mean()\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'err_sq:             {err_sq}')\n","print(f'loss_sum:           {loss_sum}')\n","print(f'loss_mean:          {loss_mean}')\n"]},{"cell_type":"markdown","metadata":{},"source":["### ---- End: Manual Calculation of Output and Loss ----"]},{"cell_type":"markdown","metadata":{},"source":["### TODO calculate w1 gradient by incrementing w1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [0.7117908323608235, 0.7430221765247834]\n","ys:                 [1.0, -1.0]\n","err_sq:             [0.08306452431126693, 3.0381263078571936]\n","loss_sum:           3.1211908321684607\n","loss_mean:          1.5605954160842304\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","err_sq_ = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","err_sq = [l.data for l in err_sq_]\n","loss_sum = sum(err_sq)\n","loss_len = len(err_sq)\n","loss_mean = loss_sum / loss_len\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'err_sq:             {err_sq}')\n","print(f'loss_sum:           {loss_sum}')\n","print(f'loss_mean:          {loss_mean}')\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0     -0.8421551440     0.0008272041           0.05000        -0.8421965042\n","  1      0.0723389503     0.0071636433           0.05000         0.0719807682\n","  2     -0.9359519026     0.0009520418           0.05000        -0.9359995046\n","  3     -0.1933517182     0.4657893189           0.05000        -0.2166411841\n","  4     -0.6729403344    -0.1570566959           0.05000        -0.6650874996\n","  5     -0.6104558430     0.1551000527           0.05000        -0.6182108457\n","  6      0.7965033847     0.1483484857           0.05000         0.7890859604\n","  7      0.8431052520    -0.0496685854           0.05000         0.8455886813\n","  8      0.0062005082     0.0494295779           0.05000         0.0037290293\n","  9     -0.0892851133    -0.2647079212           0.05000        -0.0760497173\n"," 10      0.7366115852    -0.0976631065           0.05000         0.7414947405\n"," 11      0.8746132668     0.2370734826           0.05000         0.8627595927\n"," 12     -0.4959428775     0.2643708633           0.05000        -0.5091614206\n"," 13     -0.4706891613    -0.3981703414           0.05000        -0.4507806442\n"," 14     -0.2386926988    -0.1520062258           0.05000        -0.2310923875\n"," 15     -0.0204109056     0.3575585556           0.05000        -0.0382888334\n"," 16      0.3729192143     0.3978043607           0.05000         0.3530289962\n"," 17     -0.7393307095    -0.0587125329           0.05000        -0.7363950829\n"," 18      0.8598633081    -0.0105897926           0.05000         0.8603927978\n"," 19      0.3541904108     0.0505096707           0.05000         0.3516649273\n"," 20      0.6802104046     0.0583317828           0.05000         0.6772938155\n"," 21      0.2043608216     0.1401584444           0.05000         0.1973528993\n"," 22      0.6523593721     0.9237129443           0.05000         0.6061737249\n"," 23      0.2406822492     1.1539212092           0.05000         0.1829861887\n"," 24      0.2602266220     1.2770839897           0.05000         0.1963724225\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.6209066339333676), Value(data = 0.6498726377302138)]\n","step: 0, loss: 2.8657915009265826\n","-------\n","ypred: [Value(data = 0.4956994324950637), Value(data = 0.5201449549011528)]\n","step: 1, loss: 2.5651597462972284\n","-------\n","ypred: [Value(data = 0.3460324132819894), Value(data = 0.36398307900012783)]\n","step: 2, loss: 2.288123444276448\n","-------\n","ypred: [Value(data = 0.20723393859381217), Value(data = 0.21835641005951392)]\n","step: 3, loss: 2.1128703700505858\n","-------\n","ypred: [Value(data = 0.11124478005410587), Value(data = 0.11671799306676169)]\n","step: 4, loss: 2.0369449170201306\n","-------\n","ypred: [Value(data = 0.05734227717175818), Value(data = 0.058527996534176986)]\n","step: 5, loss: 2.009085101854385\n","-------\n","ypred: [Value(data = 0.029870909229226156), Value(data = 0.027732429884213757)]\n","step: 6, loss: 1.9973844001954384\n","-------\n","ypred: [Value(data = 0.016479407417314886), Value(data = 0.011628390126521242)]\n","step: 7, loss: 1.9907047557441733\n","-------\n","ypred: [Value(data = 0.010206596139622288), Value(data = 0.0030273110675155005)]\n","step: 8, loss: 1.9857547690728432\n","-------\n","ypred: [Value(data = 0.0074707962528261194), Value(data = -0.0017994675033294982)]\n","step: 9, loss: 1.9815185233676356\n","-------\n","ypred: [Value(data = 0.006481084175330076), Value(data = -0.004744359257555037)]\n","step: 10, loss: 1.9776136265310822\n","-------\n","ypred: [Value(data = 0.006354941541441477), Value(data = -0.006765607082443563)]\n","step: 11, loss: 1.973845061473419\n","-------\n","ypred: [Value(data = 0.006664553794992306), Value(data = -0.008352248207488484)]\n","step: 12, loss: 1.9700805723224444\n","-------\n","ypred: [Value(data = 0.00720868314840276), Value(data = -0.009759289370730376)]\n","step: 13, loss: 1.9662112638034897\n","-------\n","ypred: [Value(data = 0.007899111377400111), Value(data = -0.011126239929051119)]\n","step: 14, loss: 1.9621354865626088\n","-------\n","ypred: [Value(data = 0.008704673039297001), Value(data = -0.012537124072565046)]\n","step: 15, loss: 1.9577493565890078\n","-------\n","ypred: [Value(data = 0.009624185068985979), Value(data = -0.01405138164066617)]\n","step: 16, loss: 1.9529389328449493\n","-------\n","ypred: [Value(data = 0.010673968371041846), Value(data = -0.015720527344280113)]\n","step: 17, loss: 1.9475720771501246\n","-------\n","ypred: [Value(data = 0.0118829282840924), Value(data = -0.017598176026339164)]\n","step: 18, loss: 1.9414886911631957\n","-------\n","ypred: [Value(data = 0.013291856671456645), Value(data = -0.019747485962834155)]\n","step: 19, loss: 1.9344879513870454\n","-------\n","ypred: [Value(data = 0.014955554080344536), Value(data = -0.02224843159860951)]\n","step: 20, loss: 1.9263106899485398\n","-------\n","ypred: [Value(data = 0.016947484096970056), Value(data = -0.025206724600823304)]\n","step: 21, loss: 1.9166141787867321\n","-------\n","ypred: [Value(data = 0.019367475704963426), Value(data = -0.028766281262364513)]\n","step: 22, loss: 1.904935084118192\n","-------\n","ypred: [Value(data = 0.0223538116610782), Value(data = -0.03312779111790039)]\n","step: 23, loss: 1.890633937882173\n","-------\n","ypred: [Value(data = 0.02610217059284342), Value(data = -0.03857726923120348)]\n","step: 24, loss: 1.8728106493629006\n","-------\n","ypred: [Value(data = 0.030895678109393098), Value(data = -0.04553068018925359)]\n","step: 25, loss: 1.8501748691670419\n","-------\n","ypred: [Value(data = 0.037153243263722505), Value(data = -0.05460391091768459)]\n","step: 26, loss: 1.8208476422097055\n","-------\n","ypred: [Value(data = 0.04550792271142442), Value(data = -0.0667207065554907)]\n","step: 27, loss: 1.7820653651789424\n","-------\n","ypred: [Value(data = 0.056933037281577635), Value(data = -0.08327004081583934)]\n","step: 28, loss: 1.7297691142367433\n","-------\n","ypred: [Value(data = 0.07293681234353579), Value(data = -0.10630228843177485)]\n","step: 29, loss: 1.6581417535700473\n","-------\n","ypred: [Value(data = 0.09582631034935499), Value(data = -0.13866629390104745)]\n","step: 30, loss: 1.5594258143186177\n","-------\n","ypred: [Value(data = 0.12893258760173634), Value(data = -0.1837662231266587)]\n","step: 31, loss: 1.4249960154511263\n","-------\n","ypred: [Value(data = 0.17635274088404748), Value(data = -0.24432364925732641)]\n","step: 32, loss: 1.2494415545209852\n","-------\n","ypred: [Value(data = 0.24121275951383925), Value(data = -0.3198690590142609)]\n","step: 33, loss: 1.0383361732107497\n","-------\n","ypred: [Value(data = 0.3218980929933373), Value(data = -0.4046249714191174)]\n","step: 34, loss: 0.8142936209437595\n","-------\n","ypred: [Value(data = 0.40944091014187606), Value(data = -0.4888221587986341)]\n","step: 35, loss: 0.6100628239493444\n","-------\n","ypred: [Value(data = 0.49168871348062504), Value(data = -0.5634403891324139)]\n","step: 36, loss: 0.4489646578438403\n","-------\n","ypred: [Value(data = 0.5609804143304534), Value(data = -0.6242675319217024)]\n","step: 37, loss: 0.3339130841696693\n","-------\n","ypred: [Value(data = 0.6161211330442353), Value(data = -0.6718164633186328)]\n","step: 38, loss: 0.255067418243932\n","-------\n","ypred: [Value(data = 0.6592954522298917), Value(data = -0.7086766968878518)]\n","step: 39, loss: 0.20094885580740657\n","-------\n","ypred: [Value(data = 0.693302165751903), Value(data = -0.7375398391614315)]\n","step: 40, loss: 0.16294889755988046\n","-------\n","ypred: [Value(data = 0.7204939390904352), Value(data = -0.7605355926169859)]\n","step: 41, loss: 0.13546684048847946\n","-------\n","ypred: [Value(data = 0.7426199215634847), Value(data = -0.7792047443703252)]\n","step: 42, loss: 0.11499504968456024\n","-------\n","ypred: [Value(data = 0.7609338082587769), Value(data = -0.7946351468256782)]\n","step: 43, loss: 0.09932736695296203\n","-------\n","ypred: [Value(data = 0.7763310121669514), Value(data = -0.8075965163837621)]\n","step: 44, loss: 0.08704691662592437\n","-------\n","ypred: [Value(data = 0.7894574415423495), Value(data = -0.8186405951498105)]\n","step: 45, loss: 0.0772194026495081\n","-------\n","ypred: [Value(data = 0.8007861399621256), Value(data = -0.8281696521528467)]\n","step: 46, loss: 0.06921183047246354\n","-------\n","ypred: [Value(data = 0.8106693896080079), Value(data = -0.8364822704189417)]\n","step: 47, loss: 0.06258412791874839\n","-------\n","ypred: [Value(data = 0.8193739375173392), Value(data = -0.8438039666872763)]\n","step: 48, loss: 0.05702297527061956\n","-------\n","ypred: [Value(data = 0.8271049826770405), Value(data = -0.8503078838082468)]\n","step: 49, loss: 0.052300416665071806\n","-------\n","ypred: [Value(data = 0.8340227179662706), Value(data = -0.8561289811682824)]\n","step: 50, loss: 0.0482473282109806\n","-------\n","ypred: [Value(data = 0.8402539083696567), Value(data = -0.8613739247766313)]\n","step: 51, loss: 0.044736002523005094\n","-------\n","ypred: [Value(data = 0.8459001208194771), Value(data = -0.8661280936872945)]\n","step: 52, loss: 0.04166846006324956\n","-------\n","ypred: [Value(data = 0.8510436635386464), Value(data = -0.870460622839128)]\n","step: 53, loss: 0.03896844040721466\n","-------\n","ypred: [Value(data = 0.8557519353030386), Value(data = -0.874428087715949)]\n","step: 54, loss: 0.03657580932349215\n","-------\n","ypred: [Value(data = 0.8600806545372075), Value(data = -0.8780772350276218)]\n","step: 55, loss: 0.03444258385324604\n","-------\n","ypred: [Value(data = 0.8640762879711921), Value(data = -0.8814470333637936)]\n","step: 56, loss: 0.03253006138993578\n","-------\n","ypred: [Value(data = 0.8677778994977736), Value(data = -0.8845702322718533)]\n","step: 57, loss: 0.03080671513899476\n","-------\n","ypred: [Value(data = 0.8712185736108061), Value(data = -0.8874745612709898)]\n","step: 58, loss: 0.029246630143991592\n","-------\n","ypred: [Value(data = 0.8744265228920368), Value(data = -0.8901836618471933)]\n","step: 59, loss: 0.027828326278275736\n","-------\n","ypred: [Value(data = 0.8774259581587324), Value(data = -0.8927178191214604)]\n","step: 60, loss: 0.02653386206736051\n","-------\n","ypred: [Value(data = 0.880237778410047), Value(data = -0.8950945415862711)]\n","step: 61, loss: 0.0253481449251556\n","-------\n","ypred: [Value(data = 0.8828801225770107), Value(data = -0.8973290244375952)]\n","step: 62, loss: 0.024258394910511974\n","-------\n","ypred: [Value(data = 0.8853688142892284), Value(data = -0.8994345228694849)]\n","step: 63, loss: 0.02325372392794556\n","-------\n","ypred: [Value(data = 0.8877177230880822), Value(data = -0.9014226551019007)]\n","step: 64, loss: 0.022324802635683427\n","-------\n","ypred: [Value(data = 0.8899390598422972), Value(data = -0.903303650110249)]\n","step: 65, loss: 0.021463594630398587\n","-------\n","ypred: [Value(data = 0.8920436199433867), Value(data = -0.9050865514929333)]\n","step: 66, loss: 0.020663142702431553\n","-------\n","ypred: [Value(data = 0.894040984751811), Value(data = -0.9067793862897903)]\n","step: 67, loss: 0.019917395732874085\n","-------\n","ypred: [Value(data = 0.8959396894337671), Value(data = -0.9083893055978112)]\n","step: 68, loss: 0.01922106756399207\n","-------\n","ypred: [Value(data = 0.8977473635645247), Value(data = -0.9099227023436041)]\n","step: 69, loss: 0.01856952121108445\n","-------\n","ypred: [Value(data = 0.8994708495283652), Value(data = -0.9113853104384549)]\n","step: 70, loss: 0.017958673300637593\n","-------\n","ypred: [Value(data = 0.9011163027103596), Value(data = -0.9127822886708298)]\n","step: 71, loss: 0.01738491475916771\n","-------\n","ypred: [Value(data = 0.9026892766737288), Value(data = -0.9141182920169574)]\n","step: 72, loss: 0.016845044640366715\n","-------\n","ypred: [Value(data = 0.9041947958914133), Value(data = -0.9153975325245843)]\n","step: 73, loss: 0.01633621463721673\n","-------\n","ypred: [Value(data = 0.9056374181096902), Value(data = -0.9166238315126073)]\n","step: 74, loss: 0.015855882332643515\n","-------\n","ypred: [Value(data = 0.9070212880339301), Value(data = -0.9178006645036075)]\n","step: 75, loss: 0.015401771634917873\n","-------\n","ypred: [Value(data = 0.9083501837183984), Value(data = -0.9189312000475978)]\n","step: 76, loss: 0.014971839150173943\n","-------\n","ypred: [Value(data = 0.9096275567956653), Value(data = -0.9200183333885437)]\n","step: 77, loss: 0.014564245484666848\n","-------\n","ypred: [Value(data = 0.9108565674831972), Value(data = -0.9210647157591376)]\n","step: 78, loss: 0.01417733065906351\n","-------\n","ypred: [Value(data = 0.9120401151447514), Value(data = -0.9220727799551574)]\n","step: 79, loss: 0.013809592967665926\n","-------\n","ypred: [Value(data = 0.9131808650543343), Value(data = -0.9230447627318584)]\n","step: 80, loss: 0.013459670735709682\n","-------\n","ypred: [Value(data = 0.9142812719045398), Value(data = -0.9239827244760651)]\n","step: 81, loss: 0.013126326524385272\n","-------\n","ypred: [Value(data = 0.9153436005142742), Value(data = -0.9248885665348712)]\n","step: 82, loss: 0.012808433411073268\n","-------\n","ypred: [Value(data = 0.9163699441194244), Value(data = -0.9257640465219978)]\n","step: 83, loss: 0.012504963035376297\n","-------\n","ypred: [Value(data = 0.9173622405709694), Value(data = -0.9266107918734092)]\n","step: 84, loss: 0.012214975152898398\n","-------\n","ypred: [Value(data = 0.9183222867160389), Value(data = -0.9274303118827263)]\n","step: 85, loss: 0.011937608480735336\n","-------\n","ypred: [Value(data = 0.9192517511966032), Value(data = -0.9282240084128304)]\n","step: 86, loss: 0.01167207265313672\n","-------\n","ypred: [Value(data = 0.9201521858663543), Value(data = -0.9289931854514528)]\n","step: 87, loss: 0.011417641134253004\n","-------\n","ypred: [Value(data = 0.921025035997709), Value(data = -0.9297390576546039)]\n","step: 88, loss: 0.011173644958426238\n","-------\n","ypred: [Value(data = 0.9218716494267567), Value(data = -0.9304627580015028)]\n","step: 89, loss: 0.01093946718805317\n","-------\n","ypred: [Value(data = 0.9226932847636165), Value(data = -0.9311653446676394)]\n","step: 90, loss: 0.01071453799536417\n","-------\n","ypred: [Value(data = 0.9234911187784176), Value(data = -0.9318478072081653)]\n","step: 91, loss: 0.01049833028811361\n","-------\n","ypred: [Value(data = 0.9242662530584531), Value(data = -0.932511072131534)]\n","step: 92, loss: 0.010290355810641277\n","-------\n","ypred: [Value(data = 0.9250197200195565), Value(data = -0.9331560079328641)]\n","step: 93, loss: 0.010090161661417026\n","-------\n","ypred: [Value(data = 0.9257524883440713), Value(data = -0.9337834296475519)]\n","step: 94, loss: 0.009897327176337976\n","-------\n","ypred: [Value(data = 0.9264654679086194), Value(data = -0.9343941029780068)]\n","step: 95, loss: 0.009711461133958654\n","-------\n","ypred: [Value(data = 0.9271595142570063), Value(data = -0.9349887480397888)]\n","step: 96, loss: 0.009532199244709334\n","-------\n","ypred: [Value(data = 0.9278354326668162), Value(data = -0.9355680427677645)]\n","step: 97, loss: 0.009359201891162239\n","-------\n","ypred: [Value(data = 0.9284939818523936), Value(data = -0.9361326260179881)]\n","step: 98, loss: 0.009192152090683987\n","-------\n","ypred: [Value(data = 0.9291358773418299), Value(data = -0.9366831003967794)]\n","step: 99, loss: 0.009030753655476497\n","-------\n","ypred: [Value(data = 0.9297617945611784), Value(data = -0.9372200348447798)]\n","step: 100, loss: 0.00887472952815678\n","-------\n","ypred: [Value(data = 0.9303723716552813), Value(data = -0.9377439670005734)]\n","step: 101, loss: 0.00872382027373597\n","-------\n","ypred: [Value(data = 0.9309682120712633), Value(data = -0.9382554053656578)]\n","step: 102, loss: 0.008577782711197315\n","-------\n","ypred: [Value(data = 0.9315498869278213), Value(data = -0.9387548312901192)]\n","step: 103, loss: 0.008436388669895822\n","-------\n","ypred: [Value(data = 0.9321179371908919), Value(data = -0.9392427007962282)]\n","step: 104, loss: 0.008299423857756354\n","-------\n","ypred: [Value(data = 0.9326728756740382), Value(data = -0.9397194462552931)]\n","step: 105, loss: 0.008166686829772007\n","-------\n","ypred: [Value(data = 0.9332151888799256), Value(data = -0.940185477931474)]\n","step: 106, loss: 0.008037988046630196\n","-------\n","ypred: [Value(data = 0.9337453386975266), Value(data = -0.9406411854048012)]\n","step: 107, loss: 0.007913149014452643\n","-------\n","ypred: [Value(data = 0.9342637639681672), Value(data = -0.9410869388843666)]\n","step: 108, loss: 0.007792001497647188\n","-------\n","ypred: [Value(data = 0.9347708819321722), Value(data = -0.9415230904215298)]\n","step: 109, loss: 0.0076743867977552005\n","-------\n","ypred: [Value(data = 0.9352670895666748), Value(data = -0.9419499750319774)]\n","step: 110, loss: 0.007560155091956946\n","-------\n","ypred: [Value(data = 0.9357527648240951), Value(data = -0.9423679117345855)]\n","step: 111, loss: 0.007449164825580567\n","-------\n","ypred: [Value(data = 0.9362282677798484), Value(data = -0.9427772045142538)]\n","step: 112, loss: 0.007341282153562259\n","-------\n","ypred: [Value(data = 0.9366939416970105), Value(data = -0.9431781432151769)]\n","step: 113, loss: 0.0072363804263364554\n","-------\n","ypred: [Value(data = 0.9371501140149169), Value(data = -0.9435710043703882)]\n","step: 114, loss: 0.007134339716104695\n","-------\n","ypred: [Value(data = 0.9375970972680107), Value(data = -0.9439560519728657)]\n","step: 115, loss: 0.007035046379846253\n","-------\n","ypred: [Value(data = 0.938035189940655), Value(data = -0.9443335381929834)]\n","step: 116, loss: 0.006938392655802738\n","-------\n","ypred: [Value(data = 0.9384646772631006), Value(data = -0.9447037040466486)]\n","step: 117, loss: 0.006844276290494998\n","-------\n","ypred: [Value(data = 0.9388858319533108), Value(data = -0.94506678001807)]\n","step: 118, loss: 0.006752600193622086\n","-------\n","ypred: [Value(data = 0.939298914908931), Value(data = -0.9454229866407354)]\n","step: 119, loss: 0.006663272118450539\n","-------\n","ypred: [Value(data = 0.9397041758532961), Value(data = -0.9457725350398667)]\n","step: 120, loss: 0.0065762043655327234\n","-------\n","ypred: [Value(data = 0.9401018539390252), Value(data = -0.9461156274393204)]\n","step: 121, loss: 0.006491313507799985\n","-------\n","ypred: [Value(data = 0.940492178312445), Value(data = -0.9464524576356474)]\n","step: 122, loss: 0.006408520135259969\n","-------\n","ypred: [Value(data = 0.940875368641797), Value(data = -0.9467832114417895)]\n","step: 123, loss: 0.0063277486176926915\n","-------\n","ypred: [Value(data = 0.9412516356119331), Value(data = -0.9471080671026736)]\n","step: 124, loss: 0.006248926883888362\n","-------\n","ypred: [Value(data = 0.941621181387975), Value(data = -0.9474271956847803)]\n","step: 125, loss: 0.006171986216102099\n","-------\n","ypred: [Value(data = 0.9419842000502007), Value(data = -0.9477407614415806)]\n","step: 126, loss: 0.006096861058520926\n","-------\n","ypred: [Value(data = 0.9423408780022395), Value(data = -0.9480489221565847)]\n","step: 127, loss: 0.006023488838645227\n","-------\n","ypred: [Value(data = 0.942691394354481), Value(data = -0.948351829465599)]\n","step: 128, loss: 0.005951809800584174\n","-------\n","ypred: [Value(data = 0.9430359212844518), Value(data = -0.9486496291596618)]\n","step: 129, loss: 0.0058817668493514295\n","-------\n","ypred: [Value(data = 0.943374624375769), Value(data = -0.9489424614700032)]\n","step: 130, loss: 0.005813305405327363\n","-------\n","ypred: [Value(data = 0.9437076629371574), Value(data = -0.949230461336277)]\n","step: 131, loss: 0.005746373268123954\n","-------\n","ypred: [Value(data = 0.9440351903028917), Value(data = -0.9495137586592064)]\n","step: 132, loss: 0.0056809204891544\n","-------\n","ypred: [Value(data = 0.9443573541159291), Value(data = -0.949792478538699)]\n","step: 133, loss: 0.0056168992522671116\n","-------\n","ypred: [Value(data = 0.9446742965948888), Value(data = -0.950066741498408)]\n","step: 134, loss: 0.005554263761857148\n","-------\n","ypred: [Value(data = 0.944986154785956), Value(data = -0.950336663697636)]\n","step: 135, loss: 0.00549297013791649\n","-------\n","ypred: [Value(data = 0.9452930608007025), Value(data = -0.9506023571314154)]\n","step: 136, loss: 0.005432976317527859\n","-------\n","ypred: [Value(data = 0.9455951420407387), Value(data = -0.9508639298195342)]\n","step: 137, loss: 0.005374241962347063\n","-------\n","ypred: [Value(data = 0.9458925214100502), Value(data = -0.9511214859852201)]\n","step: 138, loss: 0.005316728371654915\n","-------\n","ypred: [Value(data = 0.9461853175158048), Value(data = -0.9513751262241411)]\n","step: 139, loss: 0.005260398400592954\n","-------\n","ypred: [Value(data = 0.9464736448583595), Value(data = -0.9516249476643382)]\n","step: 140, loss: 0.005205216383227038\n","-------\n","ypred: [Value(data = 0.9467576140111457), Value(data = -0.9518710441176544)]\n","step: 141, loss: 0.0051511480601109175\n","-------\n","ypred: [Value(data = 0.9470373317910608), Value(data = -0.9521135062231899)]\n","step: 142, loss: 0.005098160510046655\n","-------\n","ypred: [Value(data = 0.9473129014199515), Value(data = -0.9523524215832735)]\n","step: 143, loss: 0.00504622208576185\n","-------\n","ypred: [Value(data = 0.9475844226777307), Value(data = -0.9525878748924029)]\n","step: 144, loss: 0.00499530235324523\n","-------\n","ypred: [Value(data = 0.9478519920476347), Value(data = -0.9528199480595846)]\n","step: 145, loss: 0.00494537203450025\n","-------\n","ypred: [Value(data = 0.9481157028540921), Value(data = -0.9530487203244595)]\n","step: 146, loss: 0.004896402953495688\n","-------\n","ypred: [Value(data = 0.9483756453936404), Value(data = -0.9532742683675909)]\n","step: 147, loss: 0.004848367985107081\n","-------\n","ypred: [Value(data = 0.9486319070593026), Value(data = -0.953496666415252)]\n","step: 148, loss: 0.004801241006858482\n","-------\n","ypred: [Value(data = 0.9488845724588025), Value(data = -0.9537159863390349)]\n","step: 149, loss: 0.004754996853287815\n","-------\n","ypred: [Value(data = 0.9491337235269772), Value(data = -0.9539322977505817)]\n","step: 150, loss: 0.004709611272771046\n","-------\n","ypred: [Value(data = 0.9493794396327169), Value(data = -0.954145668091714)]\n","step: 151, loss: 0.0046650608866530135\n","-------\n","ypred: [Value(data = 0.9496217976807445), Value(data = -0.9543561627202236)]\n","step: 152, loss: 0.004621323150542545\n","-------\n","ypred: [Value(data = 0.9498608722085267), Value(data = -0.9545638449915674)]\n","step: 153, loss: 0.004578376317640006\n","-------\n","ypred: [Value(data = 0.9500967354785859), Value(data = -0.9547687763366975)]\n","step: 154, loss: 0.004536199403973923\n","-------\n","ypred: [Value(data = 0.9503294575664698), Value(data = -0.9549710163362353)]\n","step: 155, loss: 0.004494772155432715\n","-------\n","ypred: [Value(data = 0.950559106444617), Value(data = -0.9551706227911945)]\n","step: 156, loss: 0.00445407501648408\n","-------\n","ypred: [Value(data = 0.9507857480623412), Value(data = -0.9553676517904414)]\n","step: 157, loss: 0.004414089100482648\n","-------\n","ypred: [Value(data = 0.951009446422145), Value(data = -0.9555621577750619)]\n","step: 158, loss: 0.004374796161473168\n","-------\n","ypred: [Value(data = 0.9512302636525592), Value(data = -0.9557541935998095)]\n","step: 159, loss: 0.004336178567402021\n","-------\n","ypred: [Value(data = 0.9514482600776936), Value(data = -0.9559438105917831)]\n","step: 160, loss: 0.004298219274655965\n","-------\n","ypred: [Value(data = 0.9516634942836706), Value(data = -0.9561310586064822)]\n","step: 161, loss: 0.004260901803852648\n","-------\n","ypred: [Value(data = 0.9518760231821074), Value(data = -0.9563159860813778)]\n","step: 162, loss: 0.004224210216811442\n","-------\n","ypred: [Value(data = 0.9520859020707982), Value(data = -0.9564986400871264)]\n","step: 163, loss: 0.004188129094638505\n","-------\n","ypred: [Value(data = 0.9522931846917416), Value(data = -0.9566790663765482)]\n","step: 164, loss: 0.004152643516863792\n","-------\n","ypred: [Value(data = 0.9524979232866481), Value(data = -0.9568573094314855)]\n","step: 165, loss: 0.004117739041571762\n","-------\n","ypred: [Value(data = 0.9527001686500568), Value(data = -0.9570334125076453)]\n","step: 166, loss: 0.004083401686471242\n","-------\n","ypred: [Value(data = 0.952899970180181), Value(data = -0.9572074176775313)]\n","step: 167, loss: 0.004049617910853101\n","-------\n","ypred: [Value(data = 0.9530973759275959), Value(data = -0.957379365871557)]\n","step: 168, loss: 0.004016374598387856\n","-------\n","ypred: [Value(data = 0.9532924326418758), Value(data = -0.9575492969174345)]\n","step: 169, loss: 0.0039836590407178445\n","-------\n","ypred: [Value(data = 0.9534851858162818), Value(data = -0.9577172495779165)]\n","step: 170, loss: 0.003951458921802032\n","-------\n","ypred: [Value(data = 0.9536756797305963), Value(data = -0.9578832615869812)]\n","step: 171, loss: 0.003919762302972937\n","-------\n","ypred: [Value(data = 0.9538639574921907), Value(data = -0.9580473696845266)]\n","step: 172, loss: 0.0038885576086691645\n","-------\n","ypred: [Value(data = 0.9540500610754168), Value(data = -0.9582096096496525)]\n","step: 173, loss: 0.0038578336128073437\n","-------\n","ypred: [Value(data = 0.9542340313593968), Value(data = -0.9583700163325928)]\n","step: 174, loss: 0.00382757942576127\n","-------\n","ypred: [Value(data = 0.9544159081642913), Value(data = -0.9585286236853666)]\n","step: 175, loss: 0.0037977844819162554\n","-------\n","ypred: [Value(data = 0.9545957302861164), Value(data = -0.9586854647912033)]\n","step: 176, loss: 0.003768438527769985\n","-------\n","ypred: [Value(data = 0.9547735355301759), Value(data = -0.9588405718928039)]\n","step: 177, loss: 0.003739531610551709\n","-------\n","ypred: [Value(data = 0.954949360743176), Value(data = -0.9589939764194881)]\n","step: 178, loss: 0.0037110540673339827\n","-------\n","ypred: [Value(data = 0.9551232418440813), Value(data = -0.9591457090132833)]\n","step: 179, loss: 0.0036829965146121376\n","-------\n","ypred: [Value(data = 0.9552952138537694), Value(data = -0.9592957995539984)]\n","step: 180, loss: 0.003655349838328495\n","-------\n","ypred: [Value(data = 0.955465310923541), Value(data = -0.959444277183333)]\n","step: 181, loss: 0.0036281051843191957\n","-------\n","ypred: [Value(data = 0.9556335663625355), Value(data = -0.9595911703280636)]\n","step: 182, loss: 0.003601253949163108\n","-------\n","ypred: [Value(data = 0.9558000126641004), Value(data = -0.9597365067223489)]\n","step: 183, loss: 0.0035747877714131394\n","-------\n","ypred: [Value(data = 0.9559646815311648), Value(data = -0.959880313429191)]\n","step: 184, loss: 0.0035486985231916934\n","-------\n","ypred: [Value(data = 0.956127603900656), Value(data = -0.9600226168610941)]\n","step: 185, loss: 0.0035229783021326106\n","-------\n","ypred: [Value(data = 0.9562888099670066), Value(data = -0.960163442799953)]\n","step: 186, loss: 0.0034976194236530757\n","-------\n","ypred: [Value(data = 0.9564483292047872), Value(data = -0.9603028164162036)]\n","step: 187, loss: 0.0034726144135402257\n","-------\n","ypred: [Value(data = 0.9566061903905061), Value(data = -0.9604407622872733)]\n","step: 188, loss: 0.003447956000837027\n","-------\n","ypred: [Value(data = 0.9567624216236104), Value(data = -0.9605773044153532)]\n","step: 189, loss: 0.0034236371110141703\n","-------\n","ypred: [Value(data = 0.956917050346722), Value(data = -0.960712466244528)]\n","step: 190, loss: 0.0033996508594142365\n","-------\n","ypred: [Value(data = 0.9570701033651438), Value(data = -0.9608462706772889)]\n","step: 191, loss: 0.003375990544955569\n","-------\n","ypred: [Value(data = 0.9572216068656633), Value(data = -0.9609787400904527)]\n","step: 192, loss: 0.003352649644084312\n","-------\n","ypred: [Value(data = 0.9573715864346865), Value(data = -0.9611098963505157)]\n","step: 193, loss: 0.003329621804963041\n","-------\n","ypred: [Value(data = 0.9575200670757281), Value(data = -0.9612397608284651)]\n","step: 194, loss: 0.003306900841885229\n","-------\n","ypred: [Value(data = 0.9576670732262854), Value(data = -0.9613683544140683)]\n","step: 195, loss: 0.003284480729905716\n","-------\n","ypred: [Value(data = 0.957812628774121), Value(data = -0.9614956975296649)]\n","step: 196, loss: 0.0032623555996771775\n","-------\n","ypred: [Value(data = 0.9579567570729789), Value(data = -0.9616218101434788)]\n","step: 197, loss: 0.003240519732483697\n","-------\n","ypred: [Value(data = 0.9580994809577573), Value(data = -0.9617467117824707)]\n","step: 198, loss: 0.003218967555462705\n","-------\n","ypred: [Value(data = 0.9582408227591592), Value(data = -0.9618704215447523)]\n","step: 199, loss: 0.003197693637006841\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIFklEQVR4nO3deXxU1f3/8fdM9pBkQsgKBMImyI4oGFRAQSJFC2orUluUr1oXXCha/WHrRrVxqVRbrdhaQG1tlVahUhUDCsqiyKZANbKHJQkGyE7WOb8/QgaGJJCEmbnJ5PV8POaRmXvPvfO5c0nmzbnn3mszxhgBAAD4CbvVBQAAAHgS4QYAAPgVwg0AAPArhBsAAOBXCDcAAMCvEG4AAIBfIdwAAAC/QrgBAAB+hXADAAD8CuEGAFqIFStWyGazacWKFVaXArRqhBugFVuwYIFsNpvWr19vdSktTn2fzfvvv6/HHnvMuqKO+9Of/qQFCxZYXQbgtwg3ANqM999/X48//rjVZTQYbkaOHKljx45p5MiRvi8K8COEGwA4C8YYHTt2zCPrstvtCg0Nld3On2bgbPAbBLQBmzZt0vjx4xUVFaWIiAiNGTNGn3/+uVubyspKPf744+rVq5dCQ0PVoUMHXXzxxcrIyHC1ycnJ0bRp09S5c2eFhIQoKSlJEydO1J49exp879/97ney2Wzau3dvnXmzZs1ScHCwjh49Kknavn27rr32WiUmJio0NFSdO3fW9ddfr4KCgrP+DG666Sa99NJLkiSbzeZ61HI6nXr++efVr18/hYaGKiEhQbfddpurtlopKSm68sortXTpUp1//vkKCwvTK6+8IkmaP3++LrvsMsXHxyskJER9+/bVyy+/XGf5bdu2aeXKla4aRo8eLanhMTcLFy7U0KFDFRYWptjYWP30pz/VgQMH6mxfRESEDhw4oEmTJikiIkJxcXG6//77VV1dfdafH9CaBFpdAADv2rZtmy655BJFRUXpgQceUFBQkF555RWNHj1aK1eu1PDhwyVJjz32mNLT03XLLbdo2LBhKiws1Pr167Vx40ZdfvnlkqRrr71W27Zt0913362UlBQdOnRIGRkZysrKUkpKSr3vf9111+mBBx7Q22+/rV/+8pdu895++22NGzdO7du3V0VFhdLS0lReXq67775biYmJOnDggJYsWaL8/Hw5HI6z+hxuu+02HTx4UBkZGXrjjTfqnb9gwQJNmzZN99xzj3bv3q0XX3xRmzZt0urVqxUUFORqm5mZqSlTpui2227Trbfeqt69e0uSXn75ZfXr108//OEPFRgYqPfee0933nmnnE6npk+fLkl6/vnndffddysiIkK/+tWvJEkJCQkN1l1b0wUXXKD09HTl5ubqhRde0OrVq7Vp0yZFR0e72lZXVystLU3Dhw/X7373Oy1btkzPPfecevTooTvuuOOsPj+gVTEAWq358+cbSebLL79ssM2kSZNMcHCw2blzp2vawYMHTWRkpBk5cqRr2qBBg8yECRMaXM/Ro0eNJPPss882uc7U1FQzdOhQt2nr1q0zkszrr79ujDFm06ZNRpJZuHBhk9dfn/o+m+nTp5v6/ux99tlnRpL5+9//7jb9ww8/rDO9a9euRpL58MMP66yntLS0zrS0tDTTvXt3t2n9+vUzo0aNqtP2k08+MZLMJ598YowxpqKiwsTHx5v+/fubY8eOudotWbLESDKPPPKIa9qNN95oJJnZs2e7rXPIkCF1PnvA33FYCvBj1dXV+uijjzRp0iR1797dNT0pKUk/+clPtGrVKhUWFkqSoqOjtW3bNm3fvr3edYWFhSk4OFgrVqyoc6jmTCZPnqwNGzZo586drmlvvfWWQkJCNHHiREly9cwsXbpUpaWlTVr/2Vq4cKEcDocuv/xy5eXluR5Dhw5VRESEPvnkE7f23bp1U1paWp31hIWFuZ4XFBQoLy9Po0aN0q5du5p1aG39+vU6dOiQ7rzzToWGhrqmT5gwQX369NF///vfOsvcfvvtbq8vueQS7dq1q8nvDbRmhBvAj33//fcqLS11HTY52bnnniun06l9+/ZJkmbPnq38/Hydc845GjBggH75y1/q66+/drUPCQnR008/rQ8++EAJCQkaOXKknnnmGeXk5Jyxjh//+Mey2+166623JNUMwl24cKFrHJBUExhmzpypV199VbGxsUpLS9NLL73kkfE2Z7J9+3YVFBQoPj5ecXFxbo/i4mIdOnTIrX23bt3qXc/q1as1duxYtWvXTtHR0YqLi9NDDz0kSc3ajtpxSvXtvz59+tQZxxQaGqq4uDi3ae3bt29yGAVaO8INAEk1pyHv3LlT8+bNU//+/fXqq6/qvPPO06uvvupqM2PGDH333XdKT09XaGioHn74YZ177rnatGnTadfdsWNHXXLJJXr77bclSZ9//rmysrI0efJkt3bPPfecvv76az300EM6duyY7rnnHvXr10/79+/3/AafxOl0Kj4+XhkZGfU+Zs+e7db+5B6aWjt37tSYMWOUl5enOXPm6L///a8yMjL0i1/8wvUe3hYQEOD19wBaA8IN4Mfi4uIUHh6uzMzMOvO+/fZb2e12JScnu6bFxMRo2rRp+sc//qF9+/Zp4MCBdS5616NHD91333366KOPtHXrVlVUVOi55547Yy2TJ0/WV199pczMTL311lsKDw/XVVddVafdgAED9Otf/1qffvqpPvvsMx04cEBz585t+sbX4+Szo07Wo0cPHT58WBdddJHGjh1b5zFo0KAzrvu9995TeXm5/vOf/+i2227TD37wA40dO7beINRQHafq2rWrJNW7/zIzM13zAbgj3AB+LCAgQOPGjdPixYvdTtfOzc3Vm2++qYsvvth1WOjw4cNuy0ZERKhnz54qLy+XJJWWlqqsrMytTY8ePRQZGelqczrXXnutAgIC9I9//EMLFy7UlVdeqXbt2rnmFxYWqqqqym2ZAQMGyG63u60/KytL3377beM+gFPUvl9+fr7b9Ouuu07V1dX6zW9+U2eZqqqqOu3rU9trYoxxTSsoKND8+fPrraMx6zz//PMVHx+vuXPnun0GH3zwgb755htNmDDhjOsA2iJOBQf8wLx58/Thhx/WmX7vvffqiSeeUEZGhi6++GLdeeedCgwM1CuvvKLy8nI988wzrrZ9+/bV6NGjNXToUMXExGj9+vX617/+pbvuukuS9N1332nMmDG67rrr1LdvXwUGBurdd99Vbm6urr/++jPWGB8fr0svvVRz5sxRUVFRnUNSH3/8se666y79+Mc/1jnnnKOqqiq98cYbCggI0LXXXutqN3XqVK1cudItRDTW0KFDJUn33HOP0tLSFBAQoOuvv16jRo3SbbfdpvT0dG3evFnjxo1TUFCQtm/froULF+qFF17Qj370o9Oue9y4cQoODtZVV12l2267TcXFxfrLX/6i+Ph4ZWdn16nj5Zdf1hNPPKGePXsqPj5el112WZ11BgUF6emnn9a0adM0atQoTZkyxXUqeEpKiuuQF4BTWHy2FoCzUHu6c0OPffv2GWOM2bhxo0lLSzMREREmPDzcXHrppWbNmjVu63riiSfMsGHDTHR0tAkLCzN9+vQxTz75pKmoqDDGGJOXl2emT59u+vTpY9q1a2ccDocZPny4efvttxtd71/+8hcjyURGRrqd2myMMbt27TL/93//Z3r06GFCQ0NNTEyMufTSS82yZcvc2o0aNare07kb+mxOPhW8qqrK3H333SYuLs7YbLY66/nzn/9shg4dasLCwkxkZKQZMGCAeeCBB8zBgwddbbp27drgKfP/+c9/zMCBA01oaKhJSUkxTz/9tJk3b56RZHbv3u1ql5OTYyZMmGAiIyONJNdp4aeeCl7rrbfeMkOGDDEhISEmJibG3HDDDWb//v1ubW688UbTrl27OjU9+uijjfq8AH9iM6YZ//0BAABooRhzAwAA/ArhBgAA+BXCDQAA8CuEGwAA4FcsDTcvv/yyBg4cqKioKEVFRSk1NVUffPDBaZdZuHCh+vTpo9DQUA0YMEDvv/++j6oFAACtgaXhpnPnznrqqae0YcMGrV+/XpdddpkmTpyobdu21dt+zZo1mjJlim6++WZt2rRJkyZN0qRJk7R161YfVw4AAFqqFncqeExMjJ599lndfPPNdeZNnjxZJSUlWrJkiWvahRdeqMGDBzf68uxOp1MHDx5UZGRkoy+BDgAArGWMUVFRkTp27Ci7/fR9My3mCsXV1dVauHChSkpKlJqaWm+btWvXaubMmW7T0tLStGjRogbXW15e7nbZ8gMHDqhv374eqRkAAPjWvn371Llz59O2sTzcbNmyRampqSorK1NERITefffdBsNHTk6OEhIS3KYlJCQoJyenwfWnp6fr8ccfrzN93759rnvqAACAlq2wsFDJycmKjIw8Y1vLw03v3r21efNmFRQU6F//+pduvPFGrVy50mO9K7NmzXLr7an9cGoHMQMAgNajMUNKLA83wcHB6tmzp6Sam8l9+eWXeuGFF/TKK6/UaZuYmKjc3Fy3abm5uUpMTGxw/SEhIQoJCfFs0QAAoMVqcde5cTqdbmNkTpaamqrly5e7TcvIyGhwjA4AAGh7LO25mTVrlsaPH68uXbqoqKhIb775plasWKGlS5dKkqZOnapOnTopPT1dknTvvfdq1KhReu655zRhwgT985//1Pr16/XnP//Zys0AAAAtiKXh5tChQ5o6daqys7PlcDg0cOBALV26VJdffrkkKSsry+10rxEjRujNN9/Ur3/9az300EPq1auXFi1apP79+1u1CQAAoIVpcde58bbCwkI5HA4VFBQwoBgAgFaiKd/fLW7MDQAAwNkg3AAAAL9CuAEAAH6FcAMAAPwK4QYAAPgVwg0AAPArhBsAAOBXCDceUlntVG5hmfYdKbW6FAAA2jTCjYes33NUw3+7XDfNX2d1KQAAtGmEGw9xhAVJkgrLqiyuBACAto1w4yFRYTW36So4VmlxJQAAtG2EGw+p7bmpqHKqrLLa4moAAGi7CDce0i44UHZbzfNCem8AALAM4cZD7Habolzjbgg3AABYhXDjQVGhNeGGcTcAAFiHcONBtYOKC49xxhQAAFYh3HiQg8NSAABYjnDjQRyWAgDAeoQbD3L13BBuAACwDOHGg2rPlqLnBgAA6xBuPOhEzw0DigEAsArhxoOiQrkFAwAAViPceBAX8QMAwHqEGw8i3AAAYD3CjQdxKjgAANYj3HgQA4oBALAe4caDXLdfKKuU02ksrgYAgLaJcONBtYeljJGKK+i9AQDACoQbDwoNClBIYM1HWlDKuBsAAKxAuPEwzpgCAMBahBsPc3ALBgAALEW48bDaqxRzxhQAANYg3HiYg8NSAABYinDjYa4xNxyWAgDAEoQbD3MQbgAAsBThxsO4BQMAANYi3HjYiasUM6AYAAArEG48jFPBAQCwFuHGw2oPSzHmBgAAaxBuPIxTwQEAsBbhxsOiOCwFAIClCDceduJUcAYUAwBgBcKNh9WOuTlWWa2KKqfF1QAA0PYQbjwsMjRQNlvNc8bdAADge4QbD7PbbYoIqbnWTX4p4QYAAF8j3HhBdDiDigEAsArhxguiw4IlSQXHKiyuBACAtodw4wW1PTcclgIAwPcsDTfp6em64IILFBkZqfj4eE2aNEmZmZmnXWbBggWy2Wxuj9DQUB9V3DjcggEAAOtYGm5Wrlyp6dOn6/PPP1dGRoYqKys1btw4lZSUnHa5qKgoZWdnux579+71UcWNUxtu6LkBAMD3Aq188w8//NDt9YIFCxQfH68NGzZo5MiRDS5ns9mUmJjo7fKajQHFAABYp0WNuSkoKJAkxcTEnLZdcXGxunbtquTkZE2cOFHbtm1rsG15ebkKCwvdHt5WO6A4v5QBxQAA+FqLCTdOp1MzZszQRRddpP79+zfYrnfv3po3b54WL16sv/3tb3I6nRoxYoT2799fb/v09HQ5HA7XIzk52Vub4OKoHVBMzw0AAD5nM8YYq4uQpDvuuEMffPCBVq1apc6dOzd6ucrKSp177rmaMmWKfvOb39SZX15ervLyctfrwsJCJScnq6CgQFFRUR6p/VQfbcvRz9/YoMHJ0Vo0/SKvvAcAAG1JYWGhHA5Ho76/LR1zU+uuu+7SkiVL9OmnnzYp2EhSUFCQhgwZoh07dtQ7PyQkRCEhIZ4os9Giw2sOSxXScwMAgM9ZeljKGKO77rpL7777rj7++GN169atyeuorq7Wli1blJSU5IUKm8d1thThBgAAn7O052b69Ol68803tXjxYkVGRionJ0eS5HA4FBYWJkmaOnWqOnXqpPT0dEnS7NmzdeGFF6pnz57Kz8/Xs88+q7179+qWW26xbDtOdeIifhVyOo3sdpvFFQEA0HZYGm5efvllSdLo0aPdps+fP1833XSTJCkrK0t2+4kOpqNHj+rWW29VTk6O2rdvr6FDh2rNmjXq27evr8o+o9qeG6eRiiuqFBUaZHFFAAC0HS1mQLGvNGVA0tno8/AHKqt06rMHLlVyTLjX3gcAgLagKd/fLeZUcH9z4lo3jLsBAMCXCDde4hp3w53BAQDwKcKNl0Rx80wAACxBuPGSaG6eCQCAJQg3XsLNMwEAsAbhxktqr1LMzTMBAPAtwo2XODgsBQCAJQg3XsItGAAAsAbhxktcY27ouQEAwKcIN15SexE/BhQDAOBbhBsv4SJ+AABYg3DjJQwoBgDAGoQbL6ntuSmvcqqsstriagAAaDsIN14SERKoALtNEr03AAD4EuHGS2w220mngzPuBgAAXyHceBH3lwIAwPcIN17k4P5SAAD4HOHGi2p7bo6WcFgKAABfIdx4UXxkqCTpUFG5xZUAANB2EG68KNFRE26yC8osrgQAgLaDcONFScfDTU7BMYsrAQCg7SDceBE9NwAA+B7hxouSHGGSCDcAAPgS4caLkqJrem4KjlWqtKLK4moAAGgbCDdeFBkSqHbBAZKkHHpvAADwCcKNF9lsNte4G8INAAC+QbjxMsbdAADgW4QbL0tynTHF6eAAAPgC4cbLkjgdHAAAnyLceFni8cNSjLkBAMA3CDdeRs8NAAC+RbjxMtfZUoWEGwAAfIFw42Udjx+WOlJSobLKaourAQDA/xFuvCwqLFBhQVzIDwAAXyHceJnNZmPcDQAAPkS48YET42641g0AAN5GuPGBRHpuAADwGcKND3TkWjcAAPgM4cYHkmNqwk1mTpHFlQAA4P8INz5wfkqMJGlTVj6ngwMA4GWEGx/oHttO8ZEhqqh2auPeo1aXAwCAXyPc+IDNZlNqjw6SpLW7DltcDQAA/o1w4yOp3Y+Hm52EGwAAvIlw4yMjesRKkr7an6/SiiqLqwEAwH8RbnwkOSZMnaLDVFlttH4P424AAPAWwo2P2Gw2XdidcTcAAHgb4caHagcVr96RJ2OMxdUAAOCfLA036enpuuCCCxQZGan4+HhNmjRJmZmZZ1xu4cKF6tOnj0JDQzVgwAC9//77Pqj27F3SK1aBdpu+3l+gxZsPWl0OAAB+ydJws3LlSk2fPl2ff/65MjIyVFlZqXHjxqmkpKTBZdasWaMpU6bo5ptv1qZNmzRp0iRNmjRJW7du9WHlzZMQFap7xvSSJD28eKsO5nMjTQAAPM1mWtDxke+//17x8fFauXKlRo4cWW+byZMnq6SkREuWLHFNu/DCCzV48GDNnTv3jO9RWFgoh8OhgoICRUVFeaz2xqqqdupHc9dq8758pXbvoL/edL7CgwN9XgcAAK1JU76/W9SYm4KCAklSTExMg23Wrl2rsWPHuk1LS0vT2rVrvVqbpwQG2DXnukEKDbJr7a7DGvvcSn2wJZsxOAAAeEiL6TJwOp2aMWOGLrroIvXv37/Bdjk5OUpISHCblpCQoJycnHrbl5eXq7y83PW6sLDQMwWfhe5xEfrrjRfowX9/rf1Hj+mOv29Up+gwXTWooy7tHadBydEKDQqwukwAAFqlFhNupk+frq1bt2rVqlUeXW96eroef/xxj67TEy7qGauMX4zSn1bs0LxVu3Ug/5jmrtypuSt3KjjAroGdHbqgW4wuSGmvgZ2jFRsRYnXJAAC0Ci0i3Nx1111asmSJPv30U3Xu3Pm0bRMTE5Wbm+s2LTc3V4mJifW2nzVrlmbOnOl6XVhYqOTk5LMv2gPCggN037jemn5pT3387SG9vyVbX+w+ou+LyrV+71Gt33tULx9v29ERqoGdozWgs0ODOkdrQCeHHOFBltYPAEBLZOmAYmOM7r77br377rtasWKFevXqdcZlJk+erNLSUr333nuuaSNGjNDAgQNbxYDiMzHGKOtIqdbtPqIv9xzRpqx87fi+WPXtpZQO4RrQOVqDOjs0oJND/To5FBHSIvIqAAAe1ZTvb0vDzZ133qk333xTixcvVu/evV3THQ6HwsLCJElTp05Vp06dlJ6eLqnmVPBRo0bpqaee0oQJE/TPf/5Tv/3tb7Vx48bTjtWp1dLDTX2Ky6u09UCBtuwv0Ff787XlQIH2Hi6t085mk3rGRWhAZ4cuSInRRT1i1aVDuAUVAwDgWa0m3Nhstnqnz58/XzfddJMkafTo0UpJSdGCBQtc8xcuXKhf//rX2rNnj3r16qVnnnlGP/jBDxr1nq0x3NQnv7RCX+8v0JYDBfpqX03gyS4oq9MuOSZMF/WI1cW9YjW6dzw9OwCAVqnVhBsr+Eu4qc+hojJt2V+gzfvy9fmuw9qUla8q54ndGxJo12V94nXlwI66rE+8woI5IwsA0DoQbk7Dn8PNqYrLq7Ru92Gt3nFYy7/J1Z6TDmWFBQVofP9ETR2RosHJ0dYVCQBAIxBuTqMthZuTGWO07WChlnydrSVfH9T+oydu/TCkS7TuuayXRveOa/BQIQAAViLcnEZbDTcnM8ZoY1a+/v75Xi35OlsV1U5J0qDkaD084Vydn9LwFaIBALAC4eY0CDfuvi8q118+26XX1+5RWWVNyPnx0M6a9YNzFdMu2OLqAACoQbg5DcJN/b4vKtfvlmbqrfX7JEnxkSF64fohSu3RweLKAABoxTfOhHXiIkP09I8G6t93jFDP+AgdKirXDa9+rj8u385NPQEArQrhBm6Gdm2v/9x1ka47v7OcRnou4zv9atFWVTsJOACA1oFwgzrCgwP1zI8G6YlJ/WWzSW9+kaV7/rlJlccHHgMA0JIRbtCgn17YVS9OOU9BATb99+tsPfqfbRyiAgC0eIQbnNaEgUl66SfnuXpw/rpqt9UlAQBwWoQbnNG4fon61Q/OlSQ9+f43+iTzkMUVAQDQMMINGuXmi7vphuFdZIz04L++VsGxSqtLAgCgXoQbNIrNZtPDV/ZV99h2OlRUrieW/M/qkgAAqBfhBo0WGhSgZ340UDabtHDDfq3g8BQAoAUi3KBJzk+J0U0jUiRJj7/3P1VxejgAoIUh3KDJ7hvXW+3Dg7Q7r0TvfX3Q6nIAAHBDuEGTRYQE6pZLukuS/rh8B1cvBgC0KIQbNMuNI1IUHR6kXXkleu8rem8AAC0H4QbNEhESqFuP99784ePtctJ7AwBoIQg3aLapqV0VGRKoXd+X6IvdR6wuBwAASYQbnIXI0CBNGJgkSfr3xv0WVwMAQA3CDc7KtUM7S5I+2JKt0ooqi6sBAIBwg7N0ftf26hITrpKKai3dlmN1OQAAEG5wdmw2m645r5Mk6Z2NByyuBgAAwg084JohNYemVu3IU3bBMYurAQC0dYQbnLUuHcI1tGt7GSMt+4b7TQEArEW4gUeMPidOkrRq+/cWVwIAaOsIN/CIS46HmzU7D3MzTQCApQg38IgBnRxyhAWpqKxKXx8osLocAEAbRriBRwTYbRrRo4MkadX2PIurAQC0ZYQbeMzFvWIlEW4AANYi3MBjLulZM+5mY9ZRFZdztWIAgDUIN/CYLh3C1SUmXFVOoy92Hba6HABAG0W4gUfVHppavYNwAwCwBuEGHjW0S3tJ0lbOmAIAWIRwA4/q38khSdp2sEBOp7G4GgBAW0S4gUf1iGunkEC7SiqqtedwidXlAADaIMINPCowwK5zk6IkSVsPFlpcDQCgLSLcwOP6d6oJN9sYdwMAsADhBh7Xv2PNuJutBwk3AADfI9zA42oHFW89UChjGFQMAPAtwg087pyESAUF2FRwrFL7jx6zuhwAQBtDuIHHBQfa1TsxUlLNKeEAAPgS4QZe4Rp3c4AzpgAAvkW4gVf068SgYgCANQg38Iq+STWHpTJziiyuBADQ1hBu4BXdYiMkSdkFZTpWUW1xNQCAtoRwA69oHx6kqNBASdLeI9yGAQDgO80KN/v27dP+/ftdr9etW6cZM2boz3/+c5PW8+mnn+qqq65Sx44dZbPZtGjRotO2X7FihWw2W51HTk5OczYDXmSz2dQttp0kaU9eqcXVAADakmaFm5/85Cf65JNPJEk5OTm6/PLLtW7dOv3qV7/S7NmzG72ekpISDRo0SC+99FKT3j8zM1PZ2dmuR3x8fJOWh2+k1IYbbqAJAPChwOYstHXrVg0bNkyS9Pbbb6t///5avXq1PvroI91+++165JFHGrWe8ePHa/z48U1+//j4eEVHRzd5OfhW1w61PTeEGwCA7zSr56ayslIhISGSpGXLlumHP/yhJKlPnz7Kzs72XHUNGDx4sJKSknT55Zdr9erVp21bXl6uwsJCtwd8o1tsuCR6bgAAvtWscNOvXz/NnTtXn332mTIyMnTFFVdIkg4ePKgOHTp4tMCTJSUlae7cufr3v/+tf//730pOTtbo0aO1cePGBpdJT0+Xw+FwPZKTk71WH9yldGDMDQDA92ymGXc2XLFiha6++moVFhbqxhtv1Lx58yRJDz30kL799lu98847TS/EZtO7776rSZMmNWm5UaNGqUuXLnrjjTfqnV9eXq7y8nLX68LCQiUnJ6ugoEBRUVFNrhONd7SkQkN+kyFJ+mb2FQoLDrC4IgBAa1VYWCiHw9Go7+9mjbkZPXq08vLyVFhYqPbt27um//znP1d4eHhzVtlsw4YN06pVqxqcHxIS4jqEBt9q3y5YjrAgFRyr1J7DJTo3iTAJAPC+Zh2WOnbsmMrLy13BZu/evXr++eeVmZnp8zOXNm/erKSkJJ++Jxqv9oypvYy7AQD4SLN6biZOnKhrrrlGt99+u/Lz8zV8+HAFBQUpLy9Pc+bM0R133NGo9RQXF2vHjh2u17t379bmzZsVExOjLl26aNasWTpw4IBef/11SdLzzz+vbt26qV+/fiorK9Orr76qjz/+WB999FFzNgM+0K1DuL7al6/djLsBAPhIs3puNm7cqEsuuUSS9K9//UsJCQnau3evXn/9df3hD39o9HrWr1+vIUOGaMiQIZKkmTNnasiQIa5TybOzs5WVleVqX1FRofvuu08DBgzQqFGj9NVXX2nZsmUaM2ZMczYDPsDp4AAAX2tWz01paakiI2tujPjRRx/pmmuukd1u14UXXqi9e/c2ej2jR4/W6cYzL1iwwO31Aw88oAceeKA5JcMi3biQHwDAx5rVc9OzZ08tWrRI+/bt09KlSzVu3DhJ0qFDhzgDCW64SjEAwNeaFW4eeeQR3X///UpJSdGwYcOUmpoqqaYXp/YQEyBJ3Y4flsotLFdpRZXF1QAA2oJmHZb60Y9+pIsvvljZ2dkaNGiQa/qYMWN09dVXe6w4tH6O8CBFhgaqqKxKB/OPqWd8pNUlAQD8XLPCjSQlJiYqMTHRdXfwzp07u+43BZysoyNMmWVFyi4oI9wAALyuWYelnE6nZs+eLYfDoa5du6pr166Kjo7Wb37zGzmdTk/XiFYu0REqScrOL7O4EgBAW9Csnptf/epX+utf/6qnnnpKF110kSRp1apVeuyxx1RWVqYnn3zSo0WidUuqDTcFhBsAgPc1K9y89tprevXVV113A5ekgQMHqlOnTrrzzjsJN3BT23OTU3jM4koAAG1Bsw5LHTlyRH369KkzvU+fPjpy5MhZFwX/Qs8NAMCXmhVuBg0apBdffLHO9BdffFEDBw4866LgX5IcYZKkHMINAMAHmnVY6plnntGECRO0bNky1zVu1q5dq3379un999/3aIFo/ei5AQD4UrN6bkaNGqXvvvtOV199tfLz85Wfn69rrrlG27Zt0xtvvOHpGtHK1Y65KThWyYX8AABeZzOnu7lTE3311Vc677zzVF1d7alVelxhYaEcDocKCgq4VYQP9X90qYrLq7T8vlHqERdhdTkAgFamKd/fzeq5AZrKdcYUh6YAAF5GuIFPMO4GAOArhBv4RGJUbc8N17oBAHhXk86Wuuaaa047Pz8//2xqgR9Liq45HZyeGwCAtzUp3DgcjjPOnzp16lkVBP/EYSkAgK80KdzMnz/fW3XAzyUSbgAAPsKYG/hEkoMxNwAA3yDcwCeSomrG3BwtrVRZZcu9DhIAoPUj3MAnosICFRYUIIlr3QAAvItwA5+w2WwMKgYA+AThBj7jukpxIeNuAADeQ7iBz8RHhkiSvi8qt7gSAIA/I9zAZ2IjCDcAAO8j3MBn4o733OQVV1hcCQDAnxFu4DO1PTd5xfTcAAC8h3ADn4ljzA0AwAcIN/AZem4AAL5AuIHPxEYGS5IOl1SoqtppcTUAAH9FuIHPdGgXIrtNMkY6UsqgYgCAdxBu4DMBdpti2tX03uQVEW4AAN5BuIFPua51w7gbAICXEG7gU65r3XDGFADASwg38Cl6bgAA3ka4gU/RcwMA8DbCDXwqNuL4gGJ6bgAAXkK4gU+5rlJMuAEAeAnhBj7lukoxp4IDALyEcAOfoucGAOBthBv4VG3PzdFSbsEAAPAOwg18qn148IlbMJRwaAoA4HmEG/hUgN2mDsd7bw5xOjgAwAsIN/A516Bixt0AALyAcAOfcw0qpucGAOAFhBv43IkL+THmBgDgeYQb+Fwch6UAAF5EuIHPcVgKAOBNloabTz/9VFdddZU6duwom82mRYsWnXGZFStW6LzzzlNISIh69uypBQsWeL1OeFaH44elOBUcAOANloabkpISDRo0SC+99FKj2u/evVsTJkzQpZdeqs2bN2vGjBm65ZZbtHTpUi9XCk/q0I7DUgAA7wm08s3Hjx+v8ePHN7r93Llz1a1bNz333HOSpHPPPVerVq3S73//e6WlpXmrTHhYBwYUAwC8qFWNuVm7dq3Gjh3rNi0tLU1r165tcJny8nIVFha6PWCt2uvcHCkpl9NpLK4GAOBvWlW4ycnJUUJCgtu0hIQEFRYW6tixY/Uuk56eLofD4XokJyf7olScRvvwmp4bp5Hyj1VaXA0AwN+0qnDTHLNmzVJBQYHrsW/fPqtLavOCA+1yhAVJkg4z7gYA4GGWjrlpqsTEROXm5rpNy83NVVRUlMLCwupdJiQkRCEhIb4oD00QGxGsgmOVyiuuUK+EM7cHAKCxWlXPTWpqqpYvX+42LSMjQ6mpqRZVhOaqvXnm4RJ6bgAAnmVpuCkuLtbmzZu1efNmSTWnem/evFlZWVmSag4pTZ061dX+9ttv165du/TAAw/o22+/1Z/+9Ce9/fbb+sUvfmFF+TgLtbdgOMwZUwAAD7M03Kxfv15DhgzRkCFDJEkzZ87UkCFD9Mgjj0iSsrOzXUFHkrp166b//ve/ysjI0KBBg/Tcc8/p1Vdf5TTwVqj2WjeMuQEAeJqlY25Gjx4tYxo+Fbi+qw+PHj1amzZt8mJV8AXXtW64SjEAwMNa1Zgb+A/XmBt6bgAAHka4gSVi23GVYgCAdxBuYAl6bgAA3kK4gSU6cLYUAMBLCDewROzxs6WKyqtUVlltcTUAAH9CuIElosICFWi3SZKOcMYUAMCDCDewhM1m49AUAMArCDewTO2F/PK4BQMAwIMIN7AMPTcAAG8g3MAysZwODgDwAsINLNPh+IX8DjOgGADgQYQbWKb2Qn55RfTcAAA8h3ADy3DzTACANxBuYJk4xtwAALyAcAPLuHpuCDcAAA8i3MAyJ86WqpDTaSyuBgDgLwg3sExtz02V0yj/WKXF1QAA/AXhBpYJCQxQdHiQJOl7zpgCAHgI4QaWio+sOTRFuAEAeArhBpaKqw03xWUWVwIA8BeEG1iq9nTwQ4X03AAAPINwA0vFcVgKAOBhhBtYKj4yVJL0Pde6AQB4COEGlqLnBgDgaYQbWIpwAwDwNMINLHXibCnCDQDAMwg3sFTt2VL5pZUqr6q2uBoAgD8g3MBS0eFBCgqwSZLyiissrgYA4A8IN7CUzWZz9d4w7gYA4AmEG1iOQcUAAE8i3MByhBsAgCcRbmC5uNoL+RFuAAAeQLiB5Wp7bg4VcfNMAMDZI9zAchyWAgB4EuEGlnOdLcWF/AAAHkC4geXouQEAeBLhBpaLPyncGGMsrgYA0NoRbmC52p6b8iqnCsuqLK4GANDaEW5gudCgAEWGBkqSvueMKQDAWSLcoEXo6AiTJB3MJ9wAAM4O4QYtQsfomgv5Hcg/ZnElAIDWjnCDFqFT+9qeG8INAODsEG7QInSKDpckHThKuAEAnB3CDVqE2sNS++m5AQCcJcINWoTOHJYCAHgI4QYtQu1hqZyCMlU7uZAfAKD5CDdoEeIiQxRot6nKaZRbyOngAIDmaxHh5qWXXlJKSopCQ0M1fPhwrVu3rsG2CxYskM1mc3uEhob6sFp4Q4DdpiROBwcAeIDl4eatt97SzJkz9eijj2rjxo0aNGiQ0tLSdOjQoQaXiYqKUnZ2tuuxd+9eH1YMb+kUzbgbAMDZszzczJkzR7feequmTZumvn37au7cuQoPD9e8efMaXMZmsykxMdH1SEhI8GHF8JaOx8PNfk4HBwCcBUvDTUVFhTZs2KCxY8e6ptntdo0dO1Zr165tcLni4mJ17dpVycnJmjhxorZt2+aLcuFlnY+HGw5LAQDOhqXhJi8vT9XV1XV6XhISEpSTk1PvMr1799a8efO0ePFi/e1vf5PT6dSIESO0f//+etuXl5ersLDQ7YGWiasUAwA8wfLDUk2VmpqqqVOnavDgwRo1apTeeecdxcXF6ZVXXqm3fXp6uhwOh+uRnJzs44rRWLWHpbhKMQDgbFgabmJjYxUQEKDc3Fy36bm5uUpMTGzUOoKCgjRkyBDt2LGj3vmzZs1SQUGB67Fv376zrhve0emkw1LGcK0bAEDzWBpugoODNXToUC1fvtw1zel0avny5UpNTW3UOqqrq7VlyxYlJSXVOz8kJERRUVFuD7RMtT03pRXVKjhWaXE1AIDWyvLDUjNnztRf/vIXvfbaa/rmm290xx13qKSkRNOmTZMkTZ06VbNmzXK1nz17tj766CPt2rVLGzdu1E9/+lPt3btXt9xyi1WbAA8JDQpQbESwJM6YAgA0X6DVBUyePFnff/+9HnnkEeXk5Gjw4MH68MMPXYOMs7KyZLefyGBHjx7VrbfeqpycHLVv315Dhw7VmjVr1LdvX6s2AR7UKTpMecUVOpB/TP07OawuBwDQCtlMGxvcUFhYKIfDoYKCAg5RtUB3/n2D3t+So19POFe3XNLd6nIAAC1EU76/LT8sBZyse2yEJGnn98UWVwIAaK0IN2hReiXUhJvvcgk3AIDmIdygRTknIVKS9F1uEaeDAwCahXCDFqV7XDsF2G0qKqtSbmG51eUAAFohwg1alJDAAHXtEC6ppvcGAICmItygxTkn/sShKQAAmopwgxbnnOODinccYlAxAKDpCDdocXol0HMDAGg+wg1anNozprbnFnPGFACgyQg3aHG6xbZToN2movIq5RSWWV0OAKCVIdygxQkOtCsltp0kLuYHAGg6wg1apNpBxdsZdwMAaCLCDVqkXpwODgBoJsINWqS+HWvu+Lp5X761hQAAWh3CDVqkC1JiJNWMuTlSUmFxNQCA1oRwgxYppl2wa9zNut1HLK4GANCaEG7QYg3v1kGS9MXuwxZXAgBoTQg3aLGGdas5NPXFLnpuAACNR7hBizW8e024+SanUAWllRZXAwBoLQg3aLHiI0PVPbadjJHW76X3BgDQOIQbtGiuQ1MMKgYANBLhBi1a7aGpL3YxqBgA0DiEG7RoF3avOWNqy4ECHeImmgCARiDcoEVLcoRpSJdoOY30n68OWl0OAKAVINygxbt6SCdJ0uLNhBsAwJkRbtDiTRiQpAC7TVsOFGjHoWKrywEAtHCEG7R4HSJCNLJXrCRp8eYDFlcDAGjpCDdoFSaddGjKGGNxNQCAloxwg1bh8r4JCg8OUNaRUq3ZyWnhAICGEW7QKoQHB+pHQztLkp5f9h29NwCABhFu0GrcObqnggPt+nLPUa3akWd1OQCAFopwg1Yj0RGqG4Z3kST9PoPeGwBA/Qg3aFXuGN1DoUF2bczK18ffHrK6HABAC0S4QasSHxmqG0ekSJIeeneL8ksrrC0IANDiEG7Q6swYc466x7VTbmG5Hnp3C4enAABuCDdodcKCA/T85MEKtNv0/pYcLdyw3+qSAAAtCOEGrdLAztGaMbaXJOlX727RikzG3wAAahBu0GrdMbqnJgxMUmW10e1/26B1u49YXRIAoAUg3KDVCrDb9PvrBmt07ziVVTp10/x1+mBLttVlAQAsRrhBqxYcaNfcnw7VJb1iVVpRrTv+vlFzPspUVbXT6tIAABYh3KDVCw0K0PybLtD/XdRNkvSHj3fo6j+t0baDBRZXBgCwAuEGfiEwwK5Hruqr308epMjQQG05UKAfvrhaD727RQfzj1ldHgDAh2ymjV0kpLCwUA6HQwUFBYqKirK6HHjBocIyPfbeNr2/JUeSFBxg17VDO+lnF6aob0f2OQC0Rk35/ibcwG+t231EczIy9fmuE2dRDUqO1sRBHTVhYJISokItrA4A0BSEm9Mg3LQ963Yf0etr9+jDrTmqcp74535uUpQu7tlBF/WM1bBuMQoPDrSwSgDA6RBuToNw03YdKirT+19n672vs7Vh71G3ecEBdvXrFKX+HR3q3ylK/Ts51Cs+UsGBDEsDgJaAcHMahBtIUl5xudbsPKzV2/O0akeeDtQz6DjQblOXDuHqHhuh7nHt1C22nbrEhCvJEaokR5jCggMsqBwA2ibCzWkQbnAqY4z2Hi7VV/vzte1gobYeKNDWAwUqLKs67XKOsKDjQSdUiY4wxUUEq327YMW0C1b78OM/2wUrJjyYIAQAZ6nVhZuXXnpJzz77rHJycjRo0CD98Y9/1LBhwxpsv3DhQj388MPas2ePevXqpaefflo/+MEPGvVehBs0hjFGBwvKtPv7Eu3OK9bO70u0O69EB/KPKTv/mEoqqpu0vtAgu9qHBysqNEjtQgIUERqkiJAARYQEql1IoCKP/4wIDayZFhyo8JAAhQYFKDQwQKFB9prnQcefBwbIbrd5aesBoOVpyve35SMo33rrLc2cOVNz587V8OHD9fzzzystLU2ZmZmKj4+v037NmjWaMmWK0tPTdeWVV+rNN9/UpEmTtHHjRvXv39+CLYA/stls6hQdpk7RYbq4V6zbPGOMisqrlFNQpoP5x5RTUKbsgjIdKanQkdIKHS2p0JGSCh0trdDRkkpVVDtVVulU9vF2nhIcYFeIK/TYj4egAAUH2hUUYFNQgP34o+Z5cIBdgSdNr20XaLe7LRMYYFfwKcvbbTYF1v6022W3S4F2uwLsUoDdrgCbTQF290eg3SZ77U+b++uAU6YBgCdZ3nMzfPhwXXDBBXrxxRclSU6nU8nJybr77rv1//7f/6vTfvLkySopKdGSJUtc0y688EINHjxYc+fOPeP70XMDXzLGqKSi2hV4isurVFRWpZLyKhWf9Cgpr1JxmfvrkopqlVVWq6zSqfLKapVVVauy2vKOVq84NQjZbJLdZpP9+E/bSc/ttprwabfXvm5ke1s97e1NbH98ns0m2VT7U+6vbZIamqea5XXKdNd6axZteN3HF6x/vSfWbbfVv7x04n1OrudM67Yff2I7ZXmd/FM2t9e1XNurM7c/tY1OaXNifgPrPGXdauxyDbyPzjD/5H3pkfob/AwbqOOM9bmt5bS1nDyt3hpOmnvqvPpqDQ60Kz7Ss5fbaDU9NxUVFdqwYYNmzZrlmma32zV27FitXbu23mXWrl2rmTNnuk1LS0vTokWL6m1fXl6u8vJy1+vCwsKzLxxoJJvNpoiQmkNNyTHhZ72+aqc5HniqVVblPPG80qnyqmqVVzpVUe1UZe2jyqii2qmqaqcqq41rXlW1UWX1SW2rjCqdNW0qq5yuebXtqo2R02lU5TSqrn2Yk54fn+c85efJbU6nymkkp1HFWX9CAFqC87pE6507L7Ls/S0NN3l5eaqurlZCQoLb9ISEBH377bf1LpOTk1Nv+5ycnHrbp6en6/HHH/dMwYDFAuw2tTs+Pqc1McbIaeQejKprflY5nXI6pSqnU9VOI2Mk5/H2tcvVvHafV/P6+HNnE9u7rd/I6VST2te0k4xqntduozGSkY7/PPFaxtQ7vfa1zIn3P7Wda90NLK/a16fMcx5/Yuosf+K1Tl7mdOvWqfW5h1VXnTJ1p50y78RruT1paH7te514feryDc0/XS3u625oO85Uixpda8O1nJh++mXr1uJeRGM/t3q345QXp/5X5OTPqe68k5c78cLqy2i0rr+QzTBr1iy3np7CwkIlJydbWBHQ9thsNgXYasIZAHibpeEmNjZWAQEBys3NdZuem5urxMTEepdJTExsUvuQkBCFhIR4pmAAANDiWdpvFBwcrKFDh2r58uWuaU6nU8uXL1dqamq9y6Smprq1l6SMjIwG2wMAgLbF8sNSM2fO1I033qjzzz9fw4YN0/PPP6+SkhJNmzZNkjR16lR16tRJ6enpkqR7771Xo0aN0nPPPacJEybon//8p9avX68///nPVm4GAABoISwPN5MnT9b333+vRx55RDk5ORo8eLA+/PBD16DhrKws2e0nOphGjBihN998U7/+9a/10EMPqVevXlq0aBHXuAEAAJJawHVufI3r3AAA0Po05fubWx4DAAC/QrgBAAB+hXADAAD8CuEGAAD4FcINAADwK4QbAADgVwg3AADArxBuAACAXyHcAAAAv2L57Rd8rfaCzIWFhRZXAgAAGqv2e7sxN1Zoc+GmqKhIkpScnGxxJQAAoKmKiorkcDhO26bN3VvK6XTq4MGDioyMlM1m8+i6CwsLlZycrH379vnlfav8ffskttEf+Pv2SWyjP/D37ZM8v43GGBUVFaljx45uN9SuT5vrubHb7ercubNX3yMqKspv/7FK/r99EtvoD/x9+yS20R/4+/ZJnt3GM/XY1GJAMQAA8CuEGwAA4FcINx4UEhKiRx99VCEhIVaX4hX+vn0S2+gP/H37JLbRH/j79knWbmObG1AMAAD8Gz03AADArxBuAACAXyHcAAAAv0K4AQAAfoVw4yEvvfSSUlJSFBoaquHDh2vdunVWl9Rs6enpuuCCCxQZGan4+HhNmjRJmZmZbm1Gjx4tm83m9rj99tstqrhpHnvssTq19+nTxzW/rKxM06dPV4cOHRQREaFrr71Wubm5FlbcdCkpKXW20Wazafr06ZJa5/779NNPddVVV6ljx46y2WxatGiR23xjjB555BElJSUpLCxMY8eO1fbt293aHDlyRDfccIOioqIUHR2tm2++WcXFxT7cioadbvsqKyv14IMPasCAAWrXrp06duyoqVOn6uDBg27rqG+/P/XUUz7ekoadaR/edNNNdeq/4oor3Nq05H0onXkb6/u9tNlsevbZZ11tWvJ+bMz3Q2P+hmZlZWnChAkKDw9XfHy8fvnLX6qqqspjdRJuPOCtt97SzJkz9eijj2rjxo0aNGiQ0tLSdOjQIatLa5aVK1dq+vTp+vzzz5WRkaHKykqNGzdOJSUlbu1uvfVWZWdnux7PPPOMRRU3Xb9+/dxqX7VqlWveL37xC7333ntauHChVq5cqYMHD+qaa66xsNqm+/LLL922LyMjQ5L04x//2NWmte2/kpISDRo0SC+99FK985955hn94Q9/0Ny5c/XFF1+oXbt2SktLU1lZmavNDTfcoG3btikjI0NLlizRp59+qp///Oe+2oTTOt32lZaWauPGjXr44Ye1ceNGvfPOO8rMzNQPf/jDOm1nz57ttl/vvvtuX5TfKGfah5J0xRVXuNX/j3/8w21+S96H0pm38eRty87O1rx582Sz2XTttde6tWup+7Ex3w9n+htaXV2tCRMmqKKiQmvWrNFrr72mBQsW6JFHHvFcoQZnbdiwYWb69Omu19XV1aZjx44mPT3dwqo859ChQ0aSWblypWvaqFGjzL333mtdUWfh0UcfNYMGDap3Xn5+vgkKCjILFy50Tfvmm2+MJLN27VofVeh59957r+nRo4dxOp3GmNa9/4wxRpJ59913Xa+dTqdJTEw0zz77rGtafn6+CQkJMf/4xz+MMcb873//M5LMl19+6WrzwQcfGJvNZg4cOOCz2hvj1O2rz7p164wks3fvXte0rl27mt///vfeLc5D6tvGG2+80UycOLHBZVrTPjSmcftx4sSJ5rLLLnOb1pr246nfD435G/r+++8bu91ucnJyXG1efvllExUVZcrLyz1SFz03Z6miokIbNmzQ2LFjXdPsdrvGjh2rtWvXWliZ5xQUFEiSYmJi3Kb//e9/V2xsrPr3769Zs2aptLTUivKaZfv27erYsaO6d++uG264QVlZWZKkDRs2qLKy0m1/9unTR126dGm1+7OiokJ/+9vf9H//939uN4ttzfvvVLt371ZOTo7bfnM4HBo+fLhrv61du1bR0dE6//zzXW3Gjh0ru92uL774wuc1n62CggLZbDZFR0e7TX/qqafUoUMHDRkyRM8++6xHu/p9YcWKFYqPj1fv3r11xx136PDhw655/rYPc3Nz9d///lc333xznXmtZT+e+v3QmL+ha9eu1YABA5SQkOBqk5aWpsLCQm3bts0jdbW5G2d6Wl5enqqrq912kiQlJCTo22+/tagqz3E6nZoxY4Yuuugi9e/f3zX9Jz/5ibp27aqOHTvq66+/1oMPPqjMzEy98847FlbbOMOHD9eCBQvUu3dvZWdn6/HHH9cll1yirVu3KicnR8HBwXW+MBISEpSTk2NNwWdp0aJFys/P10033eSa1pr3X31q9019v4e183JychQfH+82PzAwUDExMa1u35aVlenBBx/UlClT3G5IeM899+i8885TTEyM1qxZo1mzZik7O1tz5syxsNrGu+KKK3TNNdeoW7du2rlzpx566CGNHz9ea9euVUBAgF/tQ0l67bXXFBkZWeewd2vZj/V9PzTmb2hOTk69v6u18zyBcIPTmj59urZu3eo2JkWS2zHuAQMGKCkpSWPGjNHOnTvVo0cPX5fZJOPHj3c9HzhwoIYPH66uXbvq7bffVlhYmIWVecdf//pXjR8/Xh07dnRNa837r62rrKzUddddJ2OMXn75Zbd5M2fOdD0fOHCggoODddtttyk9Pb1VXOb/+uuvdz0fMGCABg4cqB49emjFihUaM2aMhZV5x7x583TDDTcoNDTUbXpr2Y8NfT+0BByWOkuxsbEKCAioMxI8NzdXiYmJFlXlGXfddZeWLFmiTz75RJ07dz5t2+HDh0uSduzY4YvSPCo6OlrnnHOOduzYocTERFVUVCg/P9+tTWvdn3v37tWyZct0yy23nLZda95/klz75nS/h4mJiXUG+VdVVenIkSOtZt/WBpu9e/cqIyPDrdemPsOHD1dVVZX27NnjmwI9rHv37oqNjXX9u/SHfVjrs88+U2Zm5hl/N6WWuR8b+n5ozN/QxMTEen9Xa+d5AuHmLAUHB2vo0KFavny5a5rT6dTy5cuVmppqYWXNZ4zRXXfdpXfffVcff/yxunXrdsZlNm/eLElKSkrycnWeV1xcrJ07dyopKUlDhw5VUFCQ2/7MzMxUVlZWq9yf8+fPV3x8vCZMmHDadq15/0lSt27dlJiY6LbfCgsL9cUXX7j2W2pqqvLz87VhwwZXm48//lhOp9MV7lqy2mCzfft2LVu2TB06dDjjMps3b5bdbq9zKKe12L9/vw4fPuz6d9na9+HJ/vrXv2ro0KEaNGjQGdu2pP14pu+HxvwNTU1N1ZYtW9yCam1Y79u3r8cKxVn65z//aUJCQsyCBQvM//73P/Pzn//cREdHu40Eb03uuOMO43A4zIoVK0x2drbrUVpaaowxZseOHWb27Nlm/fr1Zvfu3Wbx4sWme/fuZuTIkRZX3jj33XefWbFihdm9e7dZvXq1GTt2rImNjTWHDh0yxhhz++23my5dupiPP/7YrF+/3qSmpprU1FSLq2666upq06VLF/Pggw+6TW+t+6+oqMhs2rTJbNq0yUgyc+bMMZs2bXKdLfTUU0+Z6Ohos3jxYvP111+biRMnmm7dupljx4651nHFFVeYIUOGmC+++MKsWrXK9OrVy0yZMsWqTXJzuu2rqKgwP/zhD03nzp3N5s2b3X4va88uWbNmjfn9739vNm/ebHbu3Gn+9re/mbi4ODN16lSLt+yE021jUVGRuf/++83atWvN7t27zbJly8x5551nevXqZcrKylzraMn70Jgz/zs1xpiCggITHh5uXn755TrLt/T9eKbvB2PO/De0qqrK9O/f34wbN85s3rzZfPjhhyYuLs7MmjXLY3USbjzkj3/8o+nSpYsJDg42w4YNM59//rnVJTWbpHof8+fPN8YYk5WVZUaOHGliYmJMSEiI6dmzp/nlL39pCgoKrC28kSZPnmySkpJMcHCw6dSpk5k8ebLZsWOHa/6xY8fMnXfeadq3b2/Cw8PN1VdfbbKzsy2suHmWLl1qJJnMzEy36a11/33yySf1/ru88cYbjTE1p4M//PDDJiEhwYSEhJgxY8bU2fbDhw+bKVOmmIiICBMVFWWmTZtmioqKLNiauk63fbt3727w9/KTTz4xxhizYcMGM3z4cONwOExoaKg599xzzW9/+1u3YGC1021jaWmpGTdunImLizNBQUGma9eu5tZbb63zn8SWvA+NOfO/U2OMeeWVV0xYWJjJz8+vs3xL349n+n4wpnF/Q/fs2WPGjx9vwsLCTGxsrLnvvvtMZWWlx+q0HS8WAADALzDmBgAA+BXCDQAA8CuEGwAA4FcINwAAwK8QbgAAgF8h3AAAAL9CuAEAAH6FcAOgzUlJSdHzzz9vdRkAvIRwA8CrbrrpJk2aNEmSNHr0aM2YMcNn771gwQJFR0fXmf7ll1+63RkdgH8JtLoAAGiqiooKBQcHN3v5uLg4D1YDoKWh5waAT9x0001auXKlXnjhBdlsNtlsNu3Zs0eStHXrVo0fP14RERFKSEjQz372M+Xl5bmWHT16tO666y7NmDFDsbGxSktLkyTNmTNHAwYMULt27ZScnKw777xTxcXFkqQVK1Zo2rRpKigocL3fY489JqnuYamsrCxNnDhRERERioqK0nXXXafc3FzX/Mcee0yDBw/WG2+8oZSUFDkcDl1//fUqKiry7ocGoFkINwB84oUXXlBqaqpuvfVWZWdnKzs7W8nJycrPz9dll12mIUOGaP369frwww+Vm5ur6667zm351157TcHBwVq9erXmzp0rSbLb7frDH/6gbdu26bXXXtPHH3+sBx54QJI0YsQIPf/884qKinK93/3331+nLqfTqYkTJ+rIkSNauXKlMjIytGvXLk2ePNmt3c6dO7Vo0SItWbJES5Ys0cqVK/XUU0956dMCcDY4LAXAJxwOh4KDgxUeHq7ExETX9BdffFFDhgzRb3/7W9e0efPmKTk5Wd99953OOeccSVKvXr30zDPPuK3z5PE7KSkpeuKJJ3T77bfrT3/6k4KDg+VwOGSz2dze71TLly/Xli1btHv3biUnJ0uSXn/9dfXr109ffvmlLrjgAkk1IWjBggWKjIyUJP3sZz/T8uXL9eSTT57dBwPA4+i5AWCpr776Sp988okiIiJcjz59+kiq6S2pNXTo0DrLLlu2TGPGjFGnTp0UGRmpn/3sZzp8+LBKS0sb/f7ffPONkpOTXcFGkvr27avo6Gh98803rmkpKSmuYCNJSUlJOnToUJO2FYBv0HMDwFLFxcW66qqr9PTTT9eZl5SU5Hrerl07t3l79uzRlVdeqTvuuENPPvmkYmJitGrVKt18882qqKhQeHi4R+sMCgpye22z2eR0Oj36HgA8g3ADwGeCg4NVXV3tNu28887Tv//9b6WkpCgwsPF/kjZs2CCn06nnnntOdntNJ/Tbb799xvc71bnnnqt9+/Zp3759rt6b//3vf8rPz1ffvn0bXQ+AloPDUgB8JiUlRV988YX27NmjvLw8OZ1OTZ8+XUeOHNGUKVP05ZdfaufOnVq6dKmmTZt22mDSs2dPVVZW6o9//KN27dqlN954wzXQ+OT3Ky4u1vLly5WXl1fv4aqxY8dqwIABuuGGG7Rx40atW7dOU6dO1ahRo3T++ed7/DMA4H2EGwA+c//99ysgIEB9+/ZVXFycsrKy1LFjR61evVrV1dUaN26cBgwYoBkzZig6OtrVI1OfQYMGac6cOXr66afVv39//f3vf1d6erpbmxEjRuj222/X5MmTFRcXV2dAslRzeGnx4sVq3769Ro4cqbFjx6p79+566623PL79AHzDZowxVhcBAADgKfTcAAAAv0K4AQAAfoVwAwAA/ArhBgAA+BXCDQAA8CuEGwAA4FcINwAAwK8QbgAAgF8h3AAAAL9CuAEAAH6FcAMAAPwK4QYAAPiV/w8lifK9tmAVdAAAAABJRU5ErkJggg==","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 0.8944522142410278\n","Epoch 10 loss: 0.050363823771476746\n","Epoch 20 loss: 0.00036262418143451214\n","Epoch 30 loss: 1.9863400666508824e-06\n","\n","Prediction:\n","tensor([[ 0.9999],\n","        [-0.9999]])\n","Loss: 1.8200616125341185e-08\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[-0.4320, -0.4068,  0.0657],\n","        [ 0.1677, -0.5020,  0.3478],\n","        [ 0.1895, -0.1395,  0.1369],\n","        [ 0.2007, -0.5452, -0.3898]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-2.1500, -0.8563],\n","        [-1.5183,  1.1790],\n","        [-0.1763,  0.7764],\n","        [-0.8445,  0.9525]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.3823],\n","        [ 0.2050],\n","        [ 0.5283],\n","        [-0.4017]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-2.5323, -1.2386],\n","        [-1.3133,  1.3840],\n","        [ 0.3520,  1.3047],\n","        [-1.2462,  0.5508]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.9874, -0.8450],\n","        [-0.8651,  0.8819],\n","        [ 0.3381,  0.8629],\n","        [-0.8472,  0.5011]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[-0.0577,  0.3558,  0.0316,  0.5497],\n","        [ 0.0455,  0.5371,  0.2708,  0.1680],\n","        [ 0.4218, -0.2660, -0.4883, -0.5184],\n","        [-0.0090, -0.2105, -0.3870,  0.0299]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.9874, -0.8450],\n","        [-0.8651,  0.8819],\n","        [ 0.3381,  0.8629],\n","        [-0.8472,  0.5011]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.7058,  0.6653],\n","        [-0.5603,  0.7531],\n","        [ 0.0876, -1.2722],\n","        [ 0.0348, -0.4971]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[ 0.1907],\n","        [-0.2986],\n","        [ 0.2432],\n","        [ 0.3515]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.5151,  0.8559],\n","        [-0.8590,  0.4544],\n","        [ 0.3309, -1.0289],\n","        [ 0.3863, -0.1456]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.4739,  0.6942],\n","        [-0.6957,  0.4255],\n","        [ 0.3193, -0.7735],\n","        [ 0.3682, -0.1445]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[-0.4722, -0.6120,  0.5180,  0.3822]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.4739,  0.6942],\n","        [-0.6957,  0.4255],\n","        [ 0.3193, -0.7735],\n","        [ 0.3682, -0.1445]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 0.9557, -1.0441]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[0.0443]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9999, -0.9999]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9999, -0.9999]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9999, -0.9999]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(1.0805e-08)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","\n","# loss = torch.sum(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9999, -0.9999]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-5.0366e-05,  1.3810e-04]])\n","squared_difference: tensor([[2.5367e-09, 1.9073e-08]])\n"]},{"data":{"text/plain":["tensor(1.0805e-08)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["[0.9999496340751648, -0.9998618960380554]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# for item in output.item:\n","#   print(item)\n","# type(output)\n","output.tolist()[0]\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0804715344647775e-08\n"]}],"source":["import numpy as np\n","\n","def mse_loss(y_true, y_pred):\n","  \"\"\"Calculates the mean squared error loss.\n","\n","  Args:\n","    y_true: The ground truth labels.\n","    y_pred: The predicted labels.\n","\n","  Returns:\n","    The mean squared error loss.\n","  \"\"\"\n","\n","  loss = np.mean((y_true - y_pred)**2)\n","  return loss\n","\n","def main():\n","  \"\"\"Main function.\"\"\"\n","\n","  # y_true = np.array([1, 2, 3, 4, 5])\n","  y_true = np.array([1.0, -1.0])\n","\n","  # y_pred = np.array([0, 1, 2, 3, 4])\n","  # y_pred = np.array([0.9997345209121704, -0.9980572462081909])\n","  y_pred = np.array(output.tolist()[0])  \n","\n","  loss = mse_loss(y_true, y_pred)\n","  print(loss)\n","\n","if __name__ == \"__main__\":\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["len(squared_difference)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.0805e-08)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(2.1609e-08)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---- torch results matched backward pass results ----\n","x0.data.item()  = -3.000000\n","x0.grad.item()  =  1.000000\n","w0.data.item()  =  2.000000\n","w0.grad.item()  = -1.500000 <-- result matched micrograd\n","---\n","x1.data.item()  =  0.000000\n","x1.grad.item()  =  0.500000\n","w1.data.item()  =  1.000000\n","w1.grad.item()  =  0.000000\n","---\n","x2.data.item()  =  0.500000\n","x2.grad.item()  =  0.500000\n","w2.data.item()  =  1.000000\n","w2.grad.item()  =  0.250000\n","---\n","out.data.item() = -0.707107 <-- result matched micrograd\n"]}],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <-- result matched micrograd')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <-- result matched micrograd')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["parameters in MLP: 41\n","\n","i:  0,  -0.9627354317\n","i:  1,  -0.7200995109\n","i:  2,  -0.5294406037\n","i:  3,  -0.3255771667\n","i:  4,  -0.1882503735\n","---\n","i: 36,   0.8819355871\n","i: 37,   0.2578092668\n","i: 38,   0.5531841859\n","i: 39,  -0.3298153319\n","i: 40,   0.9939609390\n"]}],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'parameters in MLP: {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 3)\n","xs_mats:\n","[array([[ 2. ,  3. , -1. ],\n","       [ 3. , -1. ,  0.5]])]\n","\n","xs_mats_T[0].shape: (3, 2)\n","xs_mats_T:\n","[array([[ 2. ,  3. ],\n","       [ 3. , -1. ],\n","       [-1. ,  0.5]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_cnt: 3\n","\n","layer: 0, neuron_cnt: 4\n","----\n","layer: 0, neuron 0\n","w0: -0.9627354,   w0.grad:  0.0000000\n","w1: -0.7200995,   w1.grad:  0.0000000\n","w2: -0.5294406,   w2.grad:  0.0000000\n","b:  -0.3255772\n","\n","layer: 0, neuron 1\n","w0: -0.1882504,   w0.grad:  0.0000000\n","w1:  0.3272965,   w1.grad:  0.0000000\n","w2:  0.2509364,   w2.grad:  0.0000000\n","b:  -0.8916622\n","\n","layer: 0, neuron 2\n","w0: -0.0824149,   w0.grad:  0.0000000\n","w1:  0.4081694,   w1.grad:  0.0000000\n","w2:  0.6741420,   w2.grad:  0.0000000\n","b:  -0.8034486\n","\n","layer: 0, neuron 3\n","w0: -0.7665800,   w0.grad:  0.0000000\n","w1:  0.3952222,   w1.grad:  0.0000000\n","w2: -0.1372611,   w2.grad:  0.0000000\n","b:  -0.7238925\n","\n","------\n","layer: 1, neuron_cnt: 4\n","----\n","layer: 1, neuron 0\n","w0:  0.9705651,   w0.grad:  0.0000000\n","w1:  0.5149971,   w1.grad:  0.0000000\n","w2: -0.7869400,   w2.grad:  0.0000000\n","w3:  0.0694082,   w3.grad:  0.0000000\n","b:  -0.1758308\n","\n","layer: 1, neuron 1\n","w0:  0.2796673,   w0.grad:  0.0000000\n","w1: -0.6813183,   w1.grad:  0.0000000\n","w2:  0.9965673,   w2.grad:  0.0000000\n","w3: -0.9641504,   w3.grad:  0.0000000\n","b:   0.5357624\n","\n","layer: 1, neuron 2\n","w0:  0.5411460,   w0.grad:  0.0000000\n","w1:  0.8555626,   w1.grad:  0.0000000\n","w2:  0.3682823,   w2.grad:  0.0000000\n","w3:  0.5730329,   w3.grad:  0.0000000\n","b:   0.2305817\n","\n","layer: 1, neuron 3\n","w0:  0.6791519,   w0.grad:  0.0000000\n","w1:  0.8764461,   w1.grad:  0.0000000\n","w2: -0.5073044,   w2.grad:  0.0000000\n","w3: -0.8718227,   w3.grad:  0.0000000\n","b:  -0.7784774\n","\n","------\n","layer: 2, neuron_cnt: 1\n","----\n","layer: 2, neuron 0\n","w0:  0.8819356,   w0.grad:  0.0000000\n","w1:  0.2578093,   w1.grad:  0.0000000\n","w2:  0.5531842,   w2.grad:  0.0000000\n","w3: -0.3298153,   w3.grad:  0.0000000\n","b:   0.9939609\n","\n","------\n"]}],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i: 0\n","w_mat(4, 3):\n","[[-0.96273543 -0.72009951 -0.5294406 ]\n"," [-0.18825037  0.32729647  0.25093642]\n"," [-0.08241493  0.40816937  0.67414202]\n"," [-0.76657998  0.3952222  -0.13726107]]\n","b_mat(1, 4):\n","[[-0.32557717 -0.89166222 -0.80344861 -0.72389255]]\n","\n","i: 1\n","w_mat(4, 4):\n","[[ 0.97056514  0.5149971  -0.78693998  0.06940823]\n"," [ 0.27966727 -0.68131832  0.9965673  -0.96415044]\n"," [ 0.54114602  0.85556262  0.36828234  0.57303287]\n"," [ 0.67915194  0.87644611 -0.50730442 -0.87182275]]\n","b_mat(1, 4):\n","[[-0.17583081  0.53576242  0.23058171 -0.77847742]]\n","\n","i: 2\n","w_mat(1, 4):\n","[[ 0.88193559  0.25780927  0.55318419 -0.32981533]]\n","b_mat(1, 1):\n","[[0.99396094]]\n","\n"]}],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","layer: 0\n","weights (4, 3):\n","[[-0.96273543 -0.72009951 -0.5294406 ]\n"," [-0.18825037  0.32729647  0.25093642]\n"," [-0.08241493  0.40816937  0.67414202]\n"," [-0.76657998  0.3952222  -0.13726107]]\n","\n","input (3, 2):\n","[[ 2.   3. ]\n"," [ 3.  -1. ]\n"," [-1.   0.5]]\n","\n","weights_x_inputs (4, 2):\n","[[-3.55632879 -2.43282709]\n"," [ 0.35445223 -0.76657937]\n"," [ 0.38553624 -0.31834315]\n"," [-0.21023228 -2.76359269]]\n","\n","bias (4, 1):\n","[[-0.32557717]\n"," [-0.89166222]\n"," [-0.80344861]\n"," [-0.72389255]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-3.88190596 -2.75840425]\n"," [-0.53721    -1.6582416 ]\n"," [-0.41791237 -1.12179176]\n"," [-0.93412483 -3.48748523]]\n","\n","output (4, 2):\n","[[-0.99915069 -0.99199486]\n"," [-0.49087313 -0.92997995]\n"," [-0.39517026 -0.80819125]\n"," [-0.73251124 -0.99813176]]\n","\n","--------------------------------------------------\n","layer: 1\n","weights (4, 4):\n","[[ 0.97056514  0.5149971  -0.78693998  0.06940823]\n"," [ 0.27966727 -0.68131832  0.9965673  -0.96415044]\n"," [ 0.54114602  0.85556262  0.36828234  0.57303287]\n"," [ 0.67915194  0.87644611 -0.50730442 -0.87182275]]\n","\n","input (4, 2):\n","[[-0.99915069 -0.99199486]\n"," [-0.49087313 -0.92997995]\n"," [-0.39517026 -0.80819125]\n"," [-0.73251124 -0.99813176]]\n","\n","weights_x_inputs (4, 2):\n","[[-0.9624061  -0.87501316]\n"," [ 0.36744839  0.51311608]\n"," [-1.52594637 -2.20207503]\n"," [-0.2697074  -0.20859958]]\n","\n","bias (4, 1):\n","[[-0.17583081]\n"," [ 0.53576242]\n"," [ 0.23058171]\n"," [-0.77847742]]\n","\n","weights_x_inputs_plus_bias (4, 2):\n","[[-1.13823691 -1.05084397]\n"," [ 0.90321081  1.0488785 ]\n"," [-1.29536466 -1.97149332]\n"," [-1.04818483 -0.987077  ]]\n","\n","output (4, 2):\n","[[-0.81381956 -0.78213426]\n"," [ 0.71785768  0.78136996]\n"," [-0.86052509 -0.96195722]\n"," [-0.78109966 -0.75611319]]\n","\n","--------------------------------------------------\n","layer: 2\n","weights (1, 4):\n","[[ 0.88193559  0.25780927  0.55318419 -0.32981533]]\n","\n","input (4, 2):\n","[[-0.81381956 -0.78213426]\n"," [ 0.71785768  0.78136996]\n"," [-0.86052509 -0.96195722]\n"," [-0.78109966 -0.75611319]]\n","\n","weights_x_inputs (1, 2):\n","[[-0.7510763  -0.77110942]]\n","\n","bias (1, 1):\n","[[0.99396094]]\n","\n","weights_x_inputs_plus_bias (1, 2):\n","[[0.24288464 0.22285152]]\n","\n","output (1, 2):\n","[[0.23821856 0.21923421]]\n","\n","-- manual forward pass calculation --\n","manual calculation: [0.23821856 0.21923421]\n","desired output:     [1.0, -1.0]\n","loss:               2.0668430401451197\n"]}],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- micrograd forward pass calculation --\n","ypred_data:         [0.23821855523695984, 0.21923421480856373]\n","ys:                 [1.0, -1.0]\n","loss_data:          2.0668430401451197\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["=== update parameters ===\n","  i  parameter before         gradient     learning rate      parameter after\n","  0     -0.9627354317     0.0303177802           0.05000        -0.9642513207\n","  1     -0.7200995109    -0.0129184242           0.05000        -0.7194535897\n","  2     -0.5294406037     0.0060756897           0.05000        -0.5297443881\n","  3     -0.3255771667     0.0098502452           0.05000        -0.3260696789\n","  4     -0.1882503735    -0.1526408046           0.05000        -0.1806183333\n","  5      0.3272964655    -0.2625350148           0.05000         0.3404232163\n","  6      0.2509364242     0.0885290597           0.05000         0.2465099712\n","  7     -0.8916622221    -0.0793725667           0.05000        -0.8876935937\n","  8     -0.0824149300    -0.2193731792           0.05000        -0.0714462711\n","  9      0.4081693709     0.0364210559           0.05000         0.4063483181\n"," 10      0.6741420166    -0.0232155285           0.05000         0.6753027930\n"," 11     -0.8034486061    -0.0764610601           0.05000        -0.7996255531\n"," 12     -0.7665799824    -0.1240374649           0.05000        -0.7603781091\n"," 13      0.3952222031    -0.1895720664           0.05000         0.4047008064\n"," 14     -0.1372610719     0.0632972303           0.05000        -0.1404259334\n"," 15     -0.7238925486    -0.0623383569           0.05000        -0.7207756308\n"," 16      0.9705651396    -0.3608525225           0.05000         0.9886077658\n"," 17      0.5149970997    -0.5291067355           0.05000         0.5414524364\n"," 18     -0.7869399822    -0.4732633381           0.05000        -0.7632768153\n"," 19      0.0694082297    -0.4798546378           0.05000         0.0934009616\n"," 20     -0.1758308114     0.3668519831           0.05000        -0.1941734105\n"," 21      0.2796672661    -0.0517839631           0.05000         0.2822564643\n"," 22     -0.6813183167    -0.1286032412           0.05000        -0.6748881547\n"," 23      0.9965673017    -0.1174035652           0.05000         1.0024374800\n"," 24     -0.9641504369    -0.1010956488           0.05000        -0.9590956545\n"," 25      0.5357624228     0.0534972121           0.05000         0.5330875622\n"," 26      0.5411460228     0.1110452291           0.05000         0.5355937614\n"," 27      0.8555626249     0.0121335722           0.05000         0.8549559463\n"," 28      0.3682823432     0.0040630041           0.05000         0.3680791930\n"," 29      0.5730328651     0.0554505747           0.05000         0.5702603364\n"," 30      0.2305817101    -0.1104532078           0.05000         0.2361043705\n"," 31      0.6791519440     0.1406321055           0.05000         0.6721203388\n"," 32      0.8764461078     0.2142255149           0.05000         0.8657348321\n"," 33     -0.5073044245     0.1919769835           0.05000        -0.5169032737\n"," 34     -0.8718227457     0.1919183922           0.05000        -0.8814186653\n"," 35     -0.7784774250    -0.1431000122           0.05000        -0.7713224244\n"," 36      0.8819355871    -0.6459992250           0.05000         0.9142355483\n"," 37      0.2578092668     0.7821322512           0.05000         0.2187026543\n"," 38      0.5531841859    -0.9962956009           0.05000         0.6029989659\n"," 39     -0.3298153319    -0.6326192701           0.05000        -0.2981843684\n"," 40      0.9939609390     0.8841631379           0.05000         0.9497527821\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.06027715782671931), Value(data = 0.009647274695321763)]\n","step: 0, loss: 1.9024666394019192\n","-------\n","ypred: [Value(data = 0.0445247406786087), Value(data = -0.046844272874167266)]\n","step: 1, loss: 1.821438811328055\n","-------\n","ypred: [Value(data = 0.10569271978278669), Value(data = -0.07828550212748286)]\n","step: 2, loss: 1.6493431270378958\n","-------\n","ypred: [Value(data = 0.317676591925823), Value(data = -0.13809202802959894)]\n","step: 3, loss: 1.2084505853520895\n","-------\n","ypred: [Value(data = 0.6001918278798446), Value(data = -0.29218665257286197)]\n","step: 4, loss: 0.6608463092900702\n","-------\n","ypred: [Value(data = 0.663561517566777), Value(data = -0.4709264585476545)]\n","step: 5, loss: 0.3931096647268969\n","-------\n","ypred: [Value(data = 0.7178403911207505), Value(data = -0.5681445371527375)]\n","step: 6, loss: 0.2661131856739144\n","-------\n","ypred: [Value(data = 0.7581296013780221), Value(data = -0.6303797630057054)]\n","step: 7, loss: 0.19512040932527297\n","-------\n","ypred: [Value(data = 0.7889370980959087), Value(data = -0.6739513522921441)]\n","step: 8, loss: 0.15085526923229758\n","-------\n","ypred: [Value(data = 0.812887594170148), Value(data = -0.70635479647697)]\n","step: 9, loss: 0.12123855796751695\n","-------\n","ypred: [Value(data = 0.8317911087733509), Value(data = -0.7315037711665494)]\n","step: 10, loss: 0.10038445598548332\n","-------\n","ypred: [Value(data = 0.8469518494470065), Value(data = -0.7516564498264514)]\n","step: 11, loss: 0.0850982553004936\n","-------\n","ypred: [Value(data = 0.8593098467564801), Value(data = -0.7682139785162875)]\n","step: 12, loss: 0.07351847897493315\n","-------\n","ypred: [Value(data = 0.8695416209211284), Value(data = -0.7820941277305945)]\n","step: 13, loss: 0.06450235784137702\n","-------\n","ypred: [Value(data = 0.8781363588375254), Value(data = -0.7939239745825762)]\n","step: 14, loss: 0.05731807528921906\n","-------\n","ypred: [Value(data = 0.8854513127259493), Value(data = -0.8041468861218465)]\n","step: 15, loss: 0.051479843971977225\n","-------\n","ypred: [Value(data = 0.8917507645646261), Value(data = -0.8130854034760914)]\n","step: 16, loss: 0.04665496336603857\n","-------\n","ypred: [Value(data = 0.8972330062375498), Value(data = -0.8209799234165758)]\n","step: 17, loss: 0.04260924282690656\n","-------\n","ypred: [Value(data = 0.9020489699344819), Value(data = -0.8280134010624524)]\n","step: 18, loss: 0.03917379450500089\n","-------\n","ypred: [Value(data = 0.9063151579057289), Value(data = -0.834327634213062)]\n","step: 19, loss: 0.03622418242366949\n","-------\n","ypred: [Value(data = 0.9101227007640552), Value(data = -0.8400342964744498)]\n","step: 20, loss: 0.03366695522237178\n","-------\n","ypred: [Value(data = 0.9135437780284861), Value(data = -0.8452225941353071)]\n","step: 21, loss: 0.03143072368379156\n","-------\n","ypred: [Value(data = 0.9166362270143448), Value(data = -0.8499646940514903)]\n","step: 22, loss: 0.02946011167746676\n","-------\n","ypred: [Value(data = 0.9194468942461417), Value(data = -0.8543196450550988)]\n","step: 23, loss: 0.02771156866346467\n","-------\n","ypred: [Value(data = 0.9220141036955406), Value(data = -0.858336259655166)]\n","step: 24, loss: 0.02615041535089844\n","-------\n","ypred: [Value(data = 0.9243694970842469), Value(data = -0.8620552645201031)]\n","step: 25, loss: 0.02474872301790848\n","-------\n","ypred: [Value(data = 0.9265394222228851), Value(data = -0.8655109278499851)]\n","step: 26, loss: 0.02348376701511947\n","-------\n","ypred: [Value(data = 0.9285459921217677), Value(data = -0.868732306663128)]\n","step: 27, loss: 0.022336882555845566\n","-------\n","ypred: [Value(data = 0.9304079014905363), Value(data = -0.8717442139713568)]\n","step: 28, loss: 0.021292606824776\n","-------\n","ypred: [Value(data = 0.9321410624673585), Value(data = -0.874567976818352)]\n","step: 29, loss: 0.020338027842500432\n","-------\n","ypred: [Value(data = 0.9337591042306381), Value(data = -0.8772220362906921)]\n","step: 30, loss: 0.019462284644931598\n","-------\n","ypred: [Value(data = 0.9352737690884719), Value(data = -0.879722426793069)]\n","step: 31, loss: 0.018656179584561096\n","-------\n","ypred: [Value(data = 0.9366952290887627), Value(data = -0.8820831621295848)]\n","step: 32, loss: 0.017911874673482018\n","-------\n","ypred: [Value(data = 0.9380323410618799), Value(data = -0.8843165489630171)]\n","step: 33, loss: 0.01722265159809718\n","-------\n","ypred: [Value(data = 0.9392928535671351), Value(data = -0.8864334431797383)]\n","step: 34, loss: 0.016582720456031038\n","-------\n","ypred: [Value(data = 0.9404835759661898), Value(data = -0.8884434609957107)]\n","step: 35, loss: 0.01598706612438782\n","-------\n","ypred: [Value(data = 0.9416105174481195), Value(data = -0.8903551539074399)]\n","step: 36, loss: 0.015431323947337545\n","-------\n","ypred: [Value(data = 0.9426790020438752), Value(data = -0.8921761545507796)]\n","step: 37, loss: 0.01491167845414343\n","-------\n","ypred: [Value(data = 0.9436937643250073), Value(data = -0.8939132989911922)]\n","step: 38, loss: 0.014424780306820002\n","-------\n","ypred: [Value(data = 0.9446590294640685), Value(data = -0.8955727297979532)]\n","step: 39, loss: 0.013967677781710122\n","-------\n","ypred: [Value(data = 0.9455785805572123), Value(data = -0.8971599833564023)]\n","step: 40, loss: 0.013537759917423271\n","-------\n","ypred: [Value(data = 0.9464558155118543), Value(data = -0.8986800641778184)]\n","step: 41, loss: 0.013132709087511574\n","-------\n","ypred: [Value(data = 0.9472937953392638), Value(data = -0.9001375084256069)]\n","step: 42, loss: 0.012750461233185137\n","-------\n","ypred: [Value(data = 0.9480952853308189), Value(data = -0.9015364384523242)]\n","step: 43, loss: 0.01238917235754204\n","-------\n","ypred: [Value(data = 0.9488627903131809), Value(data = -0.9028806098072574)]\n","step: 44, loss: 0.0120471901659639\n","-------\n","ypred: [Value(data = 0.949598584953752), Value(data = -0.9041734519083156)]\n","step: 45, loss: 0.011723029957832063\n","-------\n","ypred: [Value(data = 0.950304739909901), Value(data = -0.9054181033595265)]\n","step: 46, loss: 0.011415354047531785\n","-------\n","ypred: [Value(data = 0.9509831444733478), Value(data = -0.9066174427246741)]\n","step: 47, loss: 0.011122954129000212\n","-------\n","ypred: [Value(data = 0.9516355262469591), Value(data = -0.9077741154297061)]\n","step: 48, loss: 0.010844736106181755\n","-------\n","ypred: [Value(data = 0.9522634682990742), Value(data = -0.9088905573545523)]\n","step: 49, loss: 0.010579706997997627\n","-------\n","ypred: [Value(data = 0.9528684241657275), Value(data = -0.9099690155836242)]\n","step: 50, loss: 0.010326963595603485\n","-------\n","ypred: [Value(data = 0.953451731010217), Value(data = -0.9110115667093803)]\n","step: 51, loss: 0.010085682605464266\n","-------\n","ypred: [Value(data = 0.9540146211995845), Value(data = -0.9120201330216661)]\n","step: 52, loss: 0.00985511205694304\n","-------\n","ypred: [Value(data = 0.9545582325165605), Value(data = -0.912996496864567)]\n","step: 53, loss: 0.009634563789856289\n","-------\n","ypred: [Value(data = 0.9550836171916629), Value(data = -0.9139423134001722)]\n","step: 54, loss: 0.009423406867499266\n","-------\n","ypred: [Value(data = 0.9555917499120675), Value(data = -0.9148591219833699)]\n","step: 55, loss: 0.009221061785315034\n","-------\n","ypred: [Value(data = 0.956083534940507), Value(data = -0.9157483563222825)]\n","step: 56, loss: 0.00902699536571875\n","-------\n","ypred: [Value(data = 0.9565598124579487), Value(data = -0.9166113535741663)]\n","step: 57, loss: 0.008840716246421286\n","-------\n","ypred: [Value(data = 0.9570213642274477), Value(data = -0.917449362505727)]\n","step: 58, loss: 0.008661770883580595\n","-------\n","ypred: [Value(data = 0.95746891866282), Value(data = -0.9182635508291498)]\n","step: 59, loss: 0.008489740002768798\n","-------\n","ypred: [Value(data = 0.9579031553741807), Value(data = -0.9190550118101796)]\n","step: 60, loss: 0.008324235440500542\n","-------\n","ypred: [Value(data = 0.9583247092525651), Value(data = -0.9198247702318589)]\n","step: 61, loss: 0.008164897327257448\n","-------\n","ypred: [Value(data = 0.9587341741475189), Value(data = -0.9205737877866682)]\n","step: 62, loss: 0.008011391569844512\n","-------\n","ypred: [Value(data = 0.9591321061844373), Value(data = -0.9213029679605192)]\n","step: 63, loss: 0.007863407596743165\n","-------\n","ypred: [Value(data = 0.9595190267623732), Value(data = -0.9220131604640875)]\n","step: 64, loss: 0.0077206563350656135\n","-------\n","ypred: [Value(data = 0.9598954252678426), Value(data = -0.922705165260102)]\n","step: 65, loss: 0.007582868391915337\n","-------\n","ypred: [Value(data = 0.9602617615356948), Value(data = -0.9233797362293041)]\n","step: 66, loss: 0.007449792416536998\n","-------\n","ypred: [Value(data = 0.9606184680842828), Value(data = -0.9240375845126692)]\n","step: 67, loss: 0.007321193622698538\n","-------\n","ypred: [Value(data = 0.9609659521488512), Value(data = -0.9246793815630527)]\n","step: 68, loss: 0.007196852453369977\n","-------\n","ypred: [Value(data = 0.9613045975342033), Value(data = -0.9253057619355753)]\n","step: 69, loss: 0.007076563372014932\n","-------\n","ypred: [Value(data = 0.9616347663052188), Value(data = -0.9259173258427025)]\n","step: 70, loss: 0.006960133766751492\n","-------\n","ypred: [Value(data = 0.9619568003316388), Value(data = -0.9265146414970603)]\n","step: 71, loss: 0.006847382955312371\n","-------\n","ypred: [Value(data = 0.962271022701655), Value(data = -0.927098247262457)]\n","step: 72, loss: 0.006738141280184885\n","-------\n","ypred: [Value(data = 0.9625777390172019), Value(data = -0.9276686536313535)]\n","step: 73, loss: 0.00663224928456576\n","-------\n","ypred: [Value(data = 0.9628772385824105), Value(data = -0.928226345045044)]\n","step: 74, loss: 0.006529556960860357\n","-------\n","ypred: [Value(data = 0.9631697954954329), Value(data = -0.9287717815710911)]\n","step: 75, loss: 0.006429923064404593\n","-------\n","ypred: [Value(data = 0.963455669652738), Value(data = -0.9293054004510214)]\n","step: 76, loss: 0.0063332144859202626\n","-------\n","ypred: [Value(data = 0.9637351076740088), Value(data = -0.9298276175299514)]\n","step: 77, loss: 0.006239305676938523\n","-------\n","ypred: [Value(data = 0.9640083437549195), Value(data = -0.9303388285786127)]\n","step: 78, loss: 0.006148078123063946\n","-------\n","ypred: [Value(data = 0.9642756004543097), Value(data = -0.9308394105172008)]\n","step: 79, loss: 0.006059419860508397\n","-------\n","ypred: [Value(data = 0.9645370894216104), Value(data = -0.9313297225495228)]\n","step: 80, loss: 0.005973225031816366\n","-------\n","ypred: [Value(data = 0.9647930120697814), Value(data = -0.931810107215098)]\n","step: 81, loss: 0.0058893934771349805\n","-------\n","ypred: [Value(data = 0.9650435601984965), Value(data = -0.9322808913661159)]\n","step: 82, loss: 0.005807830357763935\n","-------\n","ypred: [Value(data = 0.9652889165718429), Value(data = -0.932742387075498)]\n","step: 83, loss: 0.005728445809058628\n","-------\n","ypred: [Value(data = 0.9655292554543929), Value(data = -0.9331948924817224)]\n","step: 84, loss: 0.005651154620057132\n","-------\n","ypred: [Value(data = 0.9657647431091254), Value(data = -0.9336386925755357)]\n","step: 85, loss: 0.0055758759374684415\n","-------\n","ypred: [Value(data = 0.9659955382603544), Value(data = -0.9340740599332076)]\n","step: 86, loss: 0.0055025329918933245\n","-------\n","ypred: [Value(data = 0.9662217925245183), Value(data = -0.9345012554005598)]\n","step: 87, loss: 0.005431052844359376\n","-------\n","ypred: [Value(data = 0.9664436508114239), Value(data = -0.9349205287316208)]\n","step: 88, loss: 0.005361366151437446\n","-------\n","ypred: [Value(data = 0.9666612516982975), Value(data = -0.9353321191854086)]\n","step: 89, loss: 0.0052934069473744684\n","-------\n","ypred: [Value(data = 0.9668747277787886), Value(data = -0.9357362560840446)]\n","step: 90, loss: 0.00522711244182485\n","-------\n","ypred: [Value(data = 0.9670842059888713), Value(data = -0.9361331593351174)]\n","step: 91, loss: 0.005162422831896551\n","-------\n","ypred: [Value(data = 0.9672898079114258), Value(data = -0.9365230399209645)]\n","step: 92, loss: 0.005099281127346884\n","-------\n","ypred: [Value(data = 0.9674916500611186), Value(data = -0.9369061003573178)]\n","step: 93, loss: 0.005037632987869623\n","-------\n","ypred: [Value(data = 0.967689844151065), Value(data = -0.9372825351235449)]\n","step: 94, loss: 0.0049774265715118535\n","-------\n","ypred: [Value(data = 0.9678844973426295), Value(data = -0.9376525310665418)]\n","step: 95, loss: 0.00491861239334411\n","-------\n","ypred: [Value(data = 0.9680757124796036), Value(data = -0.9380162677801569)]\n","step: 96, loss: 0.004861143193586156\n","-------\n","ypred: [Value(data = 0.9682635883078982), Value(data = -0.9383739179618752)]\n","step: 97, loss: 0.004804973814460267\n","-------\n","ypred: [Value(data = 0.9684482196817945), Value(data = -0.9387256477483541)]\n","step: 98, loss: 0.004750061085107083\n","-------\n","ypred: [Value(data = 0.9686296977577087), Value(data = -0.939071617031272)]\n","step: 99, loss: 0.004696363713956697\n","-------\n","ypred: [Value(data = 0.9688081101763527), Value(data = -0.9394119797548383)]\n","step: 100, loss: 0.0046438421879986804\n","-------\n","ypred: [Value(data = 0.9689835412340988), Value(data = -0.9397468841962094)]\n","step: 101, loss: 0.004592458678441845\n","-------\n","ypred: [Value(data = 0.9691560720442925), Value(data = -0.9400764732299535)]\n","step: 102, loss: 0.004542176952297355\n","-------\n","ypred: [Value(data = 0.9693257806892035), Value(data = -0.940400884577625)]\n","step: 103, loss: 0.0044929622894564125\n","-------\n","ypred: [Value(data = 0.9694927423632437), Value(data = -0.9407202510434299)]\n","step: 104, loss: 0.004444781404869397\n","-------\n","ypred: [Value(data = 0.9696570295080399), Value(data = -0.9410347007368819)]\n","step: 105, loss: 0.004397602375465039\n","-------\n","ypred: [Value(data = 0.9698187119399), Value(data = -0.9413443572832925)]\n","step: 106, loss: 0.0043513945714767814\n","-------\n","ypred: [Value(data = 0.9699778569701715), Value(data = -0.9416493400228692)]\n","step: 107, loss: 0.0043061285918702214\n","-------\n","ypred: [Value(data = 0.9701345295189546), Value(data = -0.9419497641991426)]\n","step: 108, loss: 0.004261776203589343\n","-------\n","ypred: [Value(data = 0.970288792222599), Value(data = -0.9422457411373896)]\n","step: 109, loss: 0.004218310284361309\n","-------\n","ypred: [Value(data = 0.9704407055353768), Value(data = -0.9425373784136752)]\n","step: 110, loss: 0.004175704768819462\n","-------\n","ypred: [Value(data = 0.9705903278257042), Value(data = -0.9428247800150857)]\n","step: 111, loss: 0.004133934597722887\n","-------\n","ypred: [Value(data = 0.9707377154672492), Value(data = -0.9431080464916938)]\n","step: 112, loss: 0.004092975670066945\n","-------\n","ypred: [Value(data = 0.9708829229252435), Value(data = -0.9433872751007508)]\n","step: 113, loss: 0.00405280479789538\n","-------\n","ypred: [Value(data = 0.9710260028382939), Value(data = -0.9436625599435726)]\n","step: 114, loss: 0.004013399663638103\n","-------\n","ypred: [Value(data = 0.9711670060959655), Value(data = -0.9439339920955506)]\n","step: 115, loss: 0.00397473877981187\n","-------\n","ypred: [Value(data = 0.9713059819123924), Value(data = -0.9442016597296932)]\n","step: 116, loss: 0.003936801450932899\n","-------\n","ypred: [Value(data = 0.971442977896153), Value(data = -0.9444656482340708)]\n","step: 117, loss: 0.003899567737501577\n","-------\n","ypred: [Value(data = 0.971578040116632), Value(data = -0.944726040323521)]\n","step: 118, loss: 0.0038630184219288054\n","-------\n","ypred: [Value(data = 0.9717112131670732), Value(data = -0.9449829161459373)]\n","step: 119, loss: 0.0038271349762837364\n","-------\n","ypred: [Value(data = 0.971842540224518), Value(data = -0.9452363533834501)]\n","step: 120, loss: 0.0037918995317502407\n","-------\n","ypred: [Value(data = 0.9719720631068078), Value(data = -0.9454864273487884)]\n","step: 121, loss: 0.003757294849687687\n","-------\n","ypred: [Value(data = 0.9720998223268186), Value(data = -0.9457332110770885)]\n","step: 122, loss: 0.0037233042941989193\n","-------\n","ypred: [Value(data = 0.972225857144084), Value(data = -0.9459767754134025)]\n","step: 123, loss: 0.003689911806114777\n","-------\n","ypred: [Value(data = 0.9723502056139556), Value(data = -0.9462171890961424)]\n","step: 124, loss: 0.0036571018783106393\n","-------\n","ypred: [Value(data = 0.9724729046344349), Value(data = -0.9464545188366794)]\n","step: 125, loss: 0.003624859532276434\n","-------\n","ypred: [Value(data = 0.9725939899908077), Value(data = -0.9466888293953074)]\n","step: 126, loss: 0.0035931702958665874\n","-------\n","ypred: [Value(data = 0.9727134963982011), Value(data = -0.946920183653762)]\n","step: 127, loss: 0.0035620201821613406\n","-------\n","ypred: [Value(data = 0.9728314575421755), Value(data = -0.9471486426844828)]\n","step: 128, loss: 0.003531395669375082\n","-------\n","ypred: [Value(data = 0.9729479061174575), Value(data = -0.9473742658167854)]\n","step: 129, loss: 0.0035012836817522527\n","-------\n","ypred: [Value(data = 0.9730628738649139), Value(data = -0.9475971107001041)]\n","step: 130, loss: 0.0034716715713946846\n","-------\n","ypred: [Value(data = 0.9731763916068601), Value(data = -0.9478172333644637)]\n","step: 131, loss: 0.00344254710096737\n","-------\n","ypred: [Value(data = 0.9732884892807893), Value(data = -0.9480346882783143)]\n","step: 132, loss: 0.003413898427234478\n","-------\n","ypred: [Value(data = 0.9733991959716066), Value(data = -0.948249528403868)]\n","step: 133, loss: 0.0033857140853790543\n","-------\n","ypred: [Value(data = 0.9735085399424451), Value(data = -0.9484618052500644)]\n","step: 134, loss: 0.003357982974063314\n","-------\n","ypred: [Value(data = 0.9736165486641353), Value(data = -0.9486715689232804)]\n","step: 135, loss: 0.0033306943411894946\n","-------\n","ypred: [Value(data = 0.9737232488433996), Value(data = -0.9488788681759012)]\n","step: 136, loss: 0.003303837770322793\n","-------\n","ypred: [Value(data = 0.9738286664498336), Value(data = -0.9490837504528533)]\n","step: 137, loss: 0.0032774031677413823\n","-------\n","ypred: [Value(data = 0.9739328267417366), Value(data = -0.9492862619362056)]\n","step: 138, loss: 0.003251380750079471\n","-------\n","ypred: [Value(data = 0.9740357542908484), Value(data = -0.9494864475879241)]\n","step: 139, loss: 0.0032257610325327336\n","-------\n","ypred: [Value(data = 0.9741374730060468), Value(data = -0.9496843511908802)]\n","step: 140, loss: 0.0032005348175956324\n","-------\n","ypred: [Value(data = 0.9742380061560558), Value(data = -0.9498800153881857)]\n","step: 141, loss: 0.0031756931843039226\n","-------\n","ypred: [Value(data = 0.974337376391215), Value(data = -0.9500734817209431)]\n","step: 142, loss: 0.003151227477955175\n","-------\n","ypred: [Value(data = 0.9744356057643527), Value(data = -0.9502647906644789)]\n","step: 143, loss: 0.0031271293002837017\n","-------\n","ypred: [Value(data = 0.9745327157508085), Value(data = -0.9504539816631382)]\n","step: 144, loss: 0.0031033905000657637\n","-------\n","ypred: [Value(data = 0.974628727267644), Value(data = -0.950641093163701)]\n","step: 145, loss: 0.0030800031641340284\n","-------\n","ypred: [Value(data = 0.9747236606920817), Value(data = -0.950826162647488)]\n","step: 146, loss: 0.003056959608780321\n","-------\n","ypred: [Value(data = 0.9748175358792067), Value(data = -0.9510092266612155)]\n","step: 147, loss: 0.0030342523715272034\n","-------\n","ypred: [Value(data = 0.9749103721789674), Value(data = -0.9511903208466563)]\n","step: 148, loss: 0.003011874203250286\n","-------\n","ypred: [Value(data = 0.9750021884525063), Value(data = -0.9513694799691604)]\n","step: 149, loss: 0.002989818060633897\n","-------\n","ypred: [Value(data = 0.9750930030878523), Value(data = -0.9515467379450867)]\n","step: 150, loss: 0.0029680770989438353\n","-------\n","ypred: [Value(data = 0.9751828340150045), Value(data = -0.9517221278681952)]\n","step: 151, loss: 0.002946644665101711\n","-------\n","ypred: [Value(data = 0.9752716987204332), Value(data = -0.9518956820350452)]\n","step: 152, loss: 0.0029255142910465035\n","-------\n","ypred: [Value(data = 0.9753596142610266), Value(data = -0.9520674319694447)]\n","step: 153, loss: 0.0029046796873692213\n","-------\n","ypred: [Value(data = 0.9754465972775064), Value(data = -0.9522374084459896)]\n","step: 154, loss: 0.0028841347372081843\n","-------\n","ypred: [Value(data = 0.9755326640073372), Value(data = -0.9524056415127354)]\n","step: 155, loss: 0.002863873490392106\n","-------\n","ypred: [Value(data = 0.9756178302971504), Value(data = -0.9525721605130386)]\n","step: 156, loss: 0.002843890157819529\n","-------\n","ypred: [Value(data = 0.9757021116147064), Value(data = -0.9527369941066003)]\n","step: 157, loss: 0.00282417910606372\n","-------\n","ypred: [Value(data = 0.9757855230604127), Value(data = -0.9529001702897506)]\n","step: 158, loss: 0.002804734852192299\n","-------\n","ypred: [Value(data = 0.9758680793784182), Value(data = -0.9530617164150027)]\n","step: 159, loss: 0.0027855520587919498\n","-------\n","ypred: [Value(data = 0.9759497949673036), Value(data = -0.9532216592099091)]\n","step: 160, loss: 0.002766625529188618\n","-------\n","ypred: [Value(data = 0.9760306838903815), Value(data = -0.9533800247952474)]\n","step: 161, loss: 0.0027479502028545624\n","-------\n","ypred: [Value(data = 0.9761107598856267), Value(data = -0.9535368387025669)]\n","step: 162, loss: 0.00272952115099347\n","-------\n","ypred: [Value(data = 0.9761900363752491), Value(data = -0.953692125891118)]\n","step: 163, loss: 0.0027113335722960263\n","-------\n","ypred: [Value(data = 0.9762685264749272), Value(data = -0.9538459107641939)]\n","step: 164, loss: 0.0026933827888579827\n","-------\n","ypred: [Value(data = 0.9763462430027128), Value(data = -0.9539982171849055)]\n","step: 165, loss: 0.0026756642422538363\n","-------\n","ypred: [Value(data = 0.9764231984876246), Value(data = -0.9541490684914152)]\n","step: 166, loss: 0.0026581734897588786\n","-------\n","ypred: [Value(data = 0.9764994051779403), Value(data = -0.9542984875116475)]\n","step: 167, loss: 0.0026409062007136603\n","-------\n","ypred: [Value(data = 0.9765748750492005), Value(data = -0.9544464965775031)]\n","step: 168, loss: 0.002623858153024004\n","-------\n","ypred: [Value(data = 0.9766496198119372), Value(data = -0.9545931175385902)]\n","step: 169, loss: 0.0026070252297913585\n","-------\n","ypred: [Value(data = 0.9767236509191362), Value(data = -0.9547383717754971)]\n","step: 170, loss: 0.002590403416067345\n","-------\n","ypred: [Value(data = 0.9767969795734459), Value(data = -0.9548822802126201)]\n","step: 171, loss: 0.0025739887957276163\n","-------\n","ypred: [Value(data = 0.9768696167341412), Value(data = -0.9550248633305679)]\n","step: 172, loss: 0.0025577775484596165\n","-------\n","ypred: [Value(data = 0.9769415731238531), Value(data = -0.955166141178155)]\n","step: 173, loss: 0.002541765946859741\n","-------\n","ypred: [Value(data = 0.9770128592350724), Value(data = -0.9553061333840047)]\n","step: 174, loss: 0.002525950353634973\n","-------\n","ypred: [Value(data = 0.977083485336438), Value(data = -0.9554448591677714)]\n","step: 175, loss: 0.002510327218904974\n","-------\n","ypred: [Value(data = 0.9771534614788159), Value(data = -0.9555823373510015)]\n","step: 176, loss: 0.0024948930776001856\n","-------\n","ypred: [Value(data = 0.9772227975011792), Value(data = -0.9557185863676432)]\n","step: 177, loss: 0.0024796445469521637\n","-------\n","ypred: [Value(data = 0.977291503036295), Value(data = -0.95585362427422)]\n","step: 178, loss: 0.002464578324072334\n","-------\n","ypred: [Value(data = 0.9773595875162272), Value(data = -0.9559874687596824)]\n","step: 179, loss: 0.0024496911836153048\n","-------\n","ypred: [Value(data = 0.9774270601776607), Value(data = -0.956120137154944)]\n","step: 180, loss: 0.002434979975523875\n","-------\n","ypred: [Value(data = 0.9774939300670549), Value(data = -0.9562516464421209)]\n","step: 181, loss: 0.002420441622851809\n","-------\n","ypred: [Value(data = 0.9775602060456324), Value(data = -0.9563820132634805)]\n","step: 182, loss: 0.0024060731196616633\n","-------\n","ypred: [Value(data = 0.9776258967942094), Value(data = -0.9565112539301113)]\n","step: 183, loss: 0.002391871528994631\n","-------\n","ypred: [Value(data = 0.9776910108178729), Value(data = -0.956639384430326)]\n","step: 184, loss: 0.0023778339809093158\n","-------\n","ypred: [Value(data = 0.9777555564505117), Value(data = -0.9567664204378045)]\n","step: 185, loss: 0.0023639576705870627\n","-------\n","ypred: [Value(data = 0.9778195418592054), Value(data = -0.9568923773194887)]\n","step: 186, loss: 0.0023502398565008716\n","-------\n","ypred: [Value(data = 0.977882975048477), Value(data = -0.957017270143236)]\n","step: 187, loss: 0.002336677858645842\n","-------\n","ypred: [Value(data = 0.9779458638644143), Value(data = -0.9571411136852422)]\n","step: 188, loss: 0.0023232690568282775\n","-------\n","ypred: [Value(data = 0.9780082159986648), Value(data = -0.9572639224372405)]\n","step: 189, loss: 0.0023100108890115813\n","-------\n","ypred: [Value(data = 0.9780700389923084), Value(data = -0.9573857106134845)]\n","step: 190, loss: 0.002296900849716559\n","-------\n","ypred: [Value(data = 0.9781313402396116), Value(data = -0.9575064921575236)]\n","step: 191, loss: 0.002283936488474236\n","-------\n","ypred: [Value(data = 0.9781921269916699), Value(data = -0.9576262807487778)]\n","step: 192, loss: 0.0022711154083288544\n","-------\n","ypred: [Value(data = 0.9782524063599395), Value(data = -0.9577450898089189)]\n","step: 193, loss: 0.0022584352643895277\n","-------\n","ypred: [Value(data = 0.9783121853196632), Value(data = -0.9578629325080639)]\n","step: 194, loss: 0.0022458937624286112\n","-------\n","ypred: [Value(data = 0.9783714707131955), Value(data = -0.957979821770788)]\n","step: 195, loss: 0.0022334886575249013\n","-------\n","ypred: [Value(data = 0.978430269253228), Value(data = -0.9580957702819647)]\n","step: 196, loss: 0.002221217752750117\n","-------\n","ypred: [Value(data = 0.978488587525921), Value(data = -0.9582107904924345)]\n","step: 197, loss: 0.002209078897897161\n","-------\n","ypred: [Value(data = 0.9785464319939431), Value(data = -0.9583248946245135)]\n","step: 198, loss: 0.0021970699882484096\n","-------\n","ypred: [Value(data = 0.9786038089994216), Value(data = -0.9584380946773436)]\n","step: 199, loss: 0.002185188963382684\n","-------\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 loss: 2.2964746952056885\n","Epoch 10 loss: 0.4762929379940033\n","Epoch 20 loss: 0.02532806806266308\n","Epoch 30 loss: 0.0005137841799296439\n","\n","Prediction:\n","tensor([[ 0.9994],\n","        [-0.9956]])\n","Loss: 1.481742765463423e-05\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input xs:\n","[[2.0, 3.0, -1.0], [3.0, -1.0, 0.5]]\n","\n","target ys:\n","[1.0, -1.0]\n","---------\n","\n","layer: 0.0,  i: 0\n","\n","w,  torch.Size([4, 3]):\n","tensor([[ 0.4602,  0.1719,  0.6784],\n","        [ 0.4791,  0.4752, -0.2096],\n","        [ 0.4061, -0.5889, -0.0201],\n","        [ 0.4957, -0.4599,  0.1445]])\n","\n","input,  torch.Size([3, 2]):\n","tensor([[ 2.0000,  3.0000],\n","        [ 3.0000, -1.0000],\n","        [-1.0000,  0.5000]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[ 0.7578,  1.5480],\n","        [ 2.5935,  0.8573],\n","        [-0.9343,  1.7972],\n","        [-0.5329,  2.0192]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.5577],\n","        [-0.4238],\n","        [-0.4730],\n","        [-0.2376]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[ 0.2002,  0.9903],\n","        [ 2.1698,  0.4335],\n","        [-1.4072,  1.3242],\n","        [-0.7705,  1.7816]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[ 0.1975,  0.7575],\n","        [ 0.9743,  0.4082],\n","        [-0.8869,  0.8678],\n","        [-0.6472,  0.9449]])\n","\n","\n","layer: 1.0,  i: 2\n","\n","w,  torch.Size([4, 4]):\n","tensor([[ 0.0845, -0.2158,  0.2556,  0.2451],\n","        [-0.4152, -0.0120, -0.4505, -0.1268],\n","        [-0.1527, -0.1221,  0.2599,  0.4968],\n","        [ 0.2036, -0.0993,  0.7541, -0.0123]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[ 0.1975,  0.7575],\n","        [ 0.9743,  0.4082],\n","        [-0.8869,  0.8678],\n","        [-0.6472,  0.9449]])\n","\n","w * input,  torch.Size([4, 2]):\n","tensor([[-0.5788,  0.4293],\n","        [ 0.3879, -0.8302],\n","        [-0.7012,  0.5295],\n","        [-0.7173,  0.7565]])\n","\n","bT,  torch.Size([4, 1]):\n","tensor([[-0.2121],\n","        [ 0.0407],\n","        [-0.1676],\n","        [ 0.2338]])\n","\n","w * input + bT,  torch.Size([4, 2]):\n","tensor([[-0.7910,  0.2172],\n","        [ 0.4286, -0.7895],\n","        [-0.8688,  0.3619],\n","        [-0.4836,  0.9903]])\n","\n","output,  torch.Size([4, 2]):\n","tensor([[-0.6590,  0.2138],\n","        [ 0.4042, -0.6581],\n","        [-0.7008,  0.3469],\n","        [-0.4491,  0.7575]])\n","\n","\n","layer: 2.0,  i: 4\n","\n","w,  torch.Size([1, 4]):\n","tensor([[-0.1815,  0.5649, -0.3675, -0.7057]])\n","\n","input,  torch.Size([4, 2]):\n","tensor([[-0.6590,  0.2138],\n","        [ 0.4042, -0.6581],\n","        [-0.7008,  0.3469],\n","        [-0.4491,  0.7575]])\n","\n","w * input,  torch.Size([1, 2]):\n","tensor([[ 0.9224, -1.0726]])\n","\n","bT,  torch.Size([1, 1]):\n","tensor([[0.0770]])\n","\n","w * input + bT,  torch.Size([1, 2]):\n","tensor([[ 0.9994, -0.9956]])\n","\n","output,  torch.Size([1, 2]):\n","tensor([[ 0.9994, -0.9956]])\n","\n","\n"]}],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 2])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9994, -0.9956]]) torch.Size([1, 2])\n","tensor([[ 1., -1.]]) torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor(1.9985e-05)"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.9994, -0.9956]]) torch.Size([1, 2])\n","tensor([ 1., -1.]) torch.Size([2])\n","difference: tensor([[-0.0006,  0.0044]])\n","squared_difference: tensor([[3.1706e-07, 1.9668e-05]])\n"]},{"data":{"text/plain":["tensor(9.9924e-06)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference) / 2\n","loss"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["difference\n","len(squared_difference)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(9.9924e-06)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.9985e-05)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum((output - torch.tensor(ys))**2)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
