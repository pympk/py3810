{"cells":[{"cell_type":"markdown","metadata":{},"source":["### [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3356s)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### [chatGPT-4, released on 2023-03-14, has 1 trillion paramaters and cost $100 million to train](https://en.wikipedia.org/wiki/GPT-4)"]},{"cell_type":"code","execution_count":837,"metadata":{},"outputs":[],"source":["import math, random, torch\n","import numpy as np\n","# import random\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":838,"metadata":{},"outputs":[],"source":["# verbose = True   # print calculation output and weights and bias matrices \n","verbose = False  # print calculation output only"]},{"cell_type":"code","execution_count":839,"metadata":{},"outputs":[],"source":["def plot_losses(losses):\n","  # import matplotlib.pyplot as plt\n","  \n","  # Create a list of iterations\n","  iterations = range(len(losses))\n","\n","  # Plot the loss as a function of iteration\n","  plt.plot(iterations, losses)\n","\n","  # Add a title to the plot\n","  plt.title('Loss vs. Iteration')\n","\n","  # Add labels to the x-axis and y-axis\n","  plt.xlabel('Iteration')\n","  plt.ylabel('Loss')"]},{"cell_type":"code","execution_count":840,"metadata":{},"outputs":[],"source":["def print_parameters(parameters):\n","  # number of parameters (e.g sum (weights + bias to each neuron and output))\n","  # MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","  # print(f'Number of parameters in MLP(2, [3, 3, 1]): {len(parameters())}\\n')\n","  print(f'Total parameters: {len(parameters())}\\n')  \n","\n","  # print first 5 parameters\n","  for i, v in enumerate(parameters()):\n","    if i < 5:\n","      print(f'i: {i:>2}, {v.data:>14.10f}')\n","  \n","  print('---')\n","\n","  # print last 5 parameters   \n","  for i, v in enumerate(parameters()):\n","    if i >= len(parameters()) - 5:\n","      print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"cell_type":"code","execution_count":841,"metadata":{},"outputs":[],"source":["def get_wt_n_b_mats(layers, verbose=False):\n","  ''' Get neuron's weights and bias for each layer.\n","  Inputs: If n = MLP(2, [3, 3, 1]), input is n.layers.\n","\n","  return: two lists of np.arrays. The first list is weight matrix for each layer\n","          The second list is the bias matrix for each layer \n","  '''\n","  layer_cnt = len(layers)  # number of layers\n","  w_mats = []  # list of weights matrix for each layer \n","  b_mats = []  # list of bias matrix for each layer\n","  if verbose:\n","    print(f'layer_cnt: {layer_cnt}\\n')\n","  for i, layer in enumerate(layers):\n","      neuron_cnt = len(layer.neurons)  # numbers of neurons in the layer\n","      if verbose: \n","        print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","        print('----')\n","      b_mat = []  # accumulate neuon's bias for each row     \n","      for j, neuron in enumerate(layer.neurons):\n","          if verbose:\n","            print(f'layer: {i}, neuron {j}')\n","          b = neuron.b.data  # bias of neuron \n","          w_row = []  # accumulate neuon's weights for each row\n","          b_row = []  # accumulate neuon's bias for each row\n","          for k, w in enumerate(neuron.w):\n","              w_row.append(w.data)\n","              if verbose:\n","                print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","          if j == 0:            \n","              w_mat = np.array([w_row])\n","          else:\n","              w_mat = np.vstack((w_mat, w_row))\n","          \n","          b_mat.append(b)\n","          if verbose:\n","            print(f'b:  {b:10.7f}\\n')\n","            print(f'b:  {b:10.7f}')        \n","            print(f'b_mat:  {b_mat}\\n')\n","      w_mats.append(w_mat)  \n","      b_mats.append(np.array([b_mat]))        \n","      if verbose:\n","          print('------')\n","\n","  zipped_w_n_b = zip(w_mats, b_mats)\n","  if verbose:\n","    for i, w_n_b in enumerate(zipped_w_n_b):\n","      print(f'layer: {i}')  # 1st layer is 0    \n","      print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","      print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","\n","  return w_mats, b_mats"]},{"cell_type":"code","execution_count":842,"metadata":{},"outputs":[],"source":["def forward_pass(layers, verbose=verbose):\n","  # Get Neural Network's Weights and Biases Matrices\n","  # w_mats, b_mats = get_wt_n_b_mats(n.layers, verbose=verbose)\n","  w_mats, b_mats = get_wt_n_b_mats(layers, verbose=verbose)\n","\n","  # Calculate Neural Network Output and Loss with Matrix Multiplication\n","  for layer in range(len(layers)):\n","    if layer == 0:  # first layer, use given inputs xs as inputs\n","      input = xs_mats_T[layer]\n","    else:  # after first layer, use outputs from preceding layers as inputs\n","      input = output\n","\n","    weights = w_mats[layer]\n","    bias = np.transpose(b_mats[layer])\n","\n","    weights_x_input = np.matmul(weights, input)\n","    weights_x_input_plus_bias = weights_x_input + bias\n","\n","    # output = np.tanh(np.matmul(weights, input) + bias)\n","    output = np.tanh(weights_x_input_plus_bias)\n","\n","    print(f'{\"-\"*50}')\n","    print(f'Calculate Output of Layer: {layer}')    \n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_+_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')    \n","\n","    # print(f'output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n","    print(f'Layer {layer} Output = tanh(weights_x_inputs_+_bias) {output.shape}:\\n{output}\\n')    \n","\n","  yout = output[0]\n","  err = (yout - ys)\n","  err_sq = (err**2)\n","  loss_sum = err_sq.sum()\n","  loss_mean = err_sq.mean()\n","\n","  # print(f'-- Manual calculation results of neural network output and prediction error --')\n","  print(f'-- Results of neural network outputs and Loss --')  \n","  print(f'yout:           {yout}')   \n","  print(f'desired output: {ys}')   \n","  print(f'err:            {err}')\n","  print(f'err_sq:         {err_sq}')\n","  print(f'loss_mean:      {loss_mean}')\n","  print(f'loss_sum:       {loss_sum}')\n","\n","  return yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Micrograd Classes and Functions<br>* limited to neural network with one output, e.g. MLP(2, [3, 1])<br>* neural network with multiple outputs, e.g.  MLP(2, [3, 3]), will produce errors in backward pass "]},{"cell_type":"code","execution_count":843,"metadata":{},"outputs":[],"source":["from graphviz import Digraph\n","\n","def trace(root):\n","  \"\"\"Builds a set of all nodes and edges in a graph.\"\"\"\n","  nodes, edges = set(), set()\n","\n","  def build(v):\n","    if v not in nodes:\n","      nodes.add(v)\n","      for child in v._prev:\n","        edges.add((child, v))\n","        build(child)\n","\n","  build(root)\n","  return nodes, edges\n","\n","def draw_dot(root):\n","  \"\"\"Creates a Digraph representation of the graph.\"\"\"\n","  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # LR = left to right\n","\n","  nodes, edges = trace(root)\n","  for n in nodes:\n","    uid = str(id(n))\n","    # For any value in the graph, create a rectangular ('record') node for it.\n","    dot.node(name=uid, label=\"{ %s | data %.4f | grad % .4f }\" % (n.label, n.data, n.grad), shape=\"record\")\n","\n","    if n._op:\n","      # If this value is a result of some operation, create an op node.\n","      dot.node(name=uid + n._op, label=n._op)\n","      # And connect this node to it\n","      dot.edge(uid + n._op, uid)\n","\n","  for n1, n2 in edges:\n","    # Connect nl to the op node of n2.\n","    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","  return dot"]},{"cell_type":"code","execution_count":844,"metadata":{},"outputs":[],"source":["class Value:\n","\n","    def __init__(self, data, _children=(), _op='', label=''):\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda : None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","\n","    def __repr__(self) -> str:\n","        return f\"Value(data = {self.data})\"\n","    \n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward    \n","\n","        return out\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)        \n","        out = Value(self.data * other.data, (self, other), '*')\n","\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __rmul__(self, other):  # other * self\n","        return self * other\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only support int/float power for now\"\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += other * (self.data ** (other - 1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other):  # self / other\n","        return self * other**-1\n","\n","    def __neg__(self):  # -self\n","        return self * -1\n","    \n","    def __sub__(self, other):  # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self, ), 'tanh')\n","\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    # https://en.wikipedia.org/wiki/Hyperbolic_functions\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self, ), 'exp')\n","\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def backward(self):\n","        topo = []\n","        visited = set()\n","\n","        # topological sort\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        self.grad = 1  # initialize\n","        for node in reversed(topo):\n","            node._backward()    "]},{"cell_type":"code","execution_count":845,"metadata":{},"outputs":[],"source":["class Neuron:\n","    \n","    def __init__(self, nin):\n","        # random numbers evenly distributed between -1 and 1    \n","        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]  \n","        self.b = Value(random.uniform(-1,1))\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Neuron(w = {self.w}, b = {self.b})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        # w * x + b\n","        # print(list(zip(self.w, x)), self.b)\n","        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) \n","        out = act.tanh()\n","        return out\n","\n","    def parameters(self):\n","        # print(f'w: {self.w}, b: {[self.b]}')\n","        return self.w + [self.b]\n","\n","\n","class Layer:\n","    def __init__(self, nin, nout):\n","        self.neurons = [Neuron(nin) for _ in range(nout)]\n","\n","#### my add ##########################################\n","    def __repr__(self) -> str:\n","        return f\"Layer(neurons = {self.neurons})\"\n","######################################################\n","\n","    def __call__(self, x):\n","        outs = [n(x) for n in self.neurons]\n","        return outs[0] if len(outs) == 1 else outs\n","\n","    def parameters(self):\n","        # params = []\n","        # for neuron in self.neurons:\n","        #     ps = neuron.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for neuron in self.neurons for p in neuron.parameters()]\n","\n","class MLP:\n","    def __init__(self, nin, nouts):\n","        sz = [nin] + nouts\n","        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n","\n","    def __call__(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def parameters(self):\n","        params = []\n","        # for layer in self.layers:\n","        #     ps = layer.parameters()\n","        #     params.extend(ps)\n","        # return params\n","        return [p for layer in self.layers for p in layer.parameters()]"]},{"cell_type":"markdown","metadata":{},"source":["#   &nbsp;\n","# - Human Brain and Artificial Neural Network - "]},{"cell_type":"markdown","metadata":{},"source":["### Neurons in Human Brain\n","![](..\\karpathy\\img\\neuron_of_human_brain.png)"]},{"cell_type":"markdown","metadata":{},"source":["### Simple Artificial Neural Network<br>* input layer: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2 nodes<br>* hidden layer 1: &nbsp;3 nodes<br>* hidden layer 2:&nbsp;&nbsp;3 nodes<br>*  output layer: &nbsp;&nbsp;&nbsp; 1 node<br>* node's bias and activation function are not shown\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\MLP (2, [3, 3, 1]).png\">"]},{"cell_type":"markdown","metadata":{},"source":["### Artificial Neuron Function\n","\n","<img src=\"..\\karpathy\\img\\Artificial Neuron Function.png\">"]},{"cell_type":"markdown","metadata":{},"source":["#   &nbsp;\n","# - Visualize Math Operations in a Hidden Layer -"]},{"cell_type":"markdown","metadata":{},"source":["### * Assume hidden layer with two inputs (X0, X1), and three neurons (b0, b1, b2)<br>* Two sets of inputs (X0, X1) are shown in different shades of gray<br>* Two sets of outputs (Y0, Y1, Y2) are shown in corresponding shades of gray<br>* Multiple sets of inputs are processed in one matrix operation \n","\n","<img src=\"..\\karpathy\\img\\Hidden Layer Matrix Operations.png\">"]},{"cell_type":"markdown","metadata":{},"source":["# &nbsp;\n","# - Create Simple Neural Network -\n","##### MLP(2, [3, 3, 1])<br>* 2 input nodes<br>* 3 neurons in hidden layer 1<br>* 3 neurons in hidden layer 2<br>* 1 output node\n","##### Initialize Neurons Parameters <br>* parameters in layer 1: 3 neurons * (2 inputs + 1 bias) = &nbsp;&nbsp;&nbsp;&nbsp;  9<br>* parameters in layer 2: 3 neurons * (3 neurons + 1 bias) = 12<br>* parameters in layer 3: 1 output * (3 neurons + 1 bias) = &nbsp;&nbsp;&nbsp; 4<br>*  total parameters: 25"]},{"cell_type":"code","execution_count":846,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Neuron parameters, initialized with random numbers\n","Total parameters: 25\n","\n","i:  0,   0.0739193117\n","i:  1,   0.9140117900\n","i:  2,   0.7868583802\n","i:  3,  -0.5835426060\n","i:  4,  -0.9921402719\n","---\n","i: 20,   0.2806487198\n","i: 21,   0.4102522534\n","i: 22,   0.8997895539\n","i: 23,  -0.9651954255\n","i: 24,  -0.6864563952\n"]}],"source":["# create neural network and initialize weights and biases\n","n = MLP(2, [3, 3, 1])\n","\n","# if verbose:\n","if True:\n","  print(\"Neuron parameters, initialized with random numbers\")\n","  print_parameters(n.parameters)"]},{"cell_type":"markdown","metadata":{},"source":["# &nbsp;\n","# - Set Inputs, Desired Outputs, Learning Rate -\n","##### Inputs<br>* 1st set: [2.0, 3.0]<br>* 2nd set: [3.0, -1.0]\n","##### Desired Outputs<br>* [1.0, -1.0] for all input sets\n","##### Learning Rate<br>* 0.05"]},{"cell_type":"code","execution_count":847,"metadata":{},"outputs":[],"source":["# inputs\n","xs = [\n","  [2.0, 3.0],\n","  [3.0, -1.0]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":848,"metadata":{},"outputs":[],"source":["# if True:\n","if verbose:\n","\t# print weights and bias of each layer\n","\tfor i, layer in enumerate(n.layers):\n","\t\tneuron_cnt = len(layer.neurons)  # numbers of neurons in the layer \n","\t\tprint(f'layer: {i}, neuron_cnt: {neuron_cnt}, layer: {layer}')"]},{"cell_type":"markdown","metadata":{},"source":["# &nbsp;\n","# - Calculate Neural Network Outputs and Loss (i.e. Prediction Errors) -\n","##### * transpose inputs<br>* select activation function<br>* calculate output, (a.k.a) Forward Pass<br>* calculate Loss"]},{"cell_type":"markdown","metadata":{},"source":["##### Transpose Inputs"]},{"cell_type":"code","execution_count":849,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xs_mats[0].shape: (2, 2)\n","xs_mats:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n","\n","xs_mats_T[0].shape: (2, 2)\n","xs_mats_T:\n","[array([[ 2.,  3.],\n","       [ 3., -1.]])]\n"]}],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"cell_type":"markdown","metadata":{},"source":["##### Common Activation Functions"]},{"cell_type":"code","execution_count":850,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x300 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Common Activation Functions\n","x = np.arange(-5, 5, 0.2)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n","\n","ax1.plot(x, np.tanh(x))\n","ax1.set_title(\"Tanh\")\n","ax1.grid()\n","\n","a = x[x < 0]*0.01\n","b = x[x >= 0]\n","y = np.concatenate((a, b))\n","ax2.grid()\n","ax2.plot(x, y)\n","ax2.set_title(\"Leaky ReLU\")\n","\n","plt_title = \"Common Activation Functions\\nIntroduce non-linearity into the network\\nto learn complex relationships\"\n","plt.suptitle(plt_title , fontsize=12, y=1.2)\n","plt.subplots_adjust(wspace=0.3)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate Outputs and Loss Using Tanh Activation"]},{"cell_type":"code","execution_count":851,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","Calculate Output of Layer: 0\n","weights (3, 2):\n","[[ 0.07391931  0.91401179]\n"," [-0.58354261 -0.99214027]\n"," [-0.96053795  0.90732097]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[ 2.88987399 -0.69225385]\n"," [-4.14350603 -0.75848755]\n"," [ 0.80088703 -3.78893481]]\n","\n","bias (3, 1):\n","[[ 0.78685838]\n"," [ 0.09471754]\n"," [-0.01085797]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[ 3.67673237  0.09460453]\n"," [-4.04878849 -0.66377001]\n"," [ 0.79002905 -3.79979279]]\n","\n","Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[ 0.99872008  0.09432329]\n"," [-0.99939163 -0.58086686]\n"," [ 0.65842549 -0.99899918]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 1\n","weights (3, 3):\n","[[-0.35642334  0.37371306  0.91927229]\n"," [ 0.8960069  -0.86677986 -0.63985292]\n"," [-0.90192006 -0.01038034 -0.17922696]]\n","\n","input (3, 2):\n","[[ 0.99872008  0.09432329]\n"," [-0.99939163 -0.58086686]\n"," [ 0.65842549 -0.99899918]]\n","\n","weights_x_inputs (3, 2):\n","[[-0.12418054 -1.16904883]\n"," [ 1.33981715  1.22721056]\n"," [-1.00839925  0.10000511]]\n","\n","bias (3, 1):\n","[[-0.81793834]\n"," [-0.52715499]\n"," [ 0.28064872]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[-0.94211888 -1.98698717]\n"," [ 0.81266216  0.70005557]\n"," [-0.72775053  0.38065383]]\n","\n","Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[-0.73619425 -0.96309659]\n"," [ 0.67105622  0.60440305]\n"," [-0.62168722  0.36327515]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 2\n","weights (1, 3):\n","[[ 0.41025225  0.89978955 -0.96519543]]\n","\n","input (3, 2):\n","[[-0.73619425 -0.96309659]\n"," [ 0.67105622  0.60440305]\n"," [-0.62168722  0.36327515]]\n","\n","weights_x_inputs (1, 2):\n","[[ 0.90183368 -0.20190851]]\n","\n","bias (1, 1):\n","[[-0.6864564]]\n","\n","weights_x_inputs_+_bias (1, 2):\n","[[ 0.21537729 -0.8883649 ]]\n","\n","Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n","[[ 0.21210768 -0.71058519]]\n","\n","-- Results of neural network outputs and Loss --\n","yout:           [ 0.21210768 -0.71058519]\n","desired output: [1.0, -1.0]\n","err:            [-0.78789232  0.28941481]\n","err_sq:         [0.62077431 0.08376093]\n","loss_mean:      0.35226762201452355\n","loss_sum:       0.7045352440290471\n"]}],"source":["yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"]},{"cell_type":"code","execution_count":852,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- Neural network outputs and Loss --\n","yout:           [ 0.21210768 -0.71058519] <-- neural network output\n","desired output: [1.0, -1.0]\n","err:            [-0.78789232  0.28941481]\n","err_sq:         [0.62077431 0.08376093]\n","loss_mean:      0.35226762201452355\n","loss_sum:       0.7045352440290471 <-- sum(prediction_error)^2\n"]}],"source":["print(f'-- Neural network outputs and Loss --')\n","print(f'yout:           {yout} <-- neural network output')   \n","print(f'desired output: {ys}')   \n","print(f'err:            {err}')\n","print(f'err_sq:         {err_sq}')\n","print(f'loss_mean:      {loss_mean}')\n","print(f'loss_sum:       {loss_sum} <-- sum(prediction_error)^2')\n"]},{"cell_type":"markdown","metadata":{},"source":["#   &nbsp;\n","# - How Artificial Neural Network Learns -\n","\n","##### * calculate gradients (i.e. changes in Loss w.r.t. changes in each parameter)<br>* use gradients to adjust parameters in direction of less Loss<br>* repeat the steps"]},{"cell_type":"markdown","metadata":{},"source":["##### Example of calculating gradient for parameter W0<br>* calculate outputs and Loss<br>* increase W0 by small amount, e.g. 0.00001<br>* recalculate outputs and Loss<br>* W0 gradient = changes in Loss / changes in W0"]},{"cell_type":"code","execution_count":853,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss_sum before increase Wo:   0.7045352\n","W0_before:                     0.0739193\n","W0_after:                      0.0739293\n","W0_dif:                        0.0000100 <-- increased W0 by a small amount\n"]}],"source":["# Increase W1 by h\n","h = .00001\n","loss_sum_before = loss_sum\n","print(f'loss_sum before increase Wo:  {loss_sum_before:10.7f}')\n","W0_before = n.parameters()[0].data  # W1\n","print(f'W0_before:                    {W0_before:10.7f}')\n","n.parameters()[0].data += h\n","W0_after = n.parameters()[0].data\n","print(f'W0_after:                     {W0_after:10.7f}') \n","W0_dif = W0_after - W0_before\n","print(f'W0_dif:                       {W0_dif:10.7f} <-- increased W0 by a small amount') "]},{"cell_type":"markdown","metadata":{},"source":["##### Recalculate output and Loss with small changes in W0"]},{"cell_type":"code","execution_count":854,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","Calculate Output of Layer: 0\n","weights (3, 2):\n","[[ 0.07392931  0.91401179]\n"," [-0.58354261 -0.99214027]\n"," [-0.96053795  0.90732097]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[ 2.88989399 -0.69222385]\n"," [-4.14350603 -0.75848755]\n"," [ 0.80088703 -3.78893481]]\n","\n","bias (3, 1):\n","[[ 0.78685838]\n"," [ 0.09471754]\n"," [-0.01085797]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[ 3.67675237  0.09463453]\n"," [-4.04878849 -0.66377001]\n"," [ 0.79002905 -3.79979279]]\n","\n","Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[ 0.99872013  0.09435303]\n"," [-0.99939163 -0.58086686]\n"," [ 0.65842549 -0.99899918]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 1\n","weights (3, 3):\n","[[-0.35642334  0.37371306  0.91927229]\n"," [ 0.8960069  -0.86677986 -0.63985292]\n"," [-0.90192006 -0.01038034 -0.17922696]]\n","\n","input (3, 2):\n","[[ 0.99872013  0.09435303]\n"," [-0.99939163 -0.58086686]\n"," [ 0.65842549 -0.99899918]]\n","\n","weights_x_inputs (3, 2):\n","[[-0.12418056 -1.16905943]\n"," [ 1.3398172   1.22723721]\n"," [-1.00839929  0.0999783 ]]\n","\n","bias (3, 1):\n","[[-0.81793834]\n"," [-0.52715499]\n"," [ 0.28064872]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[-0.9421189  -1.98699777]\n"," [ 0.8126622   0.70008221]\n"," [-0.72775057  0.38062702]]\n","\n","Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[-0.73619426 -0.96309735]\n"," [ 0.67105624  0.60441996]\n"," [-0.62168725  0.36325187]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 2\n","weights (1, 3):\n","[[ 0.41025225  0.89978955 -0.96519543]]\n","\n","input (3, 2):\n","[[-0.73619426 -0.96309735]\n"," [ 0.67105624  0.60441996]\n"," [-0.62168725  0.36325187]]\n","\n","weights_x_inputs (1, 2):\n","[[ 0.90183373 -0.20187114]]\n","\n","bias (1, 1):\n","[[-0.6864564]]\n","\n","weights_x_inputs_+_bias (1, 2):\n","[[ 0.21537733 -0.88832754]]\n","\n","Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n","[[ 0.21210772 -0.71056669]]\n","\n","-- Results of neural network outputs and Loss --\n","yout:           [ 0.21210772 -0.71056669]\n","desired output: [1.0, -1.0]\n","err:            [-0.78789228  0.28943331]\n","err_sq:         [0.62077424 0.08377164]\n","loss_mean:      0.3522729412580478\n","loss_sum:       0.7045458825160956\n"]}],"source":["yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"]},{"cell_type":"code","execution_count":855,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- Recaluclate neural network outputs and loss with small change in W0 --\n","yout:             [ 0.21210772 -0.71056669]\n","desired output:   [1.0, -1.0]\n","err:              [-0.78789228  0.28943331]\n","err_sq:           [0.62077424 0.08377164]\n","loss_sum_before:  0.7045352440290471\n","loss_sum_after:   0.7045458825160956\n","\n","loss_sum_dif:     1.0638487048519885e-05 <-- change in loss_sum\n","W0_dif:           9.999999999996123e-06 <-- change in W0\n","W0_grad:          1.0638487048524008 <-- (changes in loss_sum) / (changes in W0), manual calculation\n"]}],"source":["loss_sum_after = loss_sum\n","loss_sum_dif = loss_sum_after - loss_sum_before\n","W0_grad = loss_sum_dif / W0_dif\n","\n","print(f'-- Recaluclate neural network outputs and loss with small change in W0 --')\n","print(f'yout:             {yout}')   \n","print(f'desired output:   {ys}')   \n","print(f'err:              {err}')\n","print(f'err_sq:           {err_sq}')\n","print(f'loss_sum_before:  {loss_sum_before}')\n","print(f'loss_sum_after:   {loss_sum_after}\\n')\n","print(f'loss_sum_dif:     {loss_sum_dif} <-- change in loss_sum')\n","print(f'W0_dif:           {W0_dif} <-- change in W0')\n","print(f'W0_grad:          {W0_grad} <-- (changes in loss_sum) / (changes in W0), manual calculation')"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate output and Loss with Micrograd<br>* change W0 back to initial value<br>* compare manual calculation vs Micrograd "]},{"cell_type":"code","execution_count":856,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- Calculate neural network Loss and gradient using Micrograd --\n","W0:          0.0739193116671657\n","ypred_data:  [0.21210767891162663, -0.7105851862826944]\n","ys:          [1.0, -1.0]\n","err_sq:      [0.6207743096300244, 0.0837609343990227]\n","loss_mean:   0.35226762201452355\n","loss_sum:    0.7045352440290471 <-- loss_sum, Micrograd calculation same as manual calc. 0.7045352440290471\n"]}],"source":["# change W1 back before Micrograd calculation\n","n.parameters()[0].data = W0_before\n","\n","ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","# loss.backward()\n","err_sq_ = [(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","err_sq = [l.data for l in err_sq_]\n","loss_sum = sum(err_sq)\n","loss_len = len(err_sq)\n","loss_mean = loss_sum / loss_len\n","\n","# print(f'-- Micrograd forward pass and backward pass --')\n","print(f'-- Calculate neural network Loss and gradient using Micrograd --')\n","print(f'W0:          {n.parameters()[0].data}')\n","print(f'ypred_data:  {ypred_data}')\n","print(f'ys:          {ys}')\n","print(f'err_sq:      {err_sq}')\n","print(f'loss_mean:   {loss_mean}')\n","print(f'loss_sum:    {loss_sum} <-- loss_sum, Micrograd calculation same as manual calc. {loss_sum_before}')\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Calculate gradients and adjust parameters using Micrograd"]},{"cell_type":"code","execution_count":857,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-- adjust parameters,  parameter_adjusted = parameter_before - gradient * learning_rate --\n","  0      0.0739193117     1.0637895939           0.05000         0.0207298320 <-- gradient same as manual calc. W0_grad  1.0638487049\n","  1      0.9140117900    -0.3674458253           0.05000         0.9323840813\n","  2      0.7868583802     0.3534284137           0.05000         0.7691869596\n","  3     -0.5835426060    -0.2693981368           0.05000        -0.5700726991\n","  4     -0.9921402719     0.0921642176           0.05000        -0.9967484828\n","  5      0.0947175410    -0.0895843936           0.05000         0.0991967607\n","  6     -0.9605379465     0.0637961129           0.05000        -0.9637277522\n","  7      0.9073209729     0.0962867575           0.05000         0.9025066351\n","  8     -0.0108579742     0.0319519281           0.05000        -0.0124555707\n","  9     -0.3564233435    -0.2816080900           0.05000        -0.3423429390\n"," 10      0.3737130619     0.2776542033           0.05000         0.3598303517\n"," 11      0.9192722947    -0.1946934368           0.05000         0.9290069666\n"," 12     -0.8179383395    -0.2742565685           0.05000        -0.8042255111\n"," 13      0.8960069043    -0.7279292274           0.05000         0.9324033657\n"," 14     -0.8667798555     0.6488048348           0.05000        -0.8992200972\n"," 15     -0.6398529195    -0.6535671237           0.05000        -0.6071745634\n"," 16     -0.5271549917    -0.5806653200           0.05000        -0.4981217257\n"," 17     -0.9019200635     0.8673380561           0.05000        -0.9452869663\n"," 18     -0.0103803449    -0.7511242073           0.05000         0.0271758655\n"," 19     -0.1792269607     0.8265846644           0.05000        -0.2205561939\n"," 20      0.2806487198     0.6510383500           0.05000         0.2480968023\n"," 21      0.4102522534     0.8319064552           0.05000         0.3686569306\n"," 22      0.8997895539    -0.8366682103           0.05000         0.9416229645\n"," 23     -0.9651954255     1.0396715408           0.05000        -1.0171790026\n"," 24     -0.6864563952    -1.2183301880           0.05000        -0.6255398858\n"]}],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","print('-- adjust parameters,  parameter_adjusted = parameter_before - gradient * learning_rate --')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","\n","  if i == 0:  \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f} <-- gradient same as manual calc. W0_grad {W0_grad:13.10f}')\n","  else:\n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')    "]},{"cell_type":"markdown","metadata":{},"source":["##### Repeat the steps:<br>* calculate Loss<br>* calculate gradient<br>* adjust parameters in direction of less Loss"]},{"cell_type":"code","execution_count":858,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ypred: [Value(data = 0.47186451539904095), Value(data = -0.7802632923337666)]\n","step: 0, loss: 0.3272113107906856\n","ypred: [Value(data = 0.5817635707744326), Value(data = -0.8356912038153)]\n","step: 1, loss: 0.20191909123501828\n","ypred: [Value(data = 0.6461609847955764), Value(data = -0.8632088281059693)]\n","step: 2, loss: 0.1439138733889786\n","ypred: [Value(data = 0.6887958928636817), Value(data = -0.8807642631689058)]\n","step: 3, loss: 0.11106515723616704\n","ypred: [Value(data = 0.7195013477799815), Value(data = -0.8931586366771566)]\n","step: 4, loss: 0.0900945708139307\n","ypred: [Value(data = 0.7428990542053443), Value(data = -0.9024781686038634)]\n","step: 5, loss: 0.07561140392736299\n","ypred: [Value(data = 0.7614578065929237), Value(data = -0.9097971141232831)]\n","step: 6, loss: 0.065038938655947\n","ypred: [Value(data = 0.7766249927618485), Value(data = -0.9157313653583444)]\n","step: 7, loss: 0.05699759664301308\n","ypred: [Value(data = 0.7893102044740001), Value(data = -0.9206620188434157)]\n","step: 8, loss: 0.05068470519279016\n","ypred: [Value(data = 0.8001165245610168), Value(data = -0.924838830354999)]\n","step: 9, loss: 0.04560260517597122\n","ypred: [Value(data = 0.809461185738357), Value(data = -0.9284330082324586)]\n","step: 10, loss: 0.041426874050888234\n","ypred: [Value(data = 0.8176429948328177), Value(data = -0.9315662441311542)]\n","step: 11, loss: 0.03793725627586056\n","ypred: [Value(data = 0.8248821633334309), Value(data = -0.934327651773904)]\n","step: 12, loss: 0.03497911404030879\n","ypred: [Value(data = 0.8313449414207553), Value(data = -0.9367841395244537)]\n","step: 13, loss: 0.032440773800032185\n","ypred: [Value(data = 0.8371594510783802), Value(data = -0.9389870226539192)]\n","step: 14, loss: 0.030239627777727825\n","ypred: [Value(data = 0.8424262013283311), Value(data = -0.9409763843169949)]\n","step: 15, loss: 0.028313289236114716\n","ypred: [Value(data = 0.847225271510959), Value(data = -0.9427840368575123)]\n","step: 16, loss: 0.02661378410322269\n","ypred: [Value(data = 0.8516213403930724), Value(data = -0.9444355841960724)]\n","step: 17, loss: 0.025103630930380257\n","ypred: [Value(data = 0.8556672838871684), Value(data = -0.9459518903597668)]\n","step: 18, loss: 0.02375313109618991\n","ypred: [Value(data = 0.8594067984174809), Value(data = -0.9473501458306605)]\n","step: 19, loss: 0.022538455475275568\n","ypred: [Value(data = 0.8628763466738621), Value(data = -0.9486446554707844)]\n","step: 20, loss: 0.021440267713221276\n","ypred: [Value(data = 0.8661066230080686), Value(data = -0.9498474298827262)]\n","step: 21, loss: 0.020442716691671525\n","ypred: [Value(data = 0.869123672389008), Value(data = -0.9509686355332024)]\n","step: 22, loss: 0.019532687830415646\n","ypred: [Value(data = 0.8719497555772394), Value(data = -0.9520169417680571)]\n","step: 23, loss: 0.018699238974018755\n","ypred: [Value(data = 0.8746040257459069), Value(data = -0.9529997914603481)]\n","step: 24, loss: 0.01793316996190395\n","ypred: [Value(data = 0.8771030631890832), Value(data = -0.9539236143488938)]\n","step: 25, loss: 0.01722669039217595\n","ypred: [Value(data = 0.8794613019494097), Value(data = -0.9547939968479552)]\n","step: 26, loss: 0.016573160448714055\n","ypred: [Value(data = 0.8816913732302755), Value(data = -0.9556158184271721)]\n","step: 27, loss: 0.01596688674202773\n","ypred: [Value(data = 0.8838043840901598), Value(data = -0.9563933620544558)]\n","step: 28, loss: 0.01540296002958088\n","ypred: [Value(data = 0.8858101453345777), Value(data = -0.9571304043243024)]\n","step: 29, loss: 0.01487712514190806\n","ypred: [Value(data = 0.8877173591830516), Value(data = -0.9578302895351906)]\n","step: 30, loss: 0.014385675909513703\n","ypred: [Value(data = 0.8895337748290983), Value(data = -0.9584959909810615)]\n","step: 31, loss: 0.013925369668152473\n","ypred: [Value(data = 0.8912663181798274), Value(data = -0.9591301619799936)]\n","step: 32, loss: 0.013493357221952102\n","ypred: [Value(data = 0.8929212006845347), Value(data = -0.9597351786065049)]\n","step: 33, loss: 0.013087125104691743\n","ypred: [Value(data = 0.8945040111163978), Value(data = -0.960313175672298)]\n","step: 34, loss: 0.012704447695747004\n","ypred: [Value(data = 0.896019793371147), Value(data = -0.9608660771781258)]\n","step: 35, loss: 0.012343347286007372\n","ypred: [Value(data = 0.897473112728974), Value(data = -0.9613956222113067)]\n","step: 36, loss: 0.012002060597937828\n","ypred: [Value(data = 0.8988681125456072), Value(data = -0.9619033870707672)]\n","step: 37, loss: 0.011679010576767767\n","ypred: [Value(data = 0.9002085629622261), Value(data = -0.962390804250843)]\n","step: 38, loss: 0.01137278251096241\n","ypred: [Value(data = 0.901497902927143), Value(data = -0.9628591787964614)]\n","step: 39, loss: 0.011082103727423754\n","ypred: [Value(data = 0.9027392765866372), Value(data = -0.9633097024483179)]\n","step: 40, loss: 0.010805826253321622\n","ypred: [Value(data = 0.9039355649142243), Value(data = -0.9637434659217108)]\n","step: 41, loss: 0.010542911951719355\n","ypred: [Value(data = 0.9050894132965529), Value(data = -0.9641614696025719)]\n","step: 42, loss: 0.01029241972943993\n","ypred: [Value(data = 0.9062032556720929), Value(data = -0.9645646328957505)]\n","step: 43, loss: 0.010053494488327699\n","ypred: [Value(data = 0.9072793357197144), Value(data = -0.9649538024212939)]\n","step: 44, loss: 0.00982535754930313\n","ypred: [Value(data = 0.9083197255134245), Value(data = -0.9653297592224402)]\n","step: 45, loss: 0.009607298325507786\n","ypred: [Value(data = 0.9093263419932641), Value(data = -0.9656932251228061)]\n","step: 46, loss: 0.00939866705879696\n","ypred: [Value(data = 0.9103009615478054), Value(data = -0.966044868348693)]\n","step: 47, loss: 0.009198868464705882\n","ypred: [Value(data = 0.9112452329585543), Value(data = -0.9663853085146066)]\n","step: 48, loss: 0.009007356156239476\n","ypred: [Value(data = 0.912160688919092), Value(data = -0.9667151210553122)]\n","step: 49, loss: 0.008823627737531054\n","ypred: [Value(data = 0.9130487563105314), Value(data = -0.9670348411754324)]\n","step: 50, loss: 0.008647220475474324\n","ypred: [Value(data = 0.9139107653887211), Value(data = -0.967344967377311)]\n","step: 51, loss: 0.008477707471544709\n","ypred: [Value(data = 0.9147479580166541), Value(data = -0.9676459646192221)]\n","step: 52, loss: 0.008314694267750798\n","ypred: [Value(data = 0.9155614950570267), Value(data = -0.9679382671487394)]\n","step: 53, loss: 0.008157815830430133\n","ypred: [Value(data = 0.9163524630242316), Value(data = -0.9682222810499389)]\n","step: 54, loss: 0.00800673386378162\n","ypred: [Value(data = 0.9171218800817801), Value(data = -0.9684983865378921)]\n","step: 55, loss: 0.007861134411894897\n","ypred: [Value(data = 0.917870701459842), Value(data = -0.9687669400294914)]\n","step: 56, loss: 0.007720725713819794\n","ypred: [Value(data = 0.9185998243579362), Value(data = -0.9690282760158623)]\n","step: 57, loss: 0.00758523628110845\n","ypred: [Value(data = 0.919310092389542), Value(data = -0.9692827087583966)]\n","step: 58, loss: 0.007454413171405736\n","ypred: [Value(data = 0.9200022996183038), Value(data = -0.9695305338276637)]\n","step: 59, loss: 0.0073280204351867766\n","ypred: [Value(data = 0.9206771942293979), Value(data = -0.9697720295020786)]\n","step: 60, loss: 0.007205837715743876\n","ypred: [Value(data = 0.9213354818743549), Value(data = -0.9700074580411497)]\n","step: 61, loss: 0.007087658985093343\n","ypred: [Value(data = 0.921977828723075), Value(data = -0.970237066846354)]\n","step: 62, loss: 0.006973291400674221\n","ypred: [Value(data = 0.9226048642528184), Value(data = -0.9704610895211485)]\n","step: 63, loss: 0.006862554269602269\n","ypred: [Value(data = 0.9232171838005114), Value(data = -0.9706797468402903)]\n","step: 64, loss: 0.006755278108873907\n","ypred: [Value(data = 0.9238153509017187), Value(data = -0.9708932476374728)]\n","step: 65, loss: 0.006651303791321735\n","ypred: [Value(data = 0.9243998994370132), Value(data = -0.9711017896192713)]\n","step: 66, loss: 0.006550481768342578\n","ypred: [Value(data = 0.9249713356041879), Value(data = -0.971305560112507)]\n","step: 67, loss: 0.006452671361476355\n","ypred: [Value(data = 0.9255301397327489), Value(data = -0.9715047367513528)]\n","step: 68, loss: 0.006357740115833606\n","ypred: [Value(data = 0.9260767679553621), Value(data = -0.9716994881098299)]\n","step: 69, loss: 0.006265563209171042\n","ypred: [Value(data = 0.9266116537493788), Value(data = -0.9718899742847363)]\n","step: 70, loss: 0.006176022911113849\n","ypred: [Value(data = 0.9271352093601936), Value(data = -0.9720763474335289)]\n","step: 71, loss: 0.0060890080876358135\n","ypred: [Value(data = 0.92764782711698), Value(data = -0.972258752271199)]\n","step: 72, loss: 0.006004413746445121\n","ypred: [Value(data = 0.928149880650281), Value(data = -0.9724373265297843)]\n","step: 73, loss: 0.0059221406193945905\n","ypred: [Value(data = 0.9286417260199774), Value(data = -0.9726122013837796)]\n","step: 74, loss: 0.005842094778450615\n","ypred: [Value(data = 0.9291237027613213), Value(data = -0.9727835018443955)]\n","step: 75, loss: 0.005764187282119557\n","ypred: [Value(data = 0.9295961348559669), Value(data = -0.972951347125315)]\n","step: 76, loss: 0.005688333849554403\n","ypred: [Value(data = 0.9300593316342658), Value(data = -0.9731158509823493)]\n","step: 77, loss: 0.005614454559848861\n","ypred: [Value(data = 0.9305135886144976), Value(data = -0.9732771220291597)]\n","step: 78, loss: 0.005542473574279694\n","ypred: [Value(data = 0.9309591882841725), Value(data = -0.9734352640310087)]\n","step: 79, loss: 0.005472318879482571\n","ypred: [Value(data = 0.9313964008280685), Value(data = -0.9735903761783204)]\n","step: 80, loss: 0.005403922049745662\n","ypred: [Value(data = 0.9318254848072388), Value(data = -0.9737425533416691)]\n","step: 81, loss: 0.005337218026783116\n","ypred: [Value(data = 0.9322466877928358), Value(data = -0.9738918863096597)]\n","step: 82, loss: 0.0052721449155091965\n","ypred: [Value(data = 0.9326602469582637), Value(data = -0.9740384620110413)]\n","step: 83, loss: 0.005208643794474186\n","ypred: [Value(data = 0.9330663896328515), Value(data = -0.9741823637222707)]\n","step: 84, loss: 0.005146658539750372\n","ypred: [Value(data = 0.9334653338199673), Value(data = -0.974323671261635)]\n","step: 85, loss: 0.005086135661168972\n","ypred: [Value(data = 0.9338572886822356), Value(data = -0.9744624611709478)]\n","step: 86, loss: 0.005027024149910466\n","ypred: [Value(data = 0.9342424549962967), Value(data = -0.9745988068857501)]\n","step: 87, loss: 0.004969275336541489\n","ypred: [Value(data = 0.9346210255793402), Value(data = -0.9747327788948564)]\n","step: 88, loss: 0.004912842758673496\n","ypred: [Value(data = 0.9349931856894572), Value(data = -0.9748644448900318)]\n","step: 89, loss: 0.00485768203749164\n","ypred: [Value(data = 0.9353591134016862), Value(data = -0.9749938699065044)]\n","step: 90, loss: 0.004803750762468891\n","ypred: [Value(data = 0.9357189799614785), Value(data = -0.9751211164549747)]\n","step: 91, loss: 0.00475100838363974\n","ypred: [Value(data = 0.9360729501171631), Value(data = -0.9752462446457154)]\n","step: 92, loss: 0.004699416110862494\n","ypred: [Value(data = 0.9364211824328721), Value(data = -0.9753693123053196)]\n","step: 93, loss: 0.00464893681954701\n","ypred: [Value(data = 0.9367638295832643), Value(data = -0.9754903750866016)]\n","step: 94, loss: 0.004599534962369914\n","ypred: [Value(data = 0.9371010386312862), Value(data = -0.9756094865721233)]\n","step: 95, loss: 0.004551176486538379\n","ypred: [Value(data = 0.9374329512901106), Value(data = -0.975726698371772)]\n","step: 96, loss: 0.00450382875620061\n","ypred: [Value(data = 0.9377597041703051), Value(data = -0.9758420602147932)]\n","step: 97, loss: 0.004457460479633615\n","ypred: [Value(data = 0.9380814290132042), Value(data = -0.9759556200366455)]\n","step: 98, loss: 0.00441204164086903\n","ypred: [Value(data = 0.9383982529113861), Value(data = -0.9760674240610142)]\n","step: 99, loss: 0.00436754343544487\n","ypred: [Value(data = 0.9387102985170851), Value(data = -0.9761775168773027)]\n","step: 100, loss: 0.004323938209996019\n","ypred: [Value(data = 0.9390176842393116), Value(data = -0.976285941513892)]\n","step: 101, loss: 0.004281199405418849\n","ypred: [Value(data = 0.9393205244303984), Value(data = -0.9763927395074345)]\n","step: 102, loss: 0.004239301503365722\n","ypred: [Value(data = 0.93961892956263), Value(data = -0.9764979509684392)]\n","step: 103, loss: 0.004198219975844521\n","ypred: [Value(data = 0.9399130063955785), Value(data = -0.9766016146433746)]\n","step: 104, loss: 0.004157931237714932\n","ypred: [Value(data = 0.9402028581347116), Value(data = -0.9767037679735066)]\n","step: 105, loss: 0.004118412601889645\n","ypred: [Value(data = 0.9404885845818102), Value(data = -0.9768044471506754)]\n","step: 106, loss: 0.004079642237062173\n","ypred: [Value(data = 0.9407702822776863), Value(data = -0.9769036871701916)]\n","step: 107, loss: 0.00404159912779733\n","ypred: [Value(data = 0.9410480446376654), Value(data = -0.9770015218810297)]\n","step: 108, loss: 0.00400426303683144\n","ypred: [Value(data = 0.9413219620802605), Value(data = -0.9770979840334797)]\n","step: 109, loss: 0.003967614469441134\n","ypred: [Value(data = 0.9415921221494379), Value(data = -0.9771931053244052)]\n","step: 110, loss: 0.0039316346397498605\n","ypred: [Value(data = 0.9418586096308502), Value(data = -0.9772869164402501)]\n","step: 111, loss: 0.003896305438850043\n","ypred: [Value(data = 0.942121506662383), Value(data = -0.9773794470979259)]\n","step: 112, loss: 0.0038616094046281042\n","ypred: [Value(data = 0.9423808928393409), Value(data = -0.9774707260836983)]\n","step: 113, loss: 0.0038275296931872672\n","ypred: [Value(data = 0.9426368453145778), Value(data = -0.9775607812901906)]\n","step: 114, loss: 0.003794050051770332\n","ypred: [Value(data = 0.9428894388938562), Value(data = -0.9776496397516101)]\n","step: 115, loss: 0.0037611547930913873\n","ypred: [Value(data = 0.9431387461266999), Value(data = -0.9777373276772953)]\n","step: 116, loss: 0.003728828770992009\n","ypred: [Value(data = 0.9433848373929913), Value(data = -0.9778238704836789)]\n","step: 117, loss: 0.0036970573573426855\n","ypred: [Value(data = 0.9436277809855456), Value(data = -0.9779092928247566)]\n","step: 118, loss: 0.0036658264201159694\n","ypred: [Value(data = 0.9438676431888807), Value(data = -0.9779936186211365)]\n","step: 119, loss: 0.003635122302562797\n","ypred: [Value(data = 0.944104488354389), Value(data = -0.9780768710877547)]\n","step: 120, loss: 0.003604931803427565\n","ypred: [Value(data = 0.944338378972102), Value(data = -0.9781590727603217)]\n","step: 121, loss: 0.0035752421581422555\n","ypred: [Value(data = 0.9445693757392297), Value(data = -0.9782402455205734)]\n","step: 122, loss: 0.003546041020943619\n","ypred: [Value(data = 0.9447975376256447), Value(data = -0.978320410620385)]\n","step: 123, loss: 0.0035173164478608317\n","ypred: [Value(data = 0.9450229219364689), Value(data = -0.9783995887048104)]\n","step: 124, loss: 0.003489056880524953\n","ypred: [Value(data = 0.9452455843719156), Value(data = -0.9784777998341033)]\n","step: 125, loss: 0.003461251130753932\n","ypred: [Value(data = 0.9454655790845253), Value(data = -0.9785550635047722)]\n","step: 126, loss: 0.0034338883658705153\n","ypred: [Value(data = 0.9456829587339294), Value(data = -0.9786313986697155)]\n","step: 127, loss: 0.003406958094712655\n","ypred: [Value(data = 0.9458977745392678), Value(data = -0.978706823757492)]\n","step: 128, loss: 0.0033804501542984058\n","ypred: [Value(data = 0.9461100763293762), Value(data = -0.9787813566907606)]\n","step: 129, loss: 0.0033543546971103936\n","ypred: [Value(data = 0.9463199125908553), Value(data = -0.9788550149039379)]\n","step: 130, loss: 0.0033286621789660985\n","ypred: [Value(data = 0.9465273305141282), Value(data = -0.9789278153601092)]\n","step: 131, loss: 0.0033033633474429365\n","ypred: [Value(data = 0.9467323760375799), Value(data = -0.9789997745672324)]\n","step: 132, loss: 0.0032784492308288535\n","ypred: [Value(data = 0.9469350938898791), Value(data = -0.9790709085936656)]\n","step: 133, loss: 0.0032539111275706493\n","ypred: [Value(data = 0.9471355276305646), Value(data = -0.9791412330830567)]\n","step: 134, loss: 0.003229740596194165\n","ypred: [Value(data = 0.9473337196889823), Value(data = -0.9792107632686189)]\n","step: 135, loss: 0.0032059294456720983\n","ypred: [Value(data = 0.9475297114016501), Value(data = -0.9792795139868299)]\n","step: 136, loss: 0.0031824697262161058\n","ypred: [Value(data = 0.947723543048127), Value(data = -0.9793474996905739)]\n","step: 137, loss: 0.003159353720471876\n","ypred: [Value(data = 0.9479152538854542), Value(data = -0.9794147344617595)]\n","step: 138, loss: 0.003136573935096568\n","ypred: [Value(data = 0.9481048821812368), Value(data = -0.9794812320234336)]\n","step: 139, loss: 0.0031141230926994812\n","ypred: [Value(data = 0.9482924652454272), Value(data = -0.9795470057514177)]\n","step: 140, loss: 0.0030919941241278927\n","ypred: [Value(data = 0.9484780394608725), Value(data = -0.979612068685486)]\n","step: 141, loss: 0.0030701801610807516\n","ypred: [Value(data = 0.9486616403126787), Value(data = -0.9796764335401094)]\n","step: 142, loss: 0.003048674529034364\n","ypred: [Value(data = 0.9488433024164498), Value(data = -0.9797401127147825)]\n","step: 143, loss: 0.0030274707404645343\n","ypred: [Value(data = 0.9490230595454469), Value(data = -0.979803118303955)]\n","step: 144, loss: 0.0030065624883510965\n","ypred: [Value(data = 0.9492009446567223), Value(data = -0.9798654621065817)]\n","step: 145, loss: 0.0029859436399508866\n","ypred: [Value(data = 0.949376989916268), Value(data = -0.9799271556353133)]\n","step: 146, loss: 0.002965608230826571\n","ypred: [Value(data = 0.9495512267232248), Value(data = -0.9799882101253393)]\n","step: 147, loss: 0.002945550459119041\n","ypred: [Value(data = 0.949723685733195), Value(data = -0.9800486365428996)]\n","step: 148, loss: 0.0029257646800518607\n","ypred: [Value(data = 0.9498943968806928), Value(data = -0.9801084455934822)]\n","step: 149, loss: 0.002906245400656988\n","ypred: [Value(data = 0.9500633894007753), Value(data = -0.9801676477297154)]\n","step: 150, loss: 0.002886987274711264\n","ypred: [Value(data = 0.9502306918498847), Value(data = -0.980226253158974)]\n","step: 151, loss: 0.0028679850978741137\n","ypred: [Value(data = 0.9503963321259382), Value(data = -0.980284271850705)]\n","step: 152, loss: 0.002849233803017133\n","ypred: [Value(data = 0.9505603374876962), Value(data = -0.9803417135434894)]\n","step: 153, loss: 0.0028307284557367255\n","ypred: [Value(data = 0.9507227345734395), Value(data = -0.9803985877518531)]\n","step: 154, loss: 0.0028124642500415016\n","ypred: [Value(data = 0.9508835494189849), Value(data = -0.9804549037728285)]\n","step: 155, loss: 0.002794436504206692\n","ypred: [Value(data = 0.9510428074750658), Value(data = -0.9805106706922888)]\n","step: 156, loss: 0.002776640656787888\n","ypred: [Value(data = 0.9512005336241045), Value(data = -0.9805658973910559)]\n","step: 157, loss: 0.0027590722627871215\n","ypred: [Value(data = 0.9513567521964017), Value(data = -0.9806205925507947)]\n","step: 158, loss: 0.0027417269899645846\n","ypred: [Value(data = 0.9515114869857662), Value(data = -0.9806747646597025)]\n","step: 159, loss: 0.002724600615289409\n","ypred: [Value(data = 0.9516647612646073), Value(data = -0.9807284220180011)]\n","step: 160, loss: 0.0027076890215236692\n","ypred: [Value(data = 0.9518165977985131), Value(data = -0.9807815727432406)]\n","step: 161, loss: 0.0026909881939336014\n","ypred: [Value(data = 0.9519670188603332), Value(data = -0.9808342247754208)]\n","step: 162, loss: 0.0026744942171226804\n","ypred: [Value(data = 0.9521160462437871), Value(data = -0.9808863858819395)]\n","step: 163, loss: 0.0026582032719812557\n","ypred: [Value(data = 0.9522637012766165), Value(data = -0.9809380636623733)]\n","step: 164, loss: 0.002642111632747834\n","ypred: [Value(data = 0.9524100048332997), Value(data = -0.9809892655530981)]\n","step: 165, loss: 0.002626215664177178\n","ypred: [Value(data = 0.9525549773473441), Value(data = -0.9810399988317545)]\n","step: 166, loss: 0.0026105118188109048\n","ypred: [Value(data = 0.9526986388231751), Value(data = -0.9810902706215671)]\n","step: 167, loss: 0.0025949966343460106\n","ypred: [Value(data = 0.9528410088476356), Value(data = -0.981140087895519)]\n","step: 168, loss: 0.002579666731097537\n","ypred: [Value(data = 0.9529821066011113), Value(data = -0.9811894574803911)]\n","step: 169, loss: 0.002564518809551279\n","ypred: [Value(data = 0.9531219508682965), Value(data = -0.981238386060669)]\n","step: 170, loss: 0.002549549648002901\n","ypred: [Value(data = 0.9532605600486136), Value(data = -0.9812868801823244)]\n","step: 171, loss: 0.002534756100279941\n","ypred: [Value(data = 0.9533979521662996), Value(data = -0.9813349462564716)]\n","step: 172, loss: 0.0025201350935433026\n","ypred: [Value(data = 0.9535341448801722), Value(data = -0.9813825905629122)]\n","step: 173, loss: 0.002505683626164994\n","ypred: [Value(data = 0.9536691554930868), Value(data = -0.9814298192535615)]\n","step: 174, loss: 0.002491398765679159\n","ypred: [Value(data = 0.9538030009610973), Value(data = -0.9814766383557703)]\n","step: 175, loss: 0.0024772776468033014\n","ypred: [Value(data = 0.9539356979023297), Value(data = -0.981523053775539)]\n","step: 176, loss: 0.00246331746952706\n","ypred: [Value(data = 0.9540672626055804), Value(data = -0.9815690713006344)]\n","step: 177, loss: 0.002449515497265811\n","ypred: [Value(data = 0.9541977110386494), Value(data = -0.9816146966036069)]\n","step: 178, loss: 0.002435869055076486\n","ypred: [Value(data = 0.9543270588564168), Value(data = -0.981659935244715)]\n","step: 179, loss: 0.0024223755279332657\n","ypred: [Value(data = 0.9544553214086727), Value(data = -0.9817047926747606)]\n","step: 180, loss: 0.002409032359060797\n","ypred: [Value(data = 0.9545825137477095), Value(data = -0.9817492742378359)]\n","step: 181, loss: 0.0023958370483227157\n","ypred: [Value(data = 0.9547086506356842), Value(data = -0.9817933851739887)]\n","step: 182, loss: 0.002382787150663241\n","ypred: [Value(data = 0.9548337465517606), Value(data = -0.9818371306218051)]\n","step: 183, loss: 0.00236988027459997\n","ypred: [Value(data = 0.9549578156990363), Value(data = -0.981880515620915)]\n","step: 184, loss: 0.002357114080765888\n","ypred: [Value(data = 0.9550808720112651), Value(data = -0.9819235451144239)]\n","step: 185, loss: 0.002344486280498612\n","ypred: [Value(data = 0.9552029291593809), Value(data = -0.9819662239512692)]\n","step: 186, loss: 0.002331994634475225\n","ypred: [Value(data = 0.9553240005578277), Value(data = -0.9820085568885106)]\n","step: 187, loss: 0.0023196369513909347\n","ypred: [Value(data = 0.9554440993707068), Value(data = -0.9820505485935498)]\n","step: 188, loss: 0.0023074110866799637\n","ypred: [Value(data = 0.9555632385177436), Value(data = -0.9820922036462856)]\n","step: 189, loss: 0.002295314941277053\n","ypred: [Value(data = 0.9556814306800818), Value(data = -0.9821335265412081)]\n","step: 190, loss: 0.0022833464604181076\n","ypred: [Value(data = 0.9557986883059119), Value(data = -0.9821745216894303)]\n","step: 191, loss: 0.0022715036324785234\n","ypred: [Value(data = 0.9559150236159363), Value(data = -0.9822151934206609)]\n","step: 192, loss: 0.0022597844878479603\n","ypred: [Value(data = 0.9560304486086808), Value(data = -0.982255545985122)]\n","step: 193, loss: 0.00224818709783998\n","ypred: [Value(data = 0.9561449750656543), Value(data = -0.9822955835554108)]\n","step: 194, loss: 0.0022367095736355214\n","ypred: [Value(data = 0.9562586145563629), Value(data = -0.9823353102283078)]\n","step: 195, loss: 0.0022253500652589598\n","ypred: [Value(data = 0.9563713784431838), Value(data = -0.9823747300265363)]\n","step: 196, loss: 0.0022141067605853716\n","ypred: [Value(data = 0.9564832778861028), Value(data = -0.9824138469004694)]\n","step: 197, loss: 0.0022029778843782825\n","ypred: [Value(data = 0.9565943238473199), Value(data = -0.9824526647297914)]\n","step: 198, loss: 0.0021919616973564474\n","ypred: [Value(data = 0.9567045270957282), Value(data = -0.982491187325112)]\n","step: 199, loss: 0.0021810564952888553\n"]}],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  # print('-------')"]},{"cell_type":"code","execution_count":859,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":860,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","- Results at end of iteration -\n","ypred_data: [0.9568138982112689, -0.9825294184295359]\n","loss: 0.00217026060811688\n"]}],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [value.data for value in ypred]\n","\n","print(f'\\n- Results at end of iteration -')\n","print(f'ypred_data: {ypred_data}')\n","print(f'loss: {loss.data}')  "]},{"cell_type":"code","execution_count":861,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","Calculate Output of Layer: 0\n","weights (3, 2):\n","[[-0.18742949  1.00574364]\n"," [-0.42072644 -1.04722017]\n"," [-0.93618265  0.94360256]]\n","\n","input (2, 2):\n","[[ 2.  3.]\n"," [ 3. -1.]]\n","\n","weights_x_inputs (3, 2):\n","[[ 2.64237193 -1.56803212]\n"," [-3.9831134  -0.21495916]\n"," [ 0.95844238 -3.75215051]]\n","\n","bias (3, 1):\n","[[0.70016171]\n"," [0.14891615]\n"," [0.00129682]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[ 3.34253364 -0.86787041]\n"," [-3.83419724 -0.066043  ]\n"," [ 0.9597392  -3.75085369]]\n","\n","Layer 0 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[ 0.99750426 -0.70029052]\n"," [-0.9990657  -0.06594715]\n"," [ 0.74416052 -0.99889633]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 1\n","weights (3, 3):\n","[[-0.28132113  0.30040299  0.9723427 ]\n"," [ 1.09615895 -1.01587297 -0.45527363]\n"," [-1.10958447  0.16491054 -0.35729474]]\n","\n","input (3, 2):\n","[[ 0.99750426 -0.70029052]\n"," [-0.9990657  -0.06594715]\n"," [ 0.74416052 -0.99889633]]\n","\n","weights_x_inputs (3, 2):\n","[[ 0.1428377  -0.79407375]\n"," [ 1.7695504  -0.24586464]\n"," [-1.53745633  1.12305652]]\n","\n","bias (3, 1):\n","[[-0.74608719]\n"," [-0.41930097]\n"," [ 0.1309801 ]]\n","\n","weights_x_inputs_+_bias (3, 2):\n","[[-0.60324949 -1.54016095]\n"," [ 1.35024943 -0.66516561]\n"," [-1.40647623  1.25403662]]\n","\n","Layer 1 Output = tanh(weights_x_inputs_+_bias) (3, 2):\n","[[-0.5393578  -0.91214741]\n"," [ 0.87411215 -0.58179084]\n"," [-0.8867435   0.8494117 ]]\n","\n","--------------------------------------------------\n","Calculate Output of Layer: 2\n","weights (1, 3):\n","[[ 0.18299483  1.2548591  -1.36935372]]\n","\n","input (3, 2):\n","[[-0.5393578  -0.91214741]\n"," [ 0.87411215 -0.58179084]\n"," [-0.8867435   0.8494117 ]]\n","\n","weights_x_inputs (1, 2):\n","[[ 2.2124534  -2.06012886]]\n","\n","bias (1, 1):\n","[[-0.30567633]]\n","\n","weights_x_inputs_+_bias (1, 2):\n","[[ 1.90677707 -2.36580519]]\n","\n","Layer 2 Output = tanh(weights_x_inputs_+_bias) (1, 2):\n","[[ 0.9568139  -0.98252942]]\n","\n","-- Results of neural network outputs and Loss --\n","yout:           [ 0.9568139  -0.98252942]\n","desired output: [1.0, -1.0]\n","err:            [-0.0431861   0.01747058]\n","err_sq:         [0.00186504 0.00030522]\n","loss_mean:      0.0010851303040584417\n","loss_sum:       0.0021702606081168835\n"]}],"source":["yout, err, err_sq, loss_sum, loss_mean, w_mats, b_mats = forward_pass(n.layers, verbose=verbose)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","\n","# loss = torch.sum(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.mean(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for item in output.item:\n","#   print(item)\n","# type(output)\n","output.tolist()[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def mse_loss(y_true, y_pred):\n","  \"\"\"Calculates the mean squared error loss.\n","\n","  Args:\n","    y_true: The ground truth labels.\n","    y_pred: The predicted labels.\n","\n","  Returns:\n","    The mean squared error loss.\n","  \"\"\"\n","\n","  loss = np.mean((y_true - y_pred)**2)\n","  return loss\n","\n","def main():\n","  \"\"\"Main function.\"\"\"\n","\n","  # y_true = np.array([1, 2, 3, 4, 5])\n","  y_true = np.array([1.0, -1.0])\n","\n","  # y_pred = np.array([0, 1, 2, 3, 4])\n","  # y_pred = np.array([0.9997345209121704, -0.9980572462081909])\n","  y_pred = np.array(output.tolist()[0])  \n","\n","  loss = mse_loss(y_true, y_pred)\n","  print(loss)\n","\n","if __name__ == \"__main__\":\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(squared_difference)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.sum((output - torch.tensor(ys))**2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Check Output and Gradient Calculation with PyTorch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x0 = torch.Tensor([-3.0]).double();      x0.requires_grad = True\n","x1 = torch.Tensor([0.0]).double();       x1.requires_grad = True\n","x2 = torch.Tensor([0.5]).double();       x2.requires_grad = True\n","w0 = torch.Tensor([2.0]).double();       w0.requires_grad = True\n","w1 = torch.Tensor([1.0]).double();       w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double();       w2.requires_grad = True\n","b = torch.Tensor([4.61862664]).double(); b.requires_grad  = True\n","n = x0*w0 + x1*w1 + x2*w2 + b\n","o3 = torch.tanh(n)\n","o3.backward()\n","\n","print('---- torch results matched backward pass results ----')\n","print(f'x0.data.item()  = {x0.data.item():>9.6f}')\n","print(f'x0.grad.item()  = {x0.grad.item():>9.6f}')\n","print(f'w0.data.item()  = {w0.data.item():>9.6f}')\n","print(f'w0.grad.item()  = {w0.grad.item():>9.6f} <-- result matched micrograd')\n","print('---')\n","print(f'x1.data.item()  = {x1.data.item():>9.6f}')\n","print(f'x1.grad.item()  = {x1.grad.item():>9.6f}')\n","print(f'w1.data.item()  = {w1.data.item():>9.6f}')\n","print(f'w1.grad.item()  = {w1.grad.item():>9.6f}')\n","print('---')\n","print(f'x2.data.item()  = {x2.data.item():>9.6f}')\n","print(f'x2.grad.item()  = {x2.grad.item():>9.6f}')\n","print(f'w2.data.item()  = {w2.data.item():>9.6f}')\n","print(f'w2.grad.item()  = {w2.grad.item():>9.6f}')\n","print('---')\n","print(f'out.data.item() = {o3.data.item():>9.6f} <-- result matched micrograd')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Neural Network MLP(3, [4, 4, 1])\n","    input layer:     3 nodes\n","    hidden layer 1:  4 nodes\n","    hidden layer 2:  4 nodes\n","    output layer:    1 node\n","\n","<!-- ![Getting Started](..\\karpathy\\img\\Nertual_Network_Neuron.PNG) -->\n","<img src=\"..\\karpathy\\img\\neural_network_neuron.PNG\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Create neural work, initialize weights and biases, define inputs and desired outputs "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create neural network and initialize weights and biases\n","n = MLP(3, [4, 4, 1])\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# number of parameters (e.g sum (weights + bias to each neuron and output))\n","# MLP(3, [4, 4, 1]) --> 4_neurons(3_inputs + 1_bias) + 4_neurons(4_neurons + 1_bias) + 1_output(4_neurons + 1_bias) = 41_parameters \n","print(f'parameters in MLP: {len(n.parameters())}\\n')\n","\n","# print first 5 parameters\n","for i, v in enumerate(n.parameters()):\n","  if i < 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')\n"," \n","print('---')\n","\n","# print last 5 parameters   \n","for i, v in enumerate(n.parameters()):\n","  if i >= len(n.parameters()) - 5:\n","    print(f'i: {i:>2}, {v.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ---- Start: Calculate Neural Network Output and Loss with Matrix Multiplication ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Transpose inputs xs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xs_mats = [np.array(xs)]  # convert xs to list of np.arrays\n","xs_mats_T = []\n","for mat in xs_mats:\n","  mat_transpose = np.transpose(mat)\n","  xs_mats_T.append(mat_transpose)\n","\n","print(f'xs_mats[0].shape: {xs_mats[0].shape}')\n","print(f'xs_mats:\\n{xs_mats}\\n')\n","print(f'xs_mats_T[0].shape: {xs_mats_T[0].shape}')\n","print(f'xs_mats_T:\\n{xs_mats_T}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Get Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["layer_cnt = len(n.layers)\n","w_mats = []  # list of weights matrix for each layer \n","b_mats = []  # list of bias matrix for each layer\n","print(f'layer_cnt: {layer_cnt}\\n')\n","for i, layer in enumerate(n.layers):\n","    neuron_cnt = len(layer.neurons)\n","    print(f'layer: {i}, neuron_cnt: {neuron_cnt}')\n","\n","    print('----')\n","    b_mat = []  # accumulate neuon's bias for each row     \n","    for j, neuron in enumerate(layer.neurons):\n","        print(f'layer: {i}, neuron {j}')\n","        b = neuron.b.data  # bias of neuron \n","        w_row = []  # accumulate neuon's weights for each row\n","        # b_row = []  # accumulate neuon's bias for each row\n","        for k, w in enumerate(neuron.w):\n","            w_row.append(w.data)\n","            print(f'w{k}: {w.data:10.7f},   w{k}.grad: {w.grad:10.7f}')\n","        if j == 0:            \n","            w_mat = np.array([w_row])\n","        else:\n","            w_mat = np.vstack((w_mat, w_row))\n","        \n","        b_mat.append(b)\n","        print(f'b:  {b:10.7f}\\n')\n","        # print(f'b:  {b:10.7f}')        \n","        # print(f'b_mat:  {b_mat}\\n')\n","    w_mats.append(w_mat)  \n","    b_mats.append(np.array([b_mat]))        \n","    print('------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Print Neural Network's Weights and Biases Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["zipped_w_n_b = zip(w_mats, b_mats)\n","for i, w_n_b in enumerate(zipped_w_n_b):\n","  print(f'i: {i}')    \n","  print(f'w_mat{w_n_b[0].shape}:\\n{w_n_b[0]}')\n","  print(f'b_mat{w_n_b[1].shape}:\\n{w_n_b[1]}\\n')  \n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Calculate Neural Network Output and Loss with Matrix Multiplication"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"..\\karpathy\\img\\neural_mat.PNG\">"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["verbose = True   # print calculation output and weights and bias matrices \n","# verbose = False  # print calculation output only\n","\n","for layer in range(len(n.layers)):\n","  if layer == 0:  # first layer, use given inputs xs as inputs\n","    input = xs_mats_T[layer]\n","  else:  # after first layer, use outputs from preceding layers as inputs\n","    input = output\n","\n","  weights = w_mats[layer]\n","  bias = np.transpose(b_mats[layer])\n","\n","  weights_x_input = np.matmul(weights, input)\n","  weights_x_input_plus_bias = weights_x_input + bias\n","\n","  # output = np.tanh(np.matmul(weights, input) + bias)\n","  output = np.tanh(weights_x_input_plus_bias)\n","\n","  if verbose:\n","    print(f'{\"-\"*50}')\n","    print(f'layer: {layer}')\n","    print(f'weights {weights.shape}:\\n{weights}\\n')\n","    print(f'input {input.shape}:\\n{input}\\n')\n","\n","    print(f'weights_x_inputs {weights_x_input.shape}:\\n{weights_x_input}\\n')\n","    print(f'bias {bias.shape}:\\n{bias}\\n')\n","    print(f'weights_x_inputs_plus_bias {weights_x_input_plus_bias.shape}:\\n{weights_x_input_plus_bias}\\n')\n","\n","    print(f'output {output.shape}:\\n{output}\\n')    \n","\n","yout = output[0]\n","loss = sum((yout - ys)**2)\n","\n","print(f'-- manual forward pass calculation --')\n","print(f'manual calculation: {yout}')   \n","print(f'desired output:     {ys}')   \n","print(f'loss:               {loss}')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ### ---- End: Calculate Neural Network Output and Loss with Matrix Multiplication ---- ----"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prediction with Micrograd Neural Network"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### Micrograd Forward Pass Results, Same as Matrix Multiplication"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ypred = [n(x) for x in xs]\n","loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","ypred_data = [v.data for v in ypred] \n","loss_data = loss.data\n","\n","print(f'-- micrograd forward pass calculation --')\n","print(f'ypred_data:         {ypred_data}')\n","print(f'ys:                 {ys}')\n","print(f'loss_data:          {loss_data}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Micrograd backward pass and update parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# backward pass to calculate gradients\n","for p in n.parameters():\n","  p.grad = 0.0  # zero the gradient \n","loss.backward()\n","\n","# update weights and bias\n","if verbose:\n","  print('=== update parameters ===')\n","  print(f'  i  parameter before         gradient     learning rate      parameter after')\n","for i, p in enumerate(n.parameters()):\n","  p_before = p.data\n","  p.data += -learning_rate * p.grad\n","  if verbose:    \n","    print(f'{i:>3}  {p_before:>16.10f}   {p.grad:>14.10f}    {learning_rate:>14.5f}       {p.data:>14.10f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Improve Prediction with Parameter Iteration "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a list of losses\n","losses = []\n","for k in range(200):\n","  # forward pass\n","  ypred = [n(x) for x in xs]\n","  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))  # low loss is better, perfect is loss = 0\n","  losses.append(loss.data)\n","\n","  # backward pass to calculate gradients\n","  for p in n.parameters():\n","    p.grad = 0.0  # zero the gradient \n","  loss.backward()\n","\n","  # update weights and bias\n","  for p in n.parameters():\n","      p.data += -learning_rate * p.grad\n","\n","  # print(f'x: {x}')\n","  print(f'ypred: {ypred}')\n","  print(f'step: {k}, loss: {loss.data}')   \n","  print('-------')  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Build same model with pyTorch "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class MLP_torch(nn.Module):\n","    def __init__(self):\n","        super(MLP_torch, self).__init__()\n","        self.fc1 = nn.Linear(3, 4)\n","        self.fc2 = nn.Linear(4, 4)\n","        # self.fc3 = nn.Linear(4, 4)\n","        self.fc4 = nn.Linear(4, 1)        \n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        # x = torch.tanh(self.fc3(x))        \n","        x = self.fc4(x)  \n","        return x\n","\n","\n","\n","model = MLP_torch()\n","\n","# inputs\n","xs = [\n","  [2.0, 3.0, -1.0],\n","  [3.0, -1.0, 0.5]\n","]\n","\n","# desired targets\n","ys = [1.0, -1.0]\n","\n","# convert to tensor\n","t_xs = torch.tensor(xs)\n","\n","# add a dimension to the index=1 position to target tensor,\n","#  e.g. change size from [2] to [2, 1]\n","t_ys = torch.unsqueeze(torch.tensor(ys), 1)\n","\n","# learning rate (i.e. step size)\n","learning_rate = 0.05\n","\n","losses = []\n","for epoch in range(40):\n","    # forward pass\n","    outputs = model(t_xs)\n","\n","    # calculate loss\n","    loss = torch.nn.functional.mse_loss(outputs, t_ys)\n","\n","    # remove loss gradient \n","    losses.append(loss.detach())\n","\n","    # backpropagate\n","    loss.backward()\n","\n","    # update weights\n","    for p in model.parameters():\n","        p.data -= learning_rate * p.grad.data\n","\n","    # zero gradients\n","    for p in model.parameters():\n","        p.grad.data.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch} loss: {loss}\")\n","\n","prediction = model(t_xs)\n","print('')\n","print(f\"Prediction:\\n{prediction.detach()}\")\n","print(f\"Loss: {loss}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_losses(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'input xs:\\n{xs}\\n')\n","print(f'target ys:\\n{ys}')\n","print('---------\\n')\n","l_items = list(model.parameters())\n","if len(l_items) % 2 == 0:\n","  for i in range(0, len(l_items), 2):\n","    if i == 0:\n","      x0 = torch.clone(t_xs).detach() \n","      input = torch.transpose(x0, 0, 1)\n","    else:\n","      input = output\n","\n","    w = l_items[i].detach()  # remove gradient\n","    b_ = l_items[i + 1].detach()  # remove gradient\n","    b = torch.clone(b_).detach()  # remove gradient\n","    bT = torch.unsqueeze(b, 1)  # add a dimension to index 1 position\n","    w_input = torch.matmul(w, input)\n","    w_input_bT = torch.add(w_input, bT)\n","\n","    if i == len(l_items) - 2:  # skip tanh activation on output node\n","      output = w_input_bT\n","    else:  \n","      output = torch.tanh(w_input_bT)      \n","\n","    print(f'layer: {i / 2},  i: {i}\\n')\n","    print(f'w,  {w.shape}:\\n{w}\\n')\n","    print(f'input,  {input.shape}:\\n{input}\\n')\n","    print(f'w * input,  {w_input.shape}:\\n{w_input}\\n')        \n","    print(f'bT,  {bT.shape}:\\n{bT}\\n')\n","    print(f'w * input + bT,  {w_input_bT.shape}:\\n{w_input_bT}\\n')\n","    print(f'output,  {output.shape}:\\n{output}\\n')            \n","    print('')\n","else:\n","  raise ValueError(f\"len(l_items) {len(l_items)} is not divisible by 2.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","print(output, output.shape)\n","print(t_ys_, t_ys_.shape)\n","\n","difference = output - t_ys_\n","squared_difference = torch.pow(difference, 2)\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference)\n","loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(output, output.shape)\n","print(torch.tensor(ys), torch.tensor(ys).shape)\n","\n","difference = output - torch.tensor(ys)\n","print(f'difference: {difference}')\n","squared_difference = torch.pow(difference, 2)\n","print(f'squared_difference: {squared_difference}')\n","# loss = torch.sum(squared_difference) / len(squared_difference)\n","loss = torch.sum(squared_difference) / 2\n","loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["difference\n","len(squared_difference)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t_ys = torch.tensor(ys)\n","t_ys_ = torch.unsqueeze(t_ys, 0)\n","t_ys_.shape\n","\n","torch.nn.functional.mse_loss(output, t_ys_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.sum((output - torch.tensor(ys))**2)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
