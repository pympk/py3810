{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Understanding PyTorch with an example: a step-by-step tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation, random data around the line y = 2x + 1\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)  # uniform distribution [0, 1], matrix 100x1, e.g. [[0.73199394], ... ,[0.10789143]]\n",
    "y = 1 + 2*x + 0.1*np.random.randn(100, 1)  # standard normal distribution, mean=0, std=1\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(100)  # 0, ... , 99\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Use first 80 random indices for training\n",
    "train_idx = idx[:80]  # first 80 sample of idx\n",
    "# Use remaining indices for validation\n",
    "val_idx = idx[80:] \n",
    "\n",
    "# Generate train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the x and y values for the first plot\n",
    "x1 = x_train\n",
    "y1 = y_train\n",
    "\n",
    "# Define the x and y values for the second plot\n",
    "x2 = x_val\n",
    "y2 = y_val\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Space the plots further apart\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "# Plot the first plot\n",
    "ax1.plot(x1, y1, marker=\"o\", color=\"blue\", linestyle=\"None\")\n",
    "ax1.set_title(\"Generated Data - Train\")\n",
    "\n",
    "# Plot the second plot\n",
    "ax2.plot(x2, y2, marker=\"o\", color=\"red\", linestyle=\"None\")\n",
    "ax2.set_title(\"Generated Data - Validation\")\n",
    "\n",
    "# Add a title to the figure\n",
    "# fig.suptitle(\"Two Plots Side by Side\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a and b after initialization\n",
      "a: [0.49671415], b: [-0.1382643]\n",
      "\n",
      "a and b after our gradient descent\n",
      "a: [1.02354094], b: [1.96896411]\n",
      "a.shape: (1,), b.shape: (1,), x_train.shape: (80, 1), y_train.shape: (80, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters \"a\" and \"b\" randomly\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "print('a and b after initialization')\n",
    "print(f'a: {a}, b: {b}\\n')\n",
    "\n",
    "# Set learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Define number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # Compute predicted output\n",
    "  yhat = a + b*x_train\n",
    "\n",
    "  # Calculate error\n",
    "  error = (y_train - yhat)\n",
    "\n",
    "  # Calcualte loss\n",
    "  loss = (error**2).mean()\n",
    "\n",
    "  # Calculate gradients for \"a\" (a constant) and \"b\" (the slope)\n",
    "  a_grad = -2*error.mean()\n",
    "  b_grad = -2*(x_train*error).mean()\n",
    "\n",
    "  # Update parameters a and b using gradients and learning rate\n",
    "  a += -lr*a_grad\n",
    "  b += -lr*b_grad\n",
    "\n",
    "print('a and b after our gradient descent')\n",
    "print(f'a: {a}, b: {b}')\n",
    "print(f'a.shape: {a.shape}, b.shape: {b.shape}, x_train.shape: {x_train.shape}, y_train.shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept and coef. from  Scikit-Learn\n",
      "linr.intercept_: [1.02354075],  linr.coef_[0]: [1.96896447]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: do we get the same results as our model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print('intercept and coef. from  Scikit-Learn')\n",
    "print(f'linr.intercept_: {linr.intercept_},  linr.coef_[0]: {linr.coef_[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(x_train):         <class 'numpy.ndarray'>\n",
      "type(x_train_tensor):  <class 'torch.Tensor'>\n",
      "x_train_tensor.type(): torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Transform data from numpy arrays to pytorch tensors\n",
    "# and send them to chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "print(f'type(x_train):         {type(x_train)}')\n",
    "print(f'type(x_train_tensor):  {type(x_train_tensor)}')\n",
    "print(f'x_train_tensor.type(): {x_train_tensor.type()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0.5043], requires_grad=True),  a.shape: torch.Size([1])\n",
      "b: tensor([0.6755], requires_grad=True),  b.shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(f'a: {a},  a.shape: {a.shape}\\nb: {b},  b.shape: {b.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this won't work if device is GPU\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "print(f'a: {a},  a.shape: {a.shape}\\nb: {b},  b.shape: {b.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([-1.0089], requires_grad=True),  a.shape: torch.Size([1])\n",
      "b: tensor([0.3153], requires_grad=True),  b.shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# this works if device is GPU\n",
    "a = torch.randn(1, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "a.requires_grad_()  # _ makes changes in-place\n",
    "b.requires_grad_()  # _ makes changes in-place\n",
    "print(f'a: {a},  a.shape: {a.shape}\\nb: {b},  b.shape: {b.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0.3367], requires_grad=True),  a.shape: torch.Size([1])\n",
      "b: tensor([0.1288], requires_grad=True),  b.shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch recommends specifying device at creation\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(f'a: {a},  a.shape: {a.shape}\\nb: {b},  b.shape: {b.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0.3367], requires_grad=True),  a.shape: torch.Size([1])\n",
      "b: tensor([0.1288], requires_grad=True),  b.shape: torch.Size([1])\n",
      "\n",
      "a.grad: tensor([-3.1125]),  b.grad: tensor([-1.8156])\n",
      "a.grad: tensor([-2.3184]),  b.grad: tensor([-1.4064])\n",
      "a.grad: tensor([-1.7219]),  b.grad: tensor([-1.0982])\n",
      "a.grad: tensor([-1.2737]),  b.grad: tensor([-0.8659])\n",
      "a.grad: tensor([-0.9372]),  b.grad: tensor([-0.6906])\n",
      "a.grad: tensor([-0.6845]),  b.grad: tensor([-0.5583])\n",
      "a.grad: tensor([-0.4948]),  b.grad: tensor([-0.4582])\n",
      "a.grad: tensor([-0.3526]),  b.grad: tensor([-0.3824])\n",
      "a.grad: tensor([-0.2459]),  b.grad: tensor([-0.3248])\n",
      "a.grad: tensor([-0.1660]),  b.grad: tensor([-0.2810])\n",
      "a.grad: tensor([-0.1063]),  b.grad: tensor([-0.2475])\n",
      "a.grad: tensor([-0.0616]),  b.grad: tensor([-0.2218])\n",
      "a.grad: tensor([-0.0283]),  b.grad: tensor([-0.2019])\n",
      "a.grad: tensor([-0.0036]),  b.grad: tensor([-0.1864])\n",
      "a.grad: tensor([0.0147]),  b.grad: tensor([-0.1743])\n",
      "a.grad: tensor([0.0283]),  b.grad: tensor([-0.1646])\n",
      "a.grad: tensor([0.0382]),  b.grad: tensor([-0.1568])\n",
      "a.grad: tensor([0.0453]),  b.grad: tensor([-0.1505])\n",
      "a.grad: tensor([0.0505]),  b.grad: tensor([-0.1452])\n",
      "a.grad: tensor([0.0541]),  b.grad: tensor([-0.1408])\n",
      "a.grad: tensor([0.0566]),  b.grad: tensor([-0.1370])\n",
      "a.grad: tensor([0.0582]),  b.grad: tensor([-0.1337])\n",
      "a.grad: tensor([0.0592]),  b.grad: tensor([-0.1307])\n",
      "a.grad: tensor([0.0597]),  b.grad: tensor([-0.1280])\n",
      "a.grad: tensor([0.0599]),  b.grad: tensor([-0.1255])\n",
      "a.grad: tensor([0.0598]),  b.grad: tensor([-0.1232])\n",
      "a.grad: tensor([0.0594]),  b.grad: tensor([-0.1211])\n",
      "a.grad: tensor([0.0590]),  b.grad: tensor([-0.1190])\n",
      "a.grad: tensor([0.0584]),  b.grad: tensor([-0.1170])\n",
      "a.grad: tensor([0.0578]),  b.grad: tensor([-0.1151])\n",
      "a.grad: tensor([0.0571]),  b.grad: tensor([-0.1133])\n",
      "a.grad: tensor([0.0564]),  b.grad: tensor([-0.1115])\n",
      "a.grad: tensor([0.0557]),  b.grad: tensor([-0.1098])\n",
      "a.grad: tensor([0.0549]),  b.grad: tensor([-0.1081])\n",
      "a.grad: tensor([0.0541]),  b.grad: tensor([-0.1064])\n",
      "a.grad: tensor([0.0534]),  b.grad: tensor([-0.1048])\n",
      "a.grad: tensor([0.0526]),  b.grad: tensor([-0.1032])\n",
      "a.grad: tensor([0.0518]),  b.grad: tensor([-0.1016])\n",
      "a.grad: tensor([0.0511]),  b.grad: tensor([-0.1001])\n",
      "a.grad: tensor([0.0503]),  b.grad: tensor([-0.0985])\n",
      "a.grad: tensor([0.0496]),  b.grad: tensor([-0.0970])\n",
      "a.grad: tensor([0.0488]),  b.grad: tensor([-0.0956])\n",
      "a.grad: tensor([0.0481]),  b.grad: tensor([-0.0941])\n",
      "a.grad: tensor([0.0474]),  b.grad: tensor([-0.0927])\n",
      "a.grad: tensor([0.0466]),  b.grad: tensor([-0.0913])\n",
      "a.grad: tensor([0.0459]),  b.grad: tensor([-0.0899])\n",
      "a.grad: tensor([0.0453]),  b.grad: tensor([-0.0886])\n",
      "a.grad: tensor([0.0446]),  b.grad: tensor([-0.0872])\n",
      "a.grad: tensor([0.0439]),  b.grad: tensor([-0.0859])\n",
      "a.grad: tensor([0.0432]),  b.grad: tensor([-0.0846])\n",
      "a.grad: tensor([0.0426]),  b.grad: tensor([-0.0833])\n",
      "a.grad: tensor([0.0419]),  b.grad: tensor([-0.0821])\n",
      "a.grad: tensor([0.0413]),  b.grad: tensor([-0.0808])\n",
      "a.grad: tensor([0.0407]),  b.grad: tensor([-0.0796])\n",
      "a.grad: tensor([0.0401]),  b.grad: tensor([-0.0784])\n",
      "a.grad: tensor([0.0395]),  b.grad: tensor([-0.0772])\n",
      "a.grad: tensor([0.0389]),  b.grad: tensor([-0.0761])\n",
      "a.grad: tensor([0.0383]),  b.grad: tensor([-0.0749])\n",
      "a.grad: tensor([0.0377]),  b.grad: tensor([-0.0738])\n",
      "a.grad: tensor([0.0371]),  b.grad: tensor([-0.0727])\n",
      "a.grad: tensor([0.0366]),  b.grad: tensor([-0.0716])\n",
      "a.grad: tensor([0.0360]),  b.grad: tensor([-0.0705])\n",
      "a.grad: tensor([0.0355]),  b.grad: tensor([-0.0694])\n",
      "a.grad: tensor([0.0349]),  b.grad: tensor([-0.0684])\n",
      "a.grad: tensor([0.0344]),  b.grad: tensor([-0.0673])\n",
      "a.grad: tensor([0.0339]),  b.grad: tensor([-0.0663])\n",
      "a.grad: tensor([0.0334]),  b.grad: tensor([-0.0653])\n",
      "a.grad: tensor([0.0329]),  b.grad: tensor([-0.0643])\n",
      "a.grad: tensor([0.0324]),  b.grad: tensor([-0.0634])\n",
      "a.grad: tensor([0.0319]),  b.grad: tensor([-0.0624])\n",
      "a.grad: tensor([0.0314]),  b.grad: tensor([-0.0615])\n",
      "a.grad: tensor([0.0309]),  b.grad: tensor([-0.0605])\n",
      "a.grad: tensor([0.0305]),  b.grad: tensor([-0.0596])\n",
      "a.grad: tensor([0.0300]),  b.grad: tensor([-0.0587])\n",
      "a.grad: tensor([0.0296]),  b.grad: tensor([-0.0578])\n",
      "a.grad: tensor([0.0291]),  b.grad: tensor([-0.0570])\n",
      "a.grad: tensor([0.0287]),  b.grad: tensor([-0.0561])\n",
      "a.grad: tensor([0.0282]),  b.grad: tensor([-0.0552])\n",
      "a.grad: tensor([0.0278]),  b.grad: tensor([-0.0544])\n",
      "a.grad: tensor([0.0274]),  b.grad: tensor([-0.0536])\n",
      "a.grad: tensor([0.0270]),  b.grad: tensor([-0.0528])\n",
      "a.grad: tensor([0.0266]),  b.grad: tensor([-0.0520])\n",
      "a.grad: tensor([0.0262]),  b.grad: tensor([-0.0512])\n",
      "a.grad: tensor([0.0258]),  b.grad: tensor([-0.0504])\n",
      "a.grad: tensor([0.0254]),  b.grad: tensor([-0.0497])\n",
      "a.grad: tensor([0.0250]),  b.grad: tensor([-0.0489])\n",
      "a.grad: tensor([0.0246]),  b.grad: tensor([-0.0482])\n",
      "a.grad: tensor([0.0242]),  b.grad: tensor([-0.0474])\n",
      "a.grad: tensor([0.0239]),  b.grad: tensor([-0.0467])\n",
      "a.grad: tensor([0.0235]),  b.grad: tensor([-0.0460])\n",
      "a.grad: tensor([0.0232]),  b.grad: tensor([-0.0453])\n",
      "a.grad: tensor([0.0228]),  b.grad: tensor([-0.0446])\n",
      "a.grad: tensor([0.0225]),  b.grad: tensor([-0.0440])\n",
      "a.grad: tensor([0.0221]),  b.grad: tensor([-0.0433])\n",
      "a.grad: tensor([0.0218]),  b.grad: tensor([-0.0426])\n",
      "a.grad: tensor([0.0215]),  b.grad: tensor([-0.0420])\n",
      "a.grad: tensor([0.0211]),  b.grad: tensor([-0.0414])\n",
      "a.grad: tensor([0.0208]),  b.grad: tensor([-0.0407])\n",
      "a.grad: tensor([0.0205]),  b.grad: tensor([-0.0401])\n",
      "a.grad: tensor([0.0202]),  b.grad: tensor([-0.0395])\n",
      "a.grad: tensor([0.0199]),  b.grad: tensor([-0.0389])\n",
      "a.grad: tensor([0.0196]),  b.grad: tensor([-0.0383])\n",
      "a.grad: tensor([0.0193]),  b.grad: tensor([-0.0378])\n",
      "a.grad: tensor([0.0190]),  b.grad: tensor([-0.0372])\n",
      "a.grad: tensor([0.0187]),  b.grad: tensor([-0.0366])\n",
      "a.grad: tensor([0.0184]),  b.grad: tensor([-0.0361])\n",
      "a.grad: tensor([0.0182]),  b.grad: tensor([-0.0355])\n",
      "a.grad: tensor([0.0179]),  b.grad: tensor([-0.0350])\n",
      "a.grad: tensor([0.0176]),  b.grad: tensor([-0.0345])\n",
      "a.grad: tensor([0.0173]),  b.grad: tensor([-0.0339])\n",
      "a.grad: tensor([0.0171]),  b.grad: tensor([-0.0334])\n",
      "a.grad: tensor([0.0168]),  b.grad: tensor([-0.0329])\n",
      "a.grad: tensor([0.0166]),  b.grad: tensor([-0.0324])\n",
      "a.grad: tensor([0.0163]),  b.grad: tensor([-0.0319])\n",
      "a.grad: tensor([0.0161]),  b.grad: tensor([-0.0315])\n",
      "a.grad: tensor([0.0158]),  b.grad: tensor([-0.0310])\n",
      "a.grad: tensor([0.0156]),  b.grad: tensor([-0.0305])\n",
      "a.grad: tensor([0.0154]),  b.grad: tensor([-0.0301])\n",
      "a.grad: tensor([0.0151]),  b.grad: tensor([-0.0296])\n",
      "a.grad: tensor([0.0149]),  b.grad: tensor([-0.0291])\n",
      "a.grad: tensor([0.0147]),  b.grad: tensor([-0.0287])\n",
      "a.grad: tensor([0.0145]),  b.grad: tensor([-0.0283])\n",
      "a.grad: tensor([0.0142]),  b.grad: tensor([-0.0278])\n",
      "a.grad: tensor([0.0140]),  b.grad: tensor([-0.0274])\n",
      "a.grad: tensor([0.0138]),  b.grad: tensor([-0.0270])\n",
      "a.grad: tensor([0.0136]),  b.grad: tensor([-0.0266])\n",
      "a.grad: tensor([0.0134]),  b.grad: tensor([-0.0262])\n",
      "a.grad: tensor([0.0132]),  b.grad: tensor([-0.0258])\n",
      "a.grad: tensor([0.0130]),  b.grad: tensor([-0.0254])\n",
      "a.grad: tensor([0.0128]),  b.grad: tensor([-0.0250])\n",
      "a.grad: tensor([0.0126]),  b.grad: tensor([-0.0247])\n",
      "a.grad: tensor([0.0124]),  b.grad: tensor([-0.0243])\n",
      "a.grad: tensor([0.0122]),  b.grad: tensor([-0.0239])\n",
      "a.grad: tensor([0.0120]),  b.grad: tensor([-0.0236])\n",
      "a.grad: tensor([0.0119]),  b.grad: tensor([-0.0232])\n",
      "a.grad: tensor([0.0117]),  b.grad: tensor([-0.0228])\n",
      "a.grad: tensor([0.0115]),  b.grad: tensor([-0.0225])\n",
      "a.grad: tensor([0.0113]),  b.grad: tensor([-0.0222])\n",
      "a.grad: tensor([0.0112]),  b.grad: tensor([-0.0218])\n",
      "a.grad: tensor([0.0110]),  b.grad: tensor([-0.0215])\n",
      "a.grad: tensor([0.0108]),  b.grad: tensor([-0.0212])\n",
      "a.grad: tensor([0.0107]),  b.grad: tensor([-0.0209])\n",
      "a.grad: tensor([0.0105]),  b.grad: tensor([-0.0205])\n",
      "a.grad: tensor([0.0103]),  b.grad: tensor([-0.0202])\n",
      "a.grad: tensor([0.0102]),  b.grad: tensor([-0.0199])\n",
      "a.grad: tensor([0.0100]),  b.grad: tensor([-0.0196])\n",
      "a.grad: tensor([0.0099]),  b.grad: tensor([-0.0193])\n",
      "a.grad: tensor([0.0097]),  b.grad: tensor([-0.0190])\n",
      "a.grad: tensor([0.0096]),  b.grad: tensor([-0.0187])\n",
      "a.grad: tensor([0.0094]),  b.grad: tensor([-0.0185])\n",
      "a.grad: tensor([0.0093]),  b.grad: tensor([-0.0182])\n",
      "a.grad: tensor([0.0092]),  b.grad: tensor([-0.0179])\n",
      "a.grad: tensor([0.0090]),  b.grad: tensor([-0.0176])\n",
      "a.grad: tensor([0.0089]),  b.grad: tensor([-0.0174])\n",
      "a.grad: tensor([0.0087]),  b.grad: tensor([-0.0171])\n",
      "a.grad: tensor([0.0086]),  b.grad: tensor([-0.0169])\n",
      "a.grad: tensor([0.0085]),  b.grad: tensor([-0.0166])\n",
      "a.grad: tensor([0.0084]),  b.grad: tensor([-0.0163])\n",
      "a.grad: tensor([0.0082]),  b.grad: tensor([-0.0161])\n",
      "a.grad: tensor([0.0081]),  b.grad: tensor([-0.0159])\n",
      "a.grad: tensor([0.0080]),  b.grad: tensor([-0.0156])\n",
      "a.grad: tensor([0.0079]),  b.grad: tensor([-0.0154])\n",
      "a.grad: tensor([0.0077]),  b.grad: tensor([-0.0151])\n",
      "a.grad: tensor([0.0076]),  b.grad: tensor([-0.0149])\n",
      "a.grad: tensor([0.0075]),  b.grad: tensor([-0.0147])\n",
      "a.grad: tensor([0.0074]),  b.grad: tensor([-0.0145])\n",
      "a.grad: tensor([0.0073]),  b.grad: tensor([-0.0143])\n",
      "a.grad: tensor([0.0072]),  b.grad: tensor([-0.0140])\n",
      "a.grad: tensor([0.0071]),  b.grad: tensor([-0.0138])\n",
      "a.grad: tensor([0.0070]),  b.grad: tensor([-0.0136])\n",
      "a.grad: tensor([0.0069]),  b.grad: tensor([-0.0134])\n",
      "a.grad: tensor([0.0068]),  b.grad: tensor([-0.0132])\n",
      "a.grad: tensor([0.0066]),  b.grad: tensor([-0.0130])\n",
      "a.grad: tensor([0.0065]),  b.grad: tensor([-0.0128])\n",
      "a.grad: tensor([0.0064]),  b.grad: tensor([-0.0126])\n",
      "a.grad: tensor([0.0064]),  b.grad: tensor([-0.0124])\n",
      "a.grad: tensor([0.0063]),  b.grad: tensor([-0.0122])\n",
      "a.grad: tensor([0.0062]),  b.grad: tensor([-0.0121])\n",
      "a.grad: tensor([0.0061]),  b.grad: tensor([-0.0119])\n",
      "a.grad: tensor([0.0060]),  b.grad: tensor([-0.0117])\n",
      "a.grad: tensor([0.0059]),  b.grad: tensor([-0.0115])\n",
      "a.grad: tensor([0.0058]),  b.grad: tensor([-0.0113])\n",
      "a.grad: tensor([0.0057]),  b.grad: tensor([-0.0112])\n",
      "a.grad: tensor([0.0056]),  b.grad: tensor([-0.0110])\n",
      "a.grad: tensor([0.0055]),  b.grad: tensor([-0.0108])\n",
      "a.grad: tensor([0.0055]),  b.grad: tensor([-0.0107])\n",
      "a.grad: tensor([0.0054]),  b.grad: tensor([-0.0105])\n",
      "a.grad: tensor([0.0053]),  b.grad: tensor([-0.0104])\n",
      "a.grad: tensor([0.0052]),  b.grad: tensor([-0.0102])\n",
      "a.grad: tensor([0.0051]),  b.grad: tensor([-0.0100])\n",
      "a.grad: tensor([0.0051]),  b.grad: tensor([-0.0099])\n",
      "a.grad: tensor([0.0050]),  b.grad: tensor([-0.0097])\n",
      "a.grad: tensor([0.0049]),  b.grad: tensor([-0.0096])\n",
      "a.grad: tensor([0.0048]),  b.grad: tensor([-0.0094])\n",
      "a.grad: tensor([0.0048]),  b.grad: tensor([-0.0093])\n",
      "a.grad: tensor([0.0047]),  b.grad: tensor([-0.0092])\n",
      "a.grad: tensor([0.0046]),  b.grad: tensor([-0.0090])\n",
      "a.grad: tensor([0.0045]),  b.grad: tensor([-0.0089])\n",
      "a.grad: tensor([0.0045]),  b.grad: tensor([-0.0088])\n",
      "a.grad: tensor([0.0044]),  b.grad: tensor([-0.0086])\n",
      "a.grad: tensor([0.0043]),  b.grad: tensor([-0.0085])\n",
      "a.grad: tensor([0.0043]),  b.grad: tensor([-0.0084])\n",
      "a.grad: tensor([0.0042]),  b.grad: tensor([-0.0082])\n",
      "a.grad: tensor([0.0041]),  b.grad: tensor([-0.0081])\n",
      "a.grad: tensor([0.0041]),  b.grad: tensor([-0.0080])\n",
      "a.grad: tensor([0.0040]),  b.grad: tensor([-0.0079])\n",
      "a.grad: tensor([0.0040]),  b.grad: tensor([-0.0078])\n",
      "a.grad: tensor([0.0039]),  b.grad: tensor([-0.0076])\n",
      "a.grad: tensor([0.0038]),  b.grad: tensor([-0.0075])\n",
      "a.grad: tensor([0.0038]),  b.grad: tensor([-0.0074])\n",
      "a.grad: tensor([0.0037]),  b.grad: tensor([-0.0073])\n",
      "a.grad: tensor([0.0037]),  b.grad: tensor([-0.0072])\n",
      "a.grad: tensor([0.0036]),  b.grad: tensor([-0.0071])\n",
      "a.grad: tensor([0.0036]),  b.grad: tensor([-0.0070])\n",
      "a.grad: tensor([0.0035]),  b.grad: tensor([-0.0069])\n",
      "a.grad: tensor([0.0035]),  b.grad: tensor([-0.0068])\n",
      "a.grad: tensor([0.0034]),  b.grad: tensor([-0.0067])\n",
      "a.grad: tensor([0.0034]),  b.grad: tensor([-0.0066])\n",
      "a.grad: tensor([0.0033]),  b.grad: tensor([-0.0065])\n",
      "a.grad: tensor([0.0033]),  b.grad: tensor([-0.0064])\n",
      "a.grad: tensor([0.0032]),  b.grad: tensor([-0.0063])\n",
      "a.grad: tensor([0.0032]),  b.grad: tensor([-0.0062])\n",
      "a.grad: tensor([0.0031]),  b.grad: tensor([-0.0061])\n",
      "a.grad: tensor([0.0031]),  b.grad: tensor([-0.0060])\n",
      "a.grad: tensor([0.0030]),  b.grad: tensor([-0.0059])\n",
      "a.grad: tensor([0.0030]),  b.grad: tensor([-0.0058])\n",
      "a.grad: tensor([0.0029]),  b.grad: tensor([-0.0057])\n",
      "a.grad: tensor([0.0029]),  b.grad: tensor([-0.0056])\n",
      "a.grad: tensor([0.0028]),  b.grad: tensor([-0.0055])\n",
      "a.grad: tensor([0.0028]),  b.grad: tensor([-0.0055])\n",
      "a.grad: tensor([0.0027]),  b.grad: tensor([-0.0054])\n",
      "a.grad: tensor([0.0027]),  b.grad: tensor([-0.0053])\n",
      "a.grad: tensor([0.0027]),  b.grad: tensor([-0.0052])\n",
      "a.grad: tensor([0.0026]),  b.grad: tensor([-0.0051])\n",
      "a.grad: tensor([0.0026]),  b.grad: tensor([-0.0051])\n",
      "a.grad: tensor([0.0025]),  b.grad: tensor([-0.0050])\n",
      "a.grad: tensor([0.0025]),  b.grad: tensor([-0.0049])\n",
      "a.grad: tensor([0.0025]),  b.grad: tensor([-0.0048])\n",
      "a.grad: tensor([0.0024]),  b.grad: tensor([-0.0048])\n",
      "a.grad: tensor([0.0024]),  b.grad: tensor([-0.0047])\n",
      "a.grad: tensor([0.0024]),  b.grad: tensor([-0.0046])\n",
      "a.grad: tensor([0.0023]),  b.grad: tensor([-0.0046])\n",
      "a.grad: tensor([0.0023]),  b.grad: tensor([-0.0045])\n",
      "a.grad: tensor([0.0023]),  b.grad: tensor([-0.0044])\n",
      "a.grad: tensor([0.0022]),  b.grad: tensor([-0.0043])\n",
      "a.grad: tensor([0.0022]),  b.grad: tensor([-0.0043])\n",
      "a.grad: tensor([0.0022]),  b.grad: tensor([-0.0042])\n",
      "a.grad: tensor([0.0021]),  b.grad: tensor([-0.0042])\n",
      "a.grad: tensor([0.0021]),  b.grad: tensor([-0.0041])\n",
      "a.grad: tensor([0.0021]),  b.grad: tensor([-0.0040])\n",
      "a.grad: tensor([0.0020]),  b.grad: tensor([-0.0040])\n",
      "a.grad: tensor([0.0020]),  b.grad: tensor([-0.0039])\n",
      "a.grad: tensor([0.0020]),  b.grad: tensor([-0.0038])\n",
      "a.grad: tensor([0.0019]),  b.grad: tensor([-0.0038])\n",
      "a.grad: tensor([0.0019]),  b.grad: tensor([-0.0037])\n",
      "a.grad: tensor([0.0019]),  b.grad: tensor([-0.0037])\n",
      "a.grad: tensor([0.0019]),  b.grad: tensor([-0.0036])\n",
      "a.grad: tensor([0.0018]),  b.grad: tensor([-0.0036])\n",
      "a.grad: tensor([0.0018]),  b.grad: tensor([-0.0035])\n",
      "a.grad: tensor([0.0018]),  b.grad: tensor([-0.0035])\n",
      "a.grad: tensor([0.0017]),  b.grad: tensor([-0.0034])\n",
      "a.grad: tensor([0.0017]),  b.grad: tensor([-0.0034])\n",
      "a.grad: tensor([0.0017]),  b.grad: tensor([-0.0033])\n",
      "a.grad: tensor([0.0017]),  b.grad: tensor([-0.0033])\n",
      "a.grad: tensor([0.0016]),  b.grad: tensor([-0.0032])\n",
      "a.grad: tensor([0.0016]),  b.grad: tensor([-0.0032])\n",
      "a.grad: tensor([0.0016]),  b.grad: tensor([-0.0031])\n",
      "a.grad: tensor([0.0016]),  b.grad: tensor([-0.0031])\n",
      "a.grad: tensor([0.0015]),  b.grad: tensor([-0.0030])\n",
      "a.grad: tensor([0.0015]),  b.grad: tensor([-0.0030])\n",
      "a.grad: tensor([0.0015]),  b.grad: tensor([-0.0029])\n",
      "a.grad: tensor([0.0015]),  b.grad: tensor([-0.0029])\n",
      "a.grad: tensor([0.0015]),  b.grad: tensor([-0.0028])\n",
      "a.grad: tensor([0.0014]),  b.grad: tensor([-0.0028])\n",
      "a.grad: tensor([0.0014]),  b.grad: tensor([-0.0028])\n",
      "a.grad: tensor([0.0014]),  b.grad: tensor([-0.0027])\n",
      "a.grad: tensor([0.0014]),  b.grad: tensor([-0.0027])\n",
      "a.grad: tensor([0.0013]),  b.grad: tensor([-0.0026])\n",
      "a.grad: tensor([0.0013]),  b.grad: tensor([-0.0026])\n",
      "a.grad: tensor([0.0013]),  b.grad: tensor([-0.0026])\n",
      "a.grad: tensor([0.0013]),  b.grad: tensor([-0.0025])\n",
      "a.grad: tensor([0.0013]),  b.grad: tensor([-0.0025])\n",
      "a.grad: tensor([0.0012]),  b.grad: tensor([-0.0024])\n",
      "a.grad: tensor([0.0012]),  b.grad: tensor([-0.0024])\n",
      "a.grad: tensor([0.0012]),  b.grad: tensor([-0.0024])\n",
      "a.grad: tensor([0.0012]),  b.grad: tensor([-0.0023])\n",
      "a.grad: tensor([0.0012]),  b.grad: tensor([-0.0023])\n",
      "a.grad: tensor([0.0012]),  b.grad: tensor([-0.0023])\n",
      "a.grad: tensor([0.0011]),  b.grad: tensor([-0.0022])\n",
      "a.grad: tensor([0.0011]),  b.grad: tensor([-0.0022])\n",
      "a.grad: tensor([0.0011]),  b.grad: tensor([-0.0022])\n",
      "a.grad: tensor([0.0011]),  b.grad: tensor([-0.0021])\n",
      "a.grad: tensor([0.0011]),  b.grad: tensor([-0.0021])\n",
      "a.grad: tensor([0.0011]),  b.grad: tensor([-0.0021])\n",
      "a.grad: tensor([0.0010]),  b.grad: tensor([-0.0020])\n",
      "a.grad: tensor([0.0010]),  b.grad: tensor([-0.0020])\n",
      "a.grad: tensor([0.0010]),  b.grad: tensor([-0.0020])\n",
      "a.grad: tensor([0.0010]),  b.grad: tensor([-0.0019])\n",
      "a.grad: tensor([0.0010]),  b.grad: tensor([-0.0019])\n",
      "a.grad: tensor([0.0010]),  b.grad: tensor([-0.0019])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0019])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0018])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0018])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0018])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0017])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0017])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0017])\n",
      "a.grad: tensor([0.0009]),  b.grad: tensor([-0.0017])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0016])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0016])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0016])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0016])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0015])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0015])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0015])\n",
      "a.grad: tensor([0.0008]),  b.grad: tensor([-0.0015])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0015])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0014])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0014])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0014])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0014])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0013])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0013])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0013])\n",
      "a.grad: tensor([0.0007]),  b.grad: tensor([-0.0013])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0013])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0012])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0012])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0012])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0012])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0012])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0012])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0011])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0011])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0011])\n",
      "a.grad: tensor([0.0006]),  b.grad: tensor([-0.0011])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0011])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0011])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0010])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0010])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0010])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0010])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0010])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0010])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0005]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0009])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0008])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0004]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0007])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0006])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0003]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0005])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0004])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0002]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0003])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([0.0001]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([9.9593e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([9.7900e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([9.6528e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([9.5082e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([9.3635e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([9.2131e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([9.0877e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.9555e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.8250e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.6921e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.5517e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.4098e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.2713e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.1572e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([8.0483e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([7.9146e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([7.8050e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([7.6955e-05]),  b.grad: tensor([-0.0002])\n",
      "a.grad: tensor([7.5683e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([7.4612e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([7.3319e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([7.2197e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([7.1033e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.9905e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.8819e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.7824e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.6845e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.5868e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.4931e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.4093e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.3057e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.2128e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.1230e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([6.0353e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.9323e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.8341e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.7453e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.6731e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.5748e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.4914e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.4108e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.3403e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.2583e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.1776e-05]),  b.grad: tensor([-0.0001])\n",
      "a.grad: tensor([5.1086e-05]),  b.grad: tensor([-9.9640e-05])\n",
      "a.grad: tensor([5.0306e-05]),  b.grad: tensor([-9.8126e-05])\n",
      "a.grad: tensor([4.9496e-05]),  b.grad: tensor([-9.6684e-05])\n",
      "a.grad: tensor([4.8649e-05]),  b.grad: tensor([-9.5258e-05])\n",
      "a.grad: tensor([4.7850e-05]),  b.grad: tensor([-9.3862e-05])\n",
      "a.grad: tensor([4.7218e-05]),  b.grad: tensor([-9.2392e-05])\n",
      "a.grad: tensor([4.6494e-05]),  b.grad: tensor([-9.0986e-05])\n",
      "a.grad: tensor([4.5732e-05]),  b.grad: tensor([-8.9653e-05])\n",
      "a.grad: tensor([4.5110e-05]),  b.grad: tensor([-8.8274e-05])\n",
      "a.grad: tensor([4.4386e-05]),  b.grad: tensor([-8.6958e-05])\n",
      "a.grad: tensor([4.3813e-05]),  b.grad: tensor([-8.5593e-05])\n",
      "a.grad: tensor([4.3080e-05]),  b.grad: tensor([-8.4333e-05])\n",
      "a.grad: tensor([4.2503e-05]),  b.grad: tensor([-8.3017e-05])\n",
      "a.grad: tensor([4.1799e-05]),  b.grad: tensor([-8.1784e-05])\n",
      "a.grad: tensor([4.1258e-05]),  b.grad: tensor([-8.0489e-05])\n",
      "a.grad: tensor([4.0557e-05]),  b.grad: tensor([-7.9301e-05])\n",
      "a.grad: tensor([3.9983e-05]),  b.grad: tensor([-7.8079e-05])\n",
      "a.grad: tensor([3.9206e-05]),  b.grad: tensor([-7.6996e-05])\n",
      "a.grad: tensor([3.8651e-05]),  b.grad: tensor([-7.5801e-05])\n",
      "a.grad: tensor([3.8225e-05]),  b.grad: tensor([-7.4577e-05])\n",
      "a.grad: tensor([3.7706e-05]),  b.grad: tensor([-7.3411e-05])\n",
      "a.grad: tensor([3.7102e-05]),  b.grad: tensor([-7.2300e-05])\n",
      "a.grad: tensor([3.6565e-05]),  b.grad: tensor([-7.1189e-05])\n",
      "a.grad: tensor([3.5922e-05]),  b.grad: tensor([-7.0154e-05])\n",
      "a.grad: tensor([3.5395e-05]),  b.grad: tensor([-6.9097e-05])\n",
      "a.grad: tensor([3.4794e-05]),  b.grad: tensor([-6.8070e-05])\n",
      "a.grad: tensor([3.4280e-05]),  b.grad: tensor([-6.7045e-05])\n",
      "a.grad: tensor([3.3686e-05]),  b.grad: tensor([-6.6071e-05])\n",
      "a.grad: tensor([3.3202e-05]),  b.grad: tensor([-6.5076e-05])\n",
      "a.grad: tensor([3.2708e-05]),  b.grad: tensor([-6.4071e-05])\n",
      "a.grad: tensor([3.2351e-05]),  b.grad: tensor([-6.3037e-05])\n",
      "a.grad: tensor([3.1894e-05]),  b.grad: tensor([-6.2072e-05])\n",
      "a.grad: tensor([3.1340e-05]),  b.grad: tensor([-6.1164e-05])\n",
      "a.grad: tensor([3.0885e-05]),  b.grad: tensor([-6.0245e-05])\n",
      "a.grad: tensor([3.0420e-05]),  b.grad: tensor([-5.9325e-05])\n",
      "a.grad: tensor([2.9830e-05]),  b.grad: tensor([-5.8491e-05])\n",
      "a.grad: tensor([2.9391e-05]),  b.grad: tensor([-5.7597e-05])\n",
      "a.grad: tensor([2.8838e-05]),  b.grad: tensor([-5.6787e-05])\n",
      "a.grad: tensor([2.8547e-05]),  b.grad: tensor([-5.5845e-05])\n",
      "a.grad: tensor([2.8067e-05]),  b.grad: tensor([-5.5037e-05])\n",
      "a.grad: tensor([2.7576e-05]),  b.grad: tensor([-5.4227e-05])\n",
      "a.grad: tensor([2.7156e-05]),  b.grad: tensor([-5.3419e-05])\n",
      "a.grad: tensor([2.6734e-05]),  b.grad: tensor([-5.2610e-05])\n",
      "a.grad: tensor([2.6423e-05]),  b.grad: tensor([-5.1769e-05])\n",
      "a.grad: tensor([2.6060e-05]),  b.grad: tensor([-5.0979e-05])\n",
      "a.grad: tensor([2.5657e-05]),  b.grad: tensor([-5.0212e-05])\n",
      "a.grad: tensor([2.5140e-05]),  b.grad: tensor([-4.9514e-05])\n",
      "a.grad: tensor([2.4864e-05]),  b.grad: tensor([-4.8710e-05])\n",
      "a.grad: tensor([2.4471e-05]),  b.grad: tensor([-4.7985e-05])\n",
      "a.grad: tensor([2.3978e-05]),  b.grad: tensor([-4.7320e-05])\n",
      "a.grad: tensor([2.3684e-05]),  b.grad: tensor([-4.6571e-05])\n",
      "a.grad: tensor([2.3326e-05]),  b.grad: tensor([-4.5856e-05])\n",
      "a.grad: tensor([2.2852e-05]),  b.grad: tensor([-4.5239e-05])\n",
      "a.grad: tensor([2.2600e-05]),  b.grad: tensor([-4.4504e-05])\n",
      "a.grad: tensor([2.2241e-05]),  b.grad: tensor([-4.3848e-05])\n",
      "a.grad: tensor([2.1889e-05]),  b.grad: tensor([-4.3181e-05])\n",
      "a.grad: tensor([2.1611e-05]),  b.grad: tensor([-4.2512e-05])\n",
      "a.grad: tensor([2.1398e-05]),  b.grad: tensor([-4.1804e-05])\n",
      "a.grad: tensor([2.1037e-05]),  b.grad: tensor([-4.1199e-05])\n",
      "a.grad: tensor([2.0692e-05]),  b.grad: tensor([-4.0582e-05])\n",
      "a.grad: tensor([2.0462e-05]),  b.grad: tensor([-3.9930e-05])\n",
      "a.grad: tensor([2.0137e-05]),  b.grad: tensor([-3.9340e-05])\n",
      "a.grad: tensor([1.9828e-05]),  b.grad: tensor([-3.8749e-05])\n",
      "a.grad: tensor([1.9468e-05]),  b.grad: tensor([-3.8180e-05])\n",
      "a.grad: tensor([1.9222e-05]),  b.grad: tensor([-3.7587e-05])\n",
      "a.grad: tensor([1.9011e-05]),  b.grad: tensor([-3.6970e-05])\n",
      "a.grad: tensor([1.8727e-05]),  b.grad: tensor([-3.6407e-05])\n",
      "a.grad: tensor([1.8391e-05]),  b.grad: tensor([-3.5875e-05])\n",
      "a.grad: tensor([1.8169e-05]),  b.grad: tensor([-3.5314e-05])\n",
      "a.grad: tensor([1.7992e-05]),  b.grad: tensor([-3.4718e-05])\n",
      "a.grad: tensor([1.7693e-05]),  b.grad: tensor([-3.4219e-05])\n",
      "a.grad: tensor([1.7415e-05]),  b.grad: tensor([-3.3694e-05])\n",
      "a.grad: tensor([1.6961e-05]),  b.grad: tensor([-3.3285e-05])\n",
      "a.grad: tensor([1.6763e-05]),  b.grad: tensor([-3.2756e-05])\n",
      "a.grad: tensor([1.6480e-05]),  b.grad: tensor([-3.2284e-05])\n",
      "a.grad: tensor([1.6171e-05]),  b.grad: tensor([-3.1832e-05])\n",
      "a.grad: tensor([1.5910e-05]),  b.grad: tensor([-3.1346e-05])\n",
      "a.grad: tensor([1.5727e-05]),  b.grad: tensor([-3.0851e-05])\n",
      "a.grad: tensor([1.5564e-05]),  b.grad: tensor([-3.0346e-05])\n",
      "a.grad: tensor([1.5290e-05]),  b.grad: tensor([-2.9914e-05])\n",
      "a.grad: tensor([1.4984e-05]),  b.grad: tensor([-2.9495e-05])\n",
      "a.grad: tensor([1.4712e-05]),  b.grad: tensor([-2.9066e-05])\n",
      "a.grad: tensor([1.4538e-05]),  b.grad: tensor([-2.8613e-05])\n",
      "a.grad: tensor([1.4408e-05]),  b.grad: tensor([-2.8139e-05])\n",
      "a.grad: tensor([1.4248e-05]),  b.grad: tensor([-2.7681e-05])\n",
      "a.grad: tensor([1.3953e-05]),  b.grad: tensor([-2.7312e-05])\n",
      "a.grad: tensor([1.3675e-05]),  b.grad: tensor([-2.6926e-05])\n",
      "a.grad: tensor([1.3649e-05]),  b.grad: tensor([-2.6420e-05])\n",
      "a.grad: tensor([1.3521e-05]),  b.grad: tensor([-2.5995e-05])\n",
      "a.grad: tensor([1.3351e-05]),  b.grad: tensor([-2.5576e-05])\n",
      "a.grad: tensor([1.3142e-05]),  b.grad: tensor([-2.5200e-05])\n",
      "a.grad: tensor([1.2852e-05]),  b.grad: tensor([-2.4874e-05])\n",
      "a.grad: tensor([1.2607e-05]),  b.grad: tensor([-2.4519e-05])\n",
      "a.grad: tensor([1.2361e-05]),  b.grad: tensor([-2.4165e-05])\n",
      "a.grad: tensor([1.2212e-05]),  b.grad: tensor([-2.3796e-05])\n",
      "a.grad: tensor([1.2074e-05]),  b.grad: tensor([-2.3412e-05])\n",
      "a.grad: tensor([1.1940e-05]),  b.grad: tensor([-2.3022e-05])\n",
      "a.grad: tensor([1.1697e-05]),  b.grad: tensor([-2.2717e-05])\n",
      "a.grad: tensor([1.1452e-05]),  b.grad: tensor([-2.2410e-05])\n",
      "a.grad: tensor([1.1185e-05]),  b.grad: tensor([-2.2110e-05])\n",
      "a.grad: tensor([1.1190e-05]),  b.grad: tensor([-2.1685e-05])\n",
      "a.grad: tensor([1.1090e-05]),  b.grad: tensor([-2.1328e-05])\n",
      "a.grad: tensor([1.0995e-05]),  b.grad: tensor([-2.0965e-05])\n",
      "a.grad: tensor([1.0843e-05]),  b.grad: tensor([-2.0638e-05])\n",
      "a.grad: tensor([1.0596e-05]),  b.grad: tensor([-2.0378e-05])\n",
      "a.grad: tensor([1.0381e-05]),  b.grad: tensor([-2.0099e-05])\n",
      "a.grad: tensor([1.0169e-05]),  b.grad: tensor([-1.9810e-05])\n",
      "a.grad: tensor([9.9139e-06]),  b.grad: tensor([-1.9562e-05])\n",
      "a.grad: tensor([9.8259e-06]),  b.grad: tensor([-1.9239e-05])\n",
      "a.grad: tensor([9.7388e-06]),  b.grad: tensor([-1.8920e-05])\n",
      "a.grad: tensor([9.5954e-06]),  b.grad: tensor([-1.8628e-05])\n",
      "a.grad: tensor([9.5186e-06]),  b.grad: tensor([-1.8313e-05])\n",
      "a.grad: tensor([9.2844e-06]),  b.grad: tensor([-1.8095e-05])\n",
      "a.grad: tensor([9.0720e-06]),  b.grad: tensor([-1.7857e-05])\n",
      "a.grad: tensor([8.8690e-06]),  b.grad: tensor([-1.7605e-05])\n",
      "a.grad: tensor([8.8969e-06]),  b.grad: tensor([-1.7261e-05])\n",
      "a.grad: tensor([8.7656e-06]),  b.grad: tensor([-1.7018e-05])\n",
      "a.grad: tensor([8.7041e-06]),  b.grad: tensor([-1.6719e-05])\n",
      "a.grad: tensor([8.5849e-06]),  b.grad: tensor([-1.6477e-05])\n",
      "a.grad: tensor([8.4946e-06]),  b.grad: tensor([-1.6209e-05])\n",
      "a.grad: tensor([8.4154e-06]),  b.grad: tensor([-1.5922e-05])\n",
      "a.grad: tensor([8.2180e-06]),  b.grad: tensor([-1.5732e-05])\n",
      "a.grad: tensor([8.0187e-06]),  b.grad: tensor([-1.5523e-05])\n",
      "a.grad: tensor([7.8045e-06]),  b.grad: tensor([-1.5342e-05])\n",
      "a.grad: tensor([7.5679e-06]),  b.grad: tensor([-1.5164e-05])\n",
      "a.grad: tensor([7.6322e-06]),  b.grad: tensor([-1.4845e-05])\n",
      "a.grad: tensor([7.5605e-06]),  b.grad: tensor([-1.4611e-05])\n",
      "a.grad: tensor([7.4748e-06]),  b.grad: tensor([-1.4379e-05])\n",
      "a.grad: tensor([7.3779e-06]),  b.grad: tensor([-1.4175e-05])\n",
      "a.grad: tensor([7.3090e-06]),  b.grad: tensor([-1.3923e-05])\n",
      "a.grad: tensor([7.2317e-06]),  b.grad: tensor([-1.3692e-05])\n",
      "a.grad: tensor([7.0189e-06]),  b.grad: tensor([-1.3553e-05])\n",
      "a.grad: tensor([6.8340e-06]),  b.grad: tensor([-1.3394e-05])\n",
      "a.grad: tensor([6.6645e-06]),  b.grad: tensor([-1.3220e-05])\n",
      "a.grad: tensor([6.4904e-06]),  b.grad: tensor([-1.3053e-05])\n",
      "a.grad: tensor([6.4960e-06]),  b.grad: tensor([-1.2812e-05])\n",
      "a.grad: tensor([6.5668e-06]),  b.grad: tensor([-1.2529e-05])\n",
      "a.grad: tensor([6.3819e-06]),  b.grad: tensor([-1.2364e-05])\n",
      "a.grad: tensor([6.2883e-06]),  b.grad: tensor([-1.2195e-05])\n",
      "a.grad: tensor([6.2520e-06]),  b.grad: tensor([-1.1981e-05])\n",
      "a.grad: tensor([6.1919e-06]),  b.grad: tensor([-1.1794e-05])\n",
      "a.grad: tensor([6.1188e-06]),  b.grad: tensor([-1.1604e-05])\n",
      "a.grad: tensor([6.0387e-06]),  b.grad: tensor([-1.1412e-05])\n",
      "a.grad: tensor([5.9716e-06]),  b.grad: tensor([-1.1222e-05])\n",
      "a.grad: tensor([5.8031e-06]),  b.grad: tensor([-1.1110e-05])\n",
      "a.grad: tensor([5.6336e-06]),  b.grad: tensor([-1.0979e-05])\n",
      "a.grad: tensor([5.4603e-06]),  b.grad: tensor([-1.0857e-05])\n",
      "a.grad: tensor([5.2704e-06]),  b.grad: tensor([-1.0748e-05])\n",
      "a.grad: tensor([5.3230e-06]),  b.grad: tensor([-1.0535e-05])\n",
      "a.grad: tensor([5.3840e-06]),  b.grad: tensor([-1.0292e-05])\n",
      "a.grad: tensor([5.1949e-06]),  b.grad: tensor([-1.0185e-05])\n",
      "a.grad: tensor([5.2750e-06]),  b.grad: tensor([-9.9503e-06])\n",
      "a.grad: tensor([5.2582e-06]),  b.grad: tensor([-9.7628e-06])\n",
      "a.grad: tensor([5.1539e-06]),  b.grad: tensor([-9.6364e-06])\n",
      "a.grad: tensor([5.1362e-06]),  b.grad: tensor([-9.4608e-06])\n",
      "a.grad: tensor([5.0925e-06]),  b.grad: tensor([-9.3081e-06])\n",
      "a.grad: tensor([4.9807e-06]),  b.grad: tensor([-9.1949e-06])\n",
      "a.grad: tensor([4.9304e-06]),  b.grad: tensor([-9.0390e-06])\n",
      "a.grad: tensor([4.8950e-06]),  b.grad: tensor([-8.8778e-06])\n",
      "a.grad: tensor([4.7348e-06]),  b.grad: tensor([-8.7903e-06])\n",
      "a.grad: tensor([4.5747e-06]),  b.grad: tensor([-8.7037e-06])\n",
      "a.grad: tensor([4.4145e-06]),  b.grad: tensor([-8.6257e-06])\n",
      "a.grad: tensor([4.2357e-06]),  b.grad: tensor([-8.5547e-06])\n",
      "a.grad: tensor([4.0680e-06]),  b.grad: tensor([-8.4816e-06])\n",
      "a.grad: tensor([4.1481e-06]),  b.grad: tensor([-8.2841e-06])\n",
      "a.grad: tensor([4.2226e-06]),  b.grad: tensor([-8.0937e-06])\n",
      "a.grad: tensor([4.0773e-06]),  b.grad: tensor([-8.0033e-06])\n",
      "a.grad: tensor([4.1304e-06]),  b.grad: tensor([-7.8185e-06])\n",
      "a.grad: tensor([4.2040e-06]),  b.grad: tensor([-7.6303e-06])\n",
      "a.grad: tensor([3.9209e-06]),  b.grad: tensor([-7.6341e-06])\n",
      "a.grad: tensor([3.8939e-06]),  b.grad: tensor([-7.5102e-06])\n",
      "a.grad: tensor([3.8566e-06]),  b.grad: tensor([-7.3886e-06])\n",
      "a.grad: tensor([3.7849e-06]),  b.grad: tensor([-7.3011e-06])\n",
      "a.grad: tensor([3.7821e-06]),  b.grad: tensor([-7.1619e-06])\n",
      "a.grad: tensor([3.7216e-06]),  b.grad: tensor([-7.0529e-06])\n",
      "a.grad: tensor([3.6652e-06]),  b.grad: tensor([-6.9528e-06])\n",
      "a.grad: tensor([3.6694e-06]),  b.grad: tensor([-6.8131e-06])\n",
      "a.grad: tensor([3.6312e-06]),  b.grad: tensor([-6.6911e-06])\n",
      "a.grad: tensor([3.5805e-06]),  b.grad: tensor([-6.5877e-06])\n",
      "a.grad: tensor([3.5302e-06]),  b.grad: tensor([-6.4708e-06])\n",
      "a.grad: tensor([3.3751e-06]),  b.grad: tensor([-6.4364e-06])\n",
      "a.grad: tensor([3.2438e-06]),  b.grad: tensor([-6.3798e-06])\n",
      "a.grad: tensor([3.0673e-06]),  b.grad: tensor([-6.3649e-06])\n",
      "a.grad: tensor([2.9169e-06]),  b.grad: tensor([-6.3283e-06])\n",
      "a.grad: tensor([3.0063e-06]),  b.grad: tensor([-6.1644e-06])\n",
      "a.grad: tensor([2.8526e-06]),  b.grad: tensor([-6.1288e-06])\n",
      "a.grad: tensor([2.9271e-06]),  b.grad: tensor([-5.9761e-06])\n",
      "a.grad: tensor([3.0342e-06]),  b.grad: tensor([-5.8163e-06])\n",
      "a.grad: tensor([2.9062e-06]),  b.grad: tensor([-5.7574e-06])\n",
      "a.grad: tensor([2.9374e-06]),  b.grad: tensor([-5.6359e-06])\n",
      "a.grad: tensor([3.0212e-06]),  b.grad: tensor([-5.4925e-06])\n",
      "a.grad: tensor([2.9062e-06]),  b.grad: tensor([-5.4231e-06])\n",
      "a.grad: tensor([2.9700e-06]),  b.grad: tensor([-5.2867e-06])\n",
      "a.grad: tensor([2.9458e-06]),  b.grad: tensor([-5.2082e-06])\n",
      "a.grad: tensor([2.9090e-06]),  b.grad: tensor([-5.1397e-06])\n",
      "a.grad: tensor([2.9271e-06]),  b.grad: tensor([-5.0352e-06])\n",
      "a.grad: tensor([2.8824e-06]),  b.grad: tensor([-4.9761e-06])\n",
      "a.grad: tensor([2.8471e-06]),  b.grad: tensor([-4.8955e-06])\n",
      "a.grad: tensor([2.8368e-06]),  b.grad: tensor([-4.8168e-06])\n",
      "a.grad: tensor([2.7912e-06]),  b.grad: tensor([-4.7437e-06])\n",
      "a.grad: tensor([2.7595e-06]),  b.grad: tensor([-4.6727e-06])\n",
      "a.grad: tensor([2.7707e-06]),  b.grad: tensor([-4.5723e-06])\n",
      "a.grad: tensor([2.7129e-06]),  b.grad: tensor([-4.5155e-06])\n",
      "a.grad: tensor([2.6934e-06]),  b.grad: tensor([-4.4457e-06])\n",
      "a.grad: tensor([2.6664e-06]),  b.grad: tensor([-4.3549e-06])\n",
      "a.grad: tensor([2.6403e-06]),  b.grad: tensor([-4.2808e-06])\n",
      "a.grad: tensor([2.6189e-06]),  b.grad: tensor([-4.1961e-06])\n",
      "a.grad: tensor([2.5863e-06]),  b.grad: tensor([-4.1244e-06])\n",
      "a.grad: tensor([2.4531e-06]),  b.grad: tensor([-4.1206e-06])\n",
      "a.grad: tensor([2.3218e-06]),  b.grad: tensor([-4.1244e-06])\n",
      "a.grad: tensor([2.1812e-06]),  b.grad: tensor([-4.1162e-06])\n",
      "a.grad: tensor([2.0419e-06]),  b.grad: tensor([-4.1134e-06])\n",
      "a.grad: tensor([1.9101e-06]),  b.grad: tensor([-4.1104e-06])\n",
      "a.grad: tensor([1.7472e-06]),  b.grad: tensor([-4.1234e-06])\n",
      "a.grad: tensor([1.8640e-06]),  b.grad: tensor([-3.9954e-06])\n",
      "a.grad: tensor([1.7462e-06]),  b.grad: tensor([-3.9786e-06])\n",
      "a.grad: tensor([1.7886e-06]),  b.grad: tensor([-3.9116e-06])\n",
      "a.grad: tensor([1.6550e-06]),  b.grad: tensor([-3.9032e-06])\n",
      "a.grad: tensor([1.7961e-06]),  b.grad: tensor([-3.7518e-06])\n",
      "a.grad: tensor([1.6307e-06]),  b.grad: tensor([-3.7816e-06])\n",
      "a.grad: tensor([1.7220e-06]),  b.grad: tensor([-3.6587e-06])\n",
      "a.grad: tensor([1.8515e-06]),  b.grad: tensor([-3.5283e-06])\n",
      "a.grad: tensor([1.7015e-06]),  b.grad: tensor([-3.5372e-06])\n",
      "a.grad: tensor([1.8058e-06]),  b.grad: tensor([-3.4194e-06])\n",
      "a.grad: tensor([1.6391e-06]),  b.grad: tensor([-3.4361e-06])\n",
      "a.grad: tensor([1.7486e-06]),  b.grad: tensor([-3.3178e-06])\n",
      "a.grad: tensor([1.8612e-06]),  b.grad: tensor([-3.1949e-06])\n",
      "a.grad: tensor([1.7034e-06]),  b.grad: tensor([-3.2072e-06])\n",
      "a.grad: tensor([1.8179e-06]),  b.grad: tensor([-3.0810e-06])\n",
      "a.grad: tensor([1.6848e-06]),  b.grad: tensor([-3.0710e-06])\n",
      "a.grad: tensor([1.7881e-06]),  b.grad: tensor([-2.9621e-06])\n",
      "a.grad: tensor([1.5330e-06]),  b.grad: tensor([-3.0368e-06])\n",
      "a.grad: tensor([1.6419e-06]),  b.grad: tensor([-2.9120e-06])\n",
      "a.grad: tensor([1.6047e-06]),  b.grad: tensor([-2.8966e-06])\n",
      "a.grad: tensor([1.5721e-06]),  b.grad: tensor([-2.8734e-06])\n",
      "a.grad: tensor([1.6280e-06]),  b.grad: tensor([-2.7746e-06])\n",
      "a.grad: tensor([1.5995e-06]),  b.grad: tensor([-2.7441e-06])\n",
      "a.grad: tensor([1.5688e-06]),  b.grad: tensor([-2.7178e-06])\n",
      "a.grad: tensor([1.5292e-06]),  b.grad: tensor([-2.7108e-06])\n",
      "a.grad: tensor([1.5148e-06]),  b.grad: tensor([-2.6741e-06])\n",
      "a.grad: tensor([1.5334e-06]),  b.grad: tensor([-2.6140e-06])\n",
      "a.grad: tensor([1.5250e-06]),  b.grad: tensor([-2.5670e-06])\n",
      "a.grad: tensor([1.5134e-06]),  b.grad: tensor([-2.5269e-06])\n",
      "a.grad: tensor([1.4771e-06]),  b.grad: tensor([-2.5083e-06])\n",
      "a.grad: tensor([1.5050e-06]),  b.grad: tensor([-2.4401e-06])\n",
      "a.grad: tensor([1.4761e-06]),  b.grad: tensor([-2.4112e-06])\n",
      "a.grad: tensor([1.4724e-06]),  b.grad: tensor([-2.3642e-06])\n",
      "a.grad: tensor([1.4352e-06]),  b.grad: tensor([-2.3390e-06])\n",
      "a.grad: tensor([1.4161e-06]),  b.grad: tensor([-2.3087e-06])\n",
      "a.grad: tensor([1.4072e-06]),  b.grad: tensor([-2.2666e-06])\n",
      "a.grad: tensor([1.3839e-06]),  b.grad: tensor([-2.2347e-06])\n",
      "a.grad: tensor([1.3709e-06]),  b.grad: tensor([-2.1968e-06])\n",
      "a.grad: tensor([1.3579e-06]),  b.grad: tensor([-2.1590e-06])\n",
      "a.grad: tensor([1.3672e-06]),  b.grad: tensor([-2.1036e-06])\n",
      "a.grad: tensor([1.3448e-06]),  b.grad: tensor([-2.0689e-06])\n",
      "a.grad: tensor([1.3271e-06]),  b.grad: tensor([-2.0352e-06])\n",
      "a.grad: tensor([1.2871e-06]),  b.grad: tensor([-2.0219e-06])\n",
      "a.grad: tensor([1.3132e-06]),  b.grad: tensor([-1.9546e-06])\n",
      "a.grad: tensor([1.2787e-06]),  b.grad: tensor([-1.9302e-06])\n",
      "a.grad: tensor([1.2573e-06]),  b.grad: tensor([-1.8957e-06])\n",
      "a.grad: tensor([1.2433e-06]),  b.grad: tensor([-1.8571e-06])\n",
      "a.grad: tensor([1.2196e-06]),  b.grad: tensor([-1.8277e-06])\n",
      "a.grad: tensor([1.2149e-06]),  b.grad: tensor([-1.7839e-06])\n",
      "a.grad: tensor([1.1073e-06]),  b.grad: tensor([-1.8110e-06])\n",
      "a.grad: tensor([1.0990e-06]),  b.grad: tensor([-1.7639e-06])\n",
      "a.grad: tensor([9.7509e-07]),  b.grad: tensor([-1.7984e-06])\n",
      "a.grad: tensor([9.7742e-07]),  b.grad: tensor([-1.7588e-06])\n",
      "a.grad: tensor([8.0653e-07]),  b.grad: tensor([-1.8226e-06])\n",
      "a.grad: tensor([8.1677e-07]),  b.grad: tensor([-1.7639e-06])\n",
      "a.grad: tensor([6.8732e-07]),  b.grad: tensor([-1.8040e-06])\n",
      "a.grad: tensor([6.6869e-07]),  b.grad: tensor([-1.7700e-06])\n",
      "a.grad: tensor([5.7090e-07]),  b.grad: tensor([-1.7958e-06])\n",
      "a.grad: tensor([7.9349e-07]),  b.grad: tensor([-1.6377e-06])\n",
      "a.grad: tensor([6.8545e-07]),  b.grad: tensor([-1.6615e-06])\n",
      "a.grad: tensor([5.5367e-07]),  b.grad: tensor([-1.7004e-06])\n",
      "a.grad: tensor([6.2864e-07]),  b.grad: tensor([-1.6503e-06])\n",
      "a.grad: tensor([5.2480e-07]),  b.grad: tensor([-1.6752e-06])\n",
      "a.grad: tensor([6.0443e-07]),  b.grad: tensor([-1.6293e-06])\n",
      "a.grad: tensor([5.0245e-07]),  b.grad: tensor([-1.6543e-06])\n",
      "a.grad: tensor([5.9791e-07]),  b.grad: tensor([-1.5893e-06])\n",
      "a.grad: tensor([4.6007e-07]),  b.grad: tensor([-1.6298e-06])\n",
      "a.grad: tensor([6.1467e-07]),  b.grad: tensor([-1.5278e-06])\n",
      "a.grad: tensor([4.5588e-07]),  b.grad: tensor([-1.5809e-06])\n",
      "a.grad: tensor([5.9651e-07]),  b.grad: tensor([-1.4883e-06])\n",
      "a.grad: tensor([4.4564e-07]),  b.grad: tensor([-1.5348e-06])\n",
      "a.grad: tensor([5.7835e-07]),  b.grad: tensor([-1.4510e-06])\n",
      "a.grad: tensor([7.1572e-07]),  b.grad: tensor([-1.3693e-06])\n",
      "a.grad: tensor([5.7556e-07]),  b.grad: tensor([-1.4133e-06])\n",
      "a.grad: tensor([6.8732e-07]),  b.grad: tensor([-1.3316e-06])\n",
      "a.grad: tensor([5.7463e-07]),  b.grad: tensor([-1.3630e-06])\n",
      "a.grad: tensor([6.6590e-07]),  b.grad: tensor([-1.3034e-06])\n",
      "a.grad: tensor([5.4063e-07]),  b.grad: tensor([-1.3341e-06])\n",
      "a.grad: tensor([6.3702e-07]),  b.grad: tensor([-1.2727e-06])\n",
      "a.grad: tensor([5.2061e-07]),  b.grad: tensor([-1.2992e-06])\n",
      "a.grad: tensor([6.1840e-07]),  b.grad: tensor([-1.2354e-06])\n",
      "a.grad: tensor([5.0012e-07]),  b.grad: tensor([-1.2638e-06])\n",
      "a.grad: tensor([6.1281e-07]),  b.grad: tensor([-1.1921e-06])\n",
      "a.grad: tensor([4.8708e-07]),  b.grad: tensor([-1.2342e-06])\n",
      "a.grad: tensor([6.0024e-07]),  b.grad: tensor([-1.1525e-06])\n",
      "a.grad: tensor([4.8056e-07]),  b.grad: tensor([-1.1898e-06])\n",
      "a.grad: tensor([5.9884e-07]),  b.grad: tensor([-1.1041e-06])\n",
      "a.grad: tensor([4.7404e-07]),  b.grad: tensor([-1.1437e-06])\n",
      "a.grad: tensor([5.8813e-07]),  b.grad: tensor([-1.0631e-06])\n",
      "a.grad: tensor([6.9570e-07]),  b.grad: tensor([-1.0040e-06])\n",
      "a.grad: tensor([5.5507e-07]),  b.grad: tensor([-1.0435e-06])\n",
      "a.grad: tensor([6.6217e-07]),  b.grad: tensor([-9.6997e-07])\n",
      "a.grad: tensor([5.3365e-07]),  b.grad: tensor([-1.0058e-06])\n",
      "a.grad: tensor([6.4354e-07]),  b.grad: tensor([-9.3086e-07])\n",
      "a.grad: tensor([5.2573e-07]),  b.grad: tensor([-9.6788e-07])\n",
      "a.grad: tensor([6.5658e-07]),  b.grad: tensor([-8.7451e-07])\n",
      "a.grad: tensor([5.4901e-07]),  b.grad: tensor([-9.0571e-07])\n",
      "a.grad: tensor([6.3889e-07]),  b.grad: tensor([-8.4029e-07])\n",
      "a.grad: tensor([5.0990e-07]),  b.grad: tensor([-8.8336e-07])\n",
      "a.grad: tensor([6.2678e-07]),  b.grad: tensor([-8.0187e-07])\n",
      "a.grad: tensor([5.2666e-07]),  b.grad: tensor([-8.2375e-07])\n",
      "a.grad: tensor([6.2026e-07]),  b.grad: tensor([-7.6601e-07])\n",
      "a.grad: tensor([4.8056e-07]),  b.grad: tensor([-8.0629e-07])\n",
      "a.grad: tensor([5.8440e-07]),  b.grad: tensor([-7.2992e-07])\n",
      "a.grad: tensor([7.1572e-07]),  b.grad: tensor([-6.4168e-07])\n",
      "a.grad: tensor([5.9046e-07]),  b.grad: tensor([-6.8313e-07])\n",
      "a.grad: tensor([6.7102e-07]),  b.grad: tensor([-6.2911e-07])\n",
      "a.grad: tensor([5.8115e-07]),  b.grad: tensor([-6.5123e-07])\n",
      "a.grad: tensor([6.8545e-07]),  b.grad: tensor([-5.7556e-07])\n",
      "a.grad: tensor([4.5449e-07]),  b.grad: tensor([-6.8825e-07])\n",
      "a.grad: tensor([5.7463e-07]),  b.grad: tensor([-6.0257e-07])\n",
      "a.grad: tensor([6.7707e-07]),  b.grad: tensor([-5.3365e-07])\n",
      "a.grad: tensor([4.3865e-07]),  b.grad: tensor([-6.4494e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a.grad: tensor([5.4855e-07]),  b.grad: tensor([-5.7323e-07])\n",
      "a: tensor([1.0235], requires_grad=True),  b: tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(f'a: {a},  a.shape: {a.shape}\\nb: {b},  b.shape: {b.shape}\\n')\n",
    "\n",
    "for epach in range(n_epochs):\n",
    "  yhat = a + b*x_train_tensor\n",
    "  error = y_train_tensor - yhat\n",
    "  loss = (error**2).mean()\n",
    "  loss.backward()  # no more manual computation of gradients\n",
    "  print(f'a.grad: {a.grad},  b.grad: {b.grad}')\n",
    "\n",
    "  # Use no_grad to keep the update out of the gradient computation\n",
    "  with torch.no_grad():\n",
    "    a += -lr*a.grad\n",
    "    b += -lr*b.grad\n",
    "\n",
    "  a.grad.zero_()\n",
    "  b.grad.zero_()\n",
    "\n",
    "print(f'a: {a},  b: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "yhat = a + b*x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"247pt\"\n",
       " viewBox=\"0.00 0.00 222.00 247.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 243)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-243 218,-243 218,4 -4,4\"/>\n",
       "<!-- 1974956490480 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1974956490480</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"139,-19 74,-19 74,0 139,0 139,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (80, 1)</text>\n",
       "</g>\n",
       "<!-- 1974849437312 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1974849437312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-74 62,-74 62,-55 151,-55 151,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1974849437312&#45;&gt;1974956490480 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1974849437312&#45;&gt;1974956490480</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-54.75C106.5,-47.8 106.5,-37.85 106.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-29.09 106.5,-19.09 103,-29.09 110,-29.09\"/>\n",
       "</g>\n",
       "<!-- 1974849436688 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1974849436688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-129 0,-129 0,-110 101,-110 101,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1974849436688&#45;&gt;1974849437312 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1974849436688&#45;&gt;1974849437312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-109.98C67.69,-102.23 80.01,-90.58 89.97,-81.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-83.59 97.34,-74.17 87.67,-78.5 92.48,-83.59\"/>\n",
       "</g>\n",
       "<!-- 1974953257936 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1974953257936</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-184 23.5,-184 23.5,-165 77.5,-165 77.5,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1974953257936&#45;&gt;1974849436688 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1974953257936&#45;&gt;1974849436688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-164.75C50.5,-157.8 50.5,-147.85 50.5,-139.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-139.09 50.5,-129.09 47,-139.09 54,-139.09\"/>\n",
       "</g>\n",
       "<!-- 1974849436640 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1974849436640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-129 119,-129 119,-110 208,-110 208,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1974849436640&#45;&gt;1974849437312 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1974849436640&#45;&gt;1974849437312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-109.98C146,-102.23 133.47,-90.58 123.32,-81.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-78.42 115.82,-74.17 120.76,-83.54 125.53,-78.42\"/>\n",
       "</g>\n",
       "<!-- 1974849437552 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1974849437552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-184 113,-184 113,-165 214,-165 214,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1974849437552&#45;&gt;1974849436640 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1974849437552&#45;&gt;1974849436640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-164.75C163.5,-157.8 163.5,-147.85 163.5,-139.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-139.09 163.5,-129.09 160,-139.09 167,-139.09\"/>\n",
       "</g>\n",
       "<!-- 1974953257136 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1974953257136</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-239 136.5,-239 136.5,-220 190.5,-220 190.5,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1974953257136&#45;&gt;1974849437552 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1974953257136&#45;&gt;1974849437552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-219.75C163.5,-212.8 163.5,-202.85 163.5,-194.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-194.09 163.5,-184.09 160,-194.09 167,-194.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1cbce32fd00>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 222.00 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-298 218,-298 218,4 -4,4\"/>\n",
       "<!-- 1974956490000 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1974956490000</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"139,-19 74,-19 74,0 139,0 139,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (80, 1)</text>\n",
       "</g>\n",
       "<!-- 1974714900096 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1974714900096</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-74 62,-74 62,-55 151,-55 151,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 1974714900096&#45;&gt;1974956490000 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1974714900096&#45;&gt;1974956490000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-54.75C106.5,-47.8 106.5,-37.85 106.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-29.09 106.5,-19.09 103,-29.09 110,-29.09\"/>\n",
       "</g>\n",
       "<!-- 1974849437312 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1974849437312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-129 62,-129 62,-110 151,-110 151,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1974849437312&#45;&gt;1974714900096 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1974849437312&#45;&gt;1974714900096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-109.75C106.5,-102.8 106.5,-92.85 106.5,-84.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-84.09 106.5,-74.09 103,-84.09 110,-84.09\"/>\n",
       "</g>\n",
       "<!-- 1974849436688 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1974849436688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-184 0,-184 0,-165 101,-165 101,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1974849436688&#45;&gt;1974849437312 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1974849436688&#45;&gt;1974849437312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-164.98C67.69,-157.23 80.01,-145.58 89.97,-136.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-138.59 97.34,-129.17 87.67,-133.5 92.48,-138.59\"/>\n",
       "</g>\n",
       "<!-- 1974953257936 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1974953257936</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-239 23.5,-239 23.5,-220 77.5,-220 77.5,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1974953257936&#45;&gt;1974849436688 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1974953257936&#45;&gt;1974849436688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-219.75C50.5,-212.8 50.5,-202.85 50.5,-194.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-194.09 50.5,-184.09 47,-194.09 54,-194.09\"/>\n",
       "</g>\n",
       "<!-- 1974849436640 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1974849436640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-184 119,-184 119,-165 208,-165 208,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1974849436640&#45;&gt;1974849437312 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1974849436640&#45;&gt;1974849437312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-164.98C146,-157.23 133.47,-145.58 123.32,-136.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-133.42 115.82,-129.17 120.76,-138.54 125.53,-133.42\"/>\n",
       "</g>\n",
       "<!-- 1974849437552 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1974849437552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-239 113,-239 113,-220 214,-220 214,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1974849437552&#45;&gt;1974849436640 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1974849437552&#45;&gt;1974849436640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-219.75C163.5,-212.8 163.5,-202.85 163.5,-194.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-194.09 163.5,-184.09 160,-194.09 167,-194.09\"/>\n",
       "</g>\n",
       "<!-- 1974953257136 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1974953257136</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-294 136.5,-294 136.5,-275 190.5,-275 190.5,-294\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-282\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1974953257136&#45;&gt;1974849437552 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1974953257136&#45;&gt;1974849437552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-274.75C163.5,-267.8 163.5,-257.85 163.5,-249.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-249.09 163.5,-239.09 160,-249.09 167,-249.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1cbb9890b20>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"222pt\" height=\"412pt\"\n",
       " viewBox=\"0.00 0.00 222.00 412.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 408)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-408 218,-408 218,4 -4,4\"/>\n",
       "<!-- 1974946349120 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1974946349120</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"133.5,-19 79.5,-19 79.5,0 133.5,0 133.5,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 1974985065184 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1974985065184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-74 59,-74 59,-55 154,-55 154,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 1974985065184&#45;&gt;1974946349120 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1974985065184&#45;&gt;1974946349120</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-54.75C106.5,-47.8 106.5,-37.85 106.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-29.09 106.5,-19.09 103,-29.09 110,-29.09\"/>\n",
       "</g>\n",
       "<!-- 1974985064560 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1974985064560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-129 62,-129 62,-110 151,-110 151,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 1974985064560&#45;&gt;1974985065184 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1974985064560&#45;&gt;1974985065184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-109.75C106.5,-102.8 106.5,-92.85 106.5,-84.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-84.09 106.5,-74.09 103,-84.09 110,-84.09\"/>\n",
       "</g>\n",
       "<!-- 1974714900096 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1974714900096</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-184 62,-184 62,-165 151,-165 151,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 1974714900096&#45;&gt;1974985064560 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1974714900096&#45;&gt;1974985064560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-164.75C106.5,-157.8 106.5,-147.85 106.5,-139.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-139.09 106.5,-129.09 103,-139.09 110,-139.09\"/>\n",
       "</g>\n",
       "<!-- 1974849437312 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1974849437312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-239 62,-239 62,-220 151,-220 151,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1974849437312&#45;&gt;1974714900096 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1974849437312&#45;&gt;1974714900096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.5,-219.75C106.5,-212.8 106.5,-202.85 106.5,-194.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110,-194.09 106.5,-184.09 103,-194.09 110,-194.09\"/>\n",
       "</g>\n",
       "<!-- 1974849436688 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1974849436688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-294 0,-294 0,-275 101,-275 101,-294\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-282\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1974849436688&#45;&gt;1974849437312 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1974849436688&#45;&gt;1974849437312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.5,-274.98C67.69,-267.23 80.01,-255.58 89.97,-246.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-248.59 97.34,-239.17 87.67,-243.5 92.48,-248.59\"/>\n",
       "</g>\n",
       "<!-- 1974953257936 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1974953257936</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-349 23.5,-349 23.5,-330 77.5,-330 77.5,-349\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-337\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1974953257936&#45;&gt;1974849436688 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1974953257936&#45;&gt;1974849436688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-329.75C50.5,-322.8 50.5,-312.85 50.5,-304.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-304.09 50.5,-294.09 47,-304.09 54,-304.09\"/>\n",
       "</g>\n",
       "<!-- 1974849436640 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1974849436640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-294 119,-294 119,-275 208,-275 208,-294\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-282\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1974849436640&#45;&gt;1974849437312 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1974849436640&#45;&gt;1974849437312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.34,-274.98C146,-267.23 133.47,-255.58 123.32,-246.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-243.42 115.82,-239.17 120.76,-248.54 125.53,-243.42\"/>\n",
       "</g>\n",
       "<!-- 1974849437552 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>1974849437552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-349 113,-349 113,-330 214,-330 214,-349\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-337\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1974849437552&#45;&gt;1974849436640 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1974849437552&#45;&gt;1974849436640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-329.75C163.5,-322.8 163.5,-312.85 163.5,-304.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-304.09 163.5,-294.09 160,-304.09 167,-304.09\"/>\n",
       "</g>\n",
       "<!-- 1974953257136 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>1974953257136</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-404 136.5,-404 136.5,-385 190.5,-385 190.5,-404\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-392\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1974953257136&#45;&gt;1974849437552 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>1974953257136&#45;&gt;1974849437552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.5,-384.75C163.5,-377.8 163.5,-367.85 163.5,-359.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167,-359.09 163.5,-349.09 160,-359.09 167,-359.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1cbd6488280>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_nograd: no gradients, no graph\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"247pt\"\n",
       " viewBox=\"0.00 0.00 109.00 247.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 243)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-243 105,-243 105,4 -4,4\"/>\n",
       "<!-- 1974958284128 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1974958284128</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"83,-19 18,-19 18,0 83,0 83,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (80, 1)</text>\n",
       "</g>\n",
       "<!-- 1974985065472 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1974985065472</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-74 6,-74 6,-55 95,-55 95,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1974985065472&#45;&gt;1974958284128 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1974985065472&#45;&gt;1974958284128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-54.75C50.5,-47.8 50.5,-37.85 50.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-29.09 50.5,-19.09 47,-29.09 54,-29.09\"/>\n",
       "</g>\n",
       "<!-- 1974985066912 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1974985066912</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-129 6,-129 6,-110 95,-110 95,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1974985066912&#45;&gt;1974985065472 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1974985066912&#45;&gt;1974985065472</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-109.75C50.5,-102.8 50.5,-92.85 50.5,-84.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-84.09 50.5,-74.09 47,-84.09 54,-84.09\"/>\n",
       "</g>\n",
       "<!-- 1974985064512 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1974985064512</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-184 0,-184 0,-165 101,-165 101,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1974985064512&#45;&gt;1974985066912 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1974985064512&#45;&gt;1974985066912</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-164.75C50.5,-157.8 50.5,-147.85 50.5,-139.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-139.09 50.5,-129.09 47,-139.09 54,-139.09\"/>\n",
       "</g>\n",
       "<!-- 1974957196048 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1974957196048</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-239 23.5,-239 23.5,-220 77.5,-220 77.5,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1974957196048&#45;&gt;1974985064512 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1974957196048&#45;&gt;1974985064512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-219.75C50.5,-212.8 50.5,-202.85 50.5,-194.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-194.09 50.5,-184.09 47,-194.09 54,-194.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1cbd6488130>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_nograd = torch.randn(1, requires_grad=False, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "yhat = a_nograd + b*x_train_tensor\n",
    "print(f'a_nograd: no gradients, no graph')\n",
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-sense code to demo control flow statements\n",
    "yhat = a + b*x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error**2).mean()\n",
    "if loss > 0:\n",
    "  yhat2 = b*x_train_tensor\n",
    "  error2 = y_train_tensor - yhat2\n",
    "loss += error2.mean()\n",
    "make_dot(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a and b before optimizing\n",
      "a: tensor([0.3367], requires_grad=True),  b: tensor([0.1288], requires_grad=True)\n",
      "\n",
      "Peform batch gradient descent since x_train_tensor has all the data\n",
      "len(x_train_tensor): 80\n",
      "\n",
      "a and b after optimizing\n",
      "a: tensor([1.0235], requires_grad=True),  b: tensor([1.9690], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Use SGD optimizer to update parameters\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print('a and b before optimizing')\n",
    "print(f'a: {a},  b: {b}\\n')\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "print('Peform batch gradient descent since x_train_tensor has all the data')\n",
    "print(f'len(x_train_tensor): {len(x_train_tensor)}\\n')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  yhat = a + b*x_train_tensor\n",
    "  error = y_train_tensor - yhat\n",
    "  loss = (error**2).mean()\n",
    "\n",
    "  loss.backward()  # calculate gradients\n",
    "\n",
    "  optimizer.step()  # update parameters\n",
    "  #######################################\n",
    "  # # Use no_grad to keep the update out of the gradient computation\n",
    "  # with torch.no_grad():\n",
    "  #   a += -lr*a.grad\n",
    "  #   b += -lr*b.grad\n",
    "  #######################################\n",
    "\n",
    "  optimizer.zero_grad()  # zero out gradients\n",
    "  #######################################  \n",
    "  # a.grad.zero_()\n",
    "  # b.grad.zero_()\n",
    "  #######################################  \n",
    "\n",
    "print('a and b after optimizing')\n",
    "print(f'a: {a},  b: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pytorch loss functions\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print('a and b before optimizing')\n",
    "print(f'a: {a},  b: {b}\\n')\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Use MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Use SGD optimizer to update parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "print('Peform batch gradient descent, matrix multiplication with x_train_tensor process all the data at onnce')\n",
    "print(f'len(x_train_tensor): {len(x_train_tensor)}\\n')\n",
    "\n",
    "for epoch in range(n_epochs):  # all training data is processed in an epoch\n",
    "  yhat = a + b*x_train_tensor\n",
    "  error = y_train_tensor - yhat\n",
    "\n",
    "  # loss = (error**2).mean()\n",
    "  loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "  loss.backward()  # calculate gradients\n",
    "  optimizer.step()  # update parameters\n",
    "  optimizer.zero_grad()  # zero out gradients\n",
    "\n",
    "print('a and b after optimizing')\n",
    "print(f'a: {a},  b: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple model\n",
    "class ManualLinearRegression(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Wrap \"a\" and \"b\" in nn.Parameter to make them model parameters\n",
    "    # Allow model's parameters() method to retrieve an iterator for model's parameters\n",
    "    # Get current values for all parameters with state_dict() method \n",
    "    self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "    self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Compute predictions\n",
    "    return self.a + self.b*x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create model and send it to device\n",
    "model = ManualLinearRegression().to(device=device)\n",
    "# Alternate model using pytorch vs writing our own class\n",
    "# model = nn.Sequential(nn.Linear(in_features=1, out_features=1)).to(device=device)\n",
    "\n",
    "# Print parameters\n",
    "print(f'model.state_dict():\\n {model.state_dict()}\\n')\n",
    "\n",
    "\n",
    "# a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "# b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "# print('a and b before optimizing')\n",
    "# print(f'a: {a},  b: {b}\\n')\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Use MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Use SGD optimizer to update parameters\n",
    "# optimizer = optim.SGD([a, b], lr=lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "print('Peform batch gradient descent, matrix multiplication with x_train_tensor process all the data at onnce')\n",
    "print(f'len(x_train_tensor): {len(x_train_tensor)}\\n')\n",
    "\n",
    "for epoch in range(n_epochs):  # all training data is processed in an epoch\n",
    "  # yhat = a + b*x_train_tensor\n",
    "  # error = y_train_tensor - yhat\n",
    "  model.train()  # set model to training mode vs other mode like Dropout\n",
    "\n",
    "  # No more manual predictions!\n",
    "  # yhat = a + b*x_train_tensor\n",
    "  yhat = model(x_train_tensor)\n",
    "\n",
    "  # loss = (error**2).mean()\n",
    "  loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "  loss.backward()  # calculate gradients\n",
    "  optimizer.step()  # update parameters\n",
    "  optimizer.zero_grad()  # zero out gradients\n",
    "\n",
    "# print('a and b after optimizing')\n",
    "# print(f'a: {a},  b: {b}')\n",
    "print(f'model.state_dict():\\n{model.state_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a nested model\n",
    "class LayerLinearRegression(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Wrap \"a\" and \"b\" in nn.Parameter to make them model parameters\n",
    "    # Allow model's parameters() method to retrieve an iterator for model's parameters\n",
    "    # Get current values for all parameters with state_dict() method \n",
    "    self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "    self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Compute predictions\n",
    "    # return self.a + self.b*x\n",
    "    #\n",
    "    # Change to taking a call to the layer to make predictions      \n",
    "    return self.linear(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "  # Build function that performs a step in the training loop\n",
    "  def train_step(x, y):\n",
    "    # Set model to TRAIN mode\n",
    "    model.train()\n",
    "    # Make predictions\n",
    "    yhat = model(x)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y, yhat)\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    # Update parameters and zeroes gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # Return the loss\n",
    "    return loss.item()\n",
    "\n",
    "  # Return the function that will be called inside the training loop\n",
    "  return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # Perform one train step and return loss\n",
    "  loss = train_step(x_train_tensor, y_train_tensor)\n",
    "  losses.append(loss)\n",
    "\n",
    "# Check model's parameters\n",
    "print(f'model.state_dict():\\n{model.state_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple dataset\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "class CustomerDataset(Dataset):\n",
    "  def __init__(self, x_tensor, y_tensor):\n",
    "    self.x = x_tensor\n",
    "    self.y = y_tensor\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return (self.x[index], self.y[index])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this a CPU tensor? Where is .to(device)?\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomerDataset(x_train_tensor, y_train_tensor)\n",
    "print(f'train_data[0]: {train_data[0]}')\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(f'train_data[0]: {train_data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_data:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a sample mini-batch\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataloader to load mini-batch\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # # Perform one train step and return loss\n",
    "  # loss = train_step(x_train_tensor, y_train_tensor)\n",
    "  # losses.append(loss)\n",
    "\n",
    "  # For bigger datasets, loading data sample by sample (into a CPU tensor)\n",
    "  # using Dataset’s __get_item__ and then sending all samples that belong\n",
    "  # to the same mini-batch at once to your GPU (device) is the way to go\n",
    "  # in order to make the best use of your graphics card’s RAM.\n",
    "\n",
    "  for x_batch, y_batch in train_loader:\n",
    "    # the dataset \"lives\" in the CPU, so do out mini-batches\n",
    "    # we need to send those mini-batches to the device where\n",
    "    # the model \"lives\"\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)    \n",
    "    loss = train_step(x_batch, y_batch)\n",
    "    losses.append(loss)\n",
    "\n",
    "# Check model's parameters\n",
    "print(f'model.state_dict():\\n{model.state_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split\n",
    "from torch.utils.data.dataset import random_split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_data, val_data = random_split(dataset, [80, 20])\n",
    "\n",
    "# val_loader results\n",
    "# dataset items = 2, => returns 2 tensors\n",
    "# val_data = 10      => each tensor has 10 values total\n",
    "# batch_size = 3     => returns 2 tensors, each with 3 values (last batch with 1 values)\n",
    "# [tensor([[0.1987], [0.7722], [0.5248]]),\n",
    "#  tensor([[1.2654], [2.4208], [2.0167]])]\n",
    "# [tensor([[0.1818], [0.1834], [0.7069]]),\n",
    "#  tensor([[1.3734], [1.4637], [2.4388]])]\n",
    "# [tensor([[0.8155], [0.0977], [0.1079]]),\n",
    "#  tensor([[2.6606], [1.4417], [1.2985]])]\n",
    "# [tensor([[0.5201]]), tensor([[1.9000]])]\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, batch in enumerate(val_loader):\n",
    "  print(f'batch: {j}')  \n",
    "  for i, tensor in enumerate(batch):\n",
    "    print(f'tensor: {i}, tensor.shape: {tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in val_loader:\n",
    "  print(f'{item}\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with shape (4, 3, 2) and all elements set to 0\n",
    "tensor = torch.zeros((4, 3, 2))\n",
    "# Print the tensor\n",
    "print(tensor.shape)\n",
    "# (4,3,2) tenssor: 4 x 3 matrix, each cell with 2 values\n",
    "# [[[0., 0.], [0., 0.], [0., 0.]],\n",
    "#  [[0., 0.], [0., 0.], [0., 0.]],\n",
    "#  [[0., 0.], [0., 0.], [0., 0.]],\n",
    "#  [[0., 0.], [0., 0.], [0., 0.]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "losses = []\n",
    "val_losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  for x_batch, y_batch in train_loader:\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "    losses.append(loss)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for x_val, y_val in val_loader:\n",
    "      x_val = x_val.to(device)\n",
    "      y_val = y_val.to(device)\n",
    "\n",
    "      model.eval()\n",
    "\n",
    "      yhat = model(x_val)\n",
    "      val_loss = loss_fn(y_val, yhat)\n",
    "      val_losses.append(val_loss)\n",
    "\n",
    "# Check model's parameters\n",
    "print(f'model.state_dict():\\n{model.state_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(train_data): {len(train_data)}')\n",
    "print(f'len(train_loader): {len(train_loader)}')\n",
    "print(f'n_epochs: {n_epochs}')\n",
    "print(f'len(losses): {len(losses)}')\n",
    "print(f'len(val_losses): {len(val_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, items in enumerate(train_loader):\n",
    "  print(f'i: {i}  len(items): {len(items)}')\n",
    "  for j, item in enumerate(items):\n",
    "    print(f'j: {j}  len(item): {len(item)}')\n",
    "  print('===')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
