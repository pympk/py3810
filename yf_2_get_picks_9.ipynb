{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picks the best performing symbols based on parameter settings<br> - Get dates in df_close_clean that are missing in df_picks<br> - Calculate days_to_drop from df_close_clean s.t. the last date in df_close_clean is the missing date in df_picks<br> - Get picks for the missing dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lists(list_a, list_b):\n",
    "  \"\"\"Compares two lists and returns a list of values that are in list A but not in list B.\n",
    "\n",
    "  Args:\n",
    "    list_a: A list of objects.\n",
    "    list_b: A list of objects.\n",
    "\n",
    "  Returns:\n",
    "    A list of values that are in list_a but not in list_b.\n",
    "  \"\"\"\n",
    "\n",
    "  list_difference = []\n",
    "\n",
    "  for item in list_a:\n",
    "    if item not in list_b:\n",
    "      list_difference.append(item)\n",
    "\n",
    "  return list_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from yf_utils import random_slices, lookback_slices\n",
    "from yf_utils import rank_perf, grp_tuples_sort_sum, top_set_sym_freq_cnt\n",
    "from yf_utils import best_perf_syms_sets_lookback_slices\n",
    "from myUtils import pickle_load, pickle_dump\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 11)\n",
    "pd.set_option(\"display.max_colwidth\", 26)\n",
    "pd.set_option(\"display.width\", 280)\n",
    "\n",
    "path_dir = \"C:/Users/ping/MyDrive/stocks/yfinance/\"\n",
    "path_data_dump = path_dir + \"VSCode_dump/\"\n",
    "\n",
    "fp_df_close_clean = \"df_close_clean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose : False\n",
      "store_results: True\n",
      "n_samples: 1\n",
      "l_sorted_days_lookbacks: [[30, 60, 120], [15, 30, 60, 120]]\n",
      "days_eval: 0\n",
      "n_top_syms: 20\n",
      "syms_start: 0\n",
      "syms_end: 10\n",
      "fp_df_picks: df_picks\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Set parameters ######################################################################\n",
    "# fp_df_eval_results = f'df_eval_results_{run_type}'\n",
    "fp_df_picks = f\"df_picks\"\n",
    "\n",
    "verbose = False  # True prints more output\n",
    "# verbose = True  # True prints more output\n",
    "\n",
    "store_results = True\n",
    "\n",
    "# n_samples is the number of random samples of df chunks s.t.\n",
    "#   len(df) = max_days_lookbacks. For getting the current picks\n",
    "#   n_samples is always 1 since more samples will always yield\n",
    "#   the same df chunk\n",
    "n_samples = 1  # only need 1 sample to get the current picks\n",
    "\n",
    "# for training, the number of days to lookback from iloc max-lookback iloc_end_train\n",
    "l_days_lookbacks = [[30, 60, 120], [15, 30, 60, 120]]\n",
    "\n",
    "# e.g sort from [[60, 30, 120], [15, 60, 30, 120]] to [[30, 60, 120], [15, 30, 60, 120]] \n",
    "l_sorted_days_lookbacks = []\n",
    "for days_lookbacks in l_days_lookbacks:\n",
    "    l_sorted_days_lookbacks.append(sorted(days_lookbacks))\n",
    "\n",
    "# number of days from iloc_end_train are used to evaluate effectiveness of the training\n",
    "days_eval = 0\n",
    "\n",
    "# number of the most-common symbols from days_lookbacks' performance rankings to keep\n",
    "n_top_syms = 20\n",
    "\n",
    "# slice starts and ends for selecting the best performing symbols\n",
    "syms_start = 0\n",
    "syms_end = 10\n",
    "#########################################################################\n",
    "\n",
    "print(f\"verbose : {verbose }\")\n",
    "print(f\"store_results: {store_results}\")\n",
    "print(f\"n_samples: {n_samples}\")\n",
    "print(f\"l_sorted_days_lookbacks: {l_sorted_days_lookbacks}\")\n",
    "print(f\"days_eval: {days_eval}\")\n",
    "print(f\"n_top_syms: {n_top_syms}\")\n",
    "print(f\"syms_start: {syms_start}\")\n",
    "print(f\"syms_end: {syms_end}\")\n",
    "print(f\"fp_df_picks: {fp_df_picks}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load past picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_picks, len(380):\n",
      "    date_end_df_train  max_days_lookbacks     days_lookbacks sym_freq_15 sym_freq_14  ...                 sym_freq_6                 sym_freq_5                 sym_freq_4                 sym_freq_3 sym_freq_2\n",
      "0          2023-12-13                 120      [30, 60, 120]          []          []  ...                    ['EDU']     ['ANF', 'GDDY', 'LRN']   ['CBOE', 'GBTC', 'HIBB']  ['AXGN', 'BPMC', 'BTC-...         []\n",
      "1          2023-12-13                 120  [15, 30, 60, 120]          []          []  ...       ['EDU', 'HA', 'MBI']  ['ANF', 'AXGN', 'GDDY'...  ['BPMC', 'CBOE', 'GBTC...  ['BTC-USD', 'EFX', 'HO...         []\n",
      "2          2023-12-12                 120      [30, 60, 120]          []          []  ...            ['EDU', 'GDDY']     ['ANF', 'LRN', 'WING']                   ['VRNS']  ['AMKR', 'AXGN', 'BPMC...         []\n",
      "3          2023-12-12                 120  [15, 30, 60, 120]          []          []  ...  ['BPMC', 'EDU', 'GDDY'...     ['ANF', 'LRN', 'WING']  ['AEO', 'AXGN', 'SQ', ...  ['AMKR', 'BTC-USD', 'C...         []\n",
      "4          2023-12-11                 120      [30, 60, 120]          []          []  ...                   ['GDDY']  ['ANF', 'EDU', 'LRN', ...           ['CBOE', 'VRNS']  ['AEO', 'AXGN', 'BTC-U...         []\n",
      "..                ...                 ...                ...         ...         ...  ...                        ...                        ...                        ...                        ...        ...\n",
      "375        2023-03-17                 120  [15, 30, 60, 120]          []          []  ...  ['ANET', 'FCN', 'GBTC'...  ['ACLS', 'FSLR', 'HY',...     ['AMD', 'ATCO', 'MLR']                   ['AJRD']         []\n",
      "376        2023-03-16                 120      [30, 60, 120]          []          []  ...    ['LNTH', 'SGEN', 'WST']  ['ACLS', 'AMPH', 'CTLT...       ['HY', 'MLR', 'OEC']  ['ANET', 'ATCO', 'ATKR...         []\n",
      "377        2023-03-16                 120  [15, 30, 60, 120]          []          []  ...  ['ANET', 'FCN', 'FSLR'...  ['ACLS', 'AMPH', 'CTLT...                         []                         []         []\n",
      "378        2023-03-15                 120      [30, 60, 120]          []          []  ...  ['CTLT', 'ELF', 'SGEN'...  ['ACLS', 'AMPH', 'FCN'...  ['ATCO', 'HY', 'LNTH',...  ['ATKR', 'BURL', 'FIZZ...         []\n",
      "379        2023-03-15                 120  [15, 30, 60, 120]          []          []  ...  ['CTLT', 'ELF', 'FIZZ'...   ['ACLS', 'ANET', 'SRPT']           ['ATCO', 'LNTH']                         []         []\n",
      "\n",
      "[380 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "df_picks = pickle_load(path_data_dump, fp_df_picks)\n",
    "# drop duplicates\n",
    "df_picks = df_picks.drop_duplicates(subset=['date_end_df_train', 'max_days_lookbacks', 'days_lookbacks'], keep='last')\n",
    "# sort, most recent date is first\n",
    "df_picks = df_picks.sort_values(by=['date_end_df_train', 'max_days_lookbacks', 'days_lookbacks'], ascending=False)\n",
    "# re-index\n",
    "df_picks = df_picks.reset_index(drop=True)\n",
    "# save results\n",
    "pickle_dump(df_picks, path_data_dump, fp_df_picks)\n",
    "print(f'df_picks, len({len(df_picks)}):\\n{df_picks}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dates in df_close_clean that are missing in df_picks<br>=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date, earliest date in df_picks:   2023-03-15\n",
      "end_date, latest date in df_close_clean: 2023-12-13\n",
      "date range to find dates in df_close_clean that are missing in df_picks: 2023-03-15 - 2023-12-13\n"
     ]
    }
   ],
   "source": [
    "# earliest date in df_picks\n",
    "start_date = df_picks.date_end_df_train.min()\n",
    "# load df with symbols' close\n",
    "df_close_clean = pickle_load(path_data_dump, fp_df_close_clean)\n",
    "# latest date in df_close_clean\n",
    "end_date = df_close_clean.index[-1].strftime('%Y-%m-%d')\n",
    "print(f'start_date, earliest date in df_picks:   {start_date}')\n",
    "print(f'end_date, latest date in df_close_clean: {end_date}')\n",
    "print(f'date range to find dates in df_close_clean that are missing in df_picks: {start_date} - {end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_dates_df_picks, len(190):\n",
      "['2023-12-13', '2023-12-12', '2023-12-11', '2023-12-08', '2023-12-07', '2023-12-06', '2023-12-05', '2023-12-04', '2023-12-01', '2023-11-30', '2023-11-29', '2023-11-28', '2023-11-27', '2023-11-24', '2023-11-22', '2023-11-21', '2023-11-20', '2023-11-17', '2023-11-16', '2023-11-15', '2023-11-14', '2023-11-13', '2023-11-10', '2023-11-09', '2023-11-08', '2023-11-07', '2023-11-06', '2023-11-03', '2023-11-02', '2023-11-01', '2023-10-31', '2023-10-30', '2023-10-27', '2023-10-26', '2023-10-25', '2023-10-24', '2023-10-23', '2023-10-20', '2023-10-19', '2023-10-18', '2023-10-17', '2023-10-16', '2023-10-13', '2023-10-12', '2023-10-11', '2023-10-10', '2023-10-09', '2023-10-06', '2023-10-05', '2023-10-04', '2023-10-03', '2023-10-02', '2023-09-29', '2023-09-28', '2023-09-27', '2023-09-26', '2023-09-25', '2023-09-22', '2023-09-21', '2023-09-20', '2023-09-19', '2023-09-18', '2023-09-15', '2023-09-14', '2023-09-13', '2023-09-12', '2023-09-11', '2023-09-08', '2023-09-07', '2023-09-06', '2023-09-05', '2023-09-01', '2023-08-31', '2023-08-30', '2023-08-29', '2023-08-28', '2023-08-25', '2023-08-24', '2023-08-23', '2023-08-22', '2023-08-21', '2023-08-18', '2023-08-17', '2023-08-16', '2023-08-15', '2023-08-14', '2023-08-11', '2023-08-10', '2023-08-09', '2023-08-08', '2023-08-07', '2023-08-04', '2023-08-03', '2023-08-02', '2023-08-01', '2023-07-31', '2023-07-28', '2023-07-27', '2023-07-26', '2023-07-25', '2023-07-24', '2023-07-21', '2023-07-20', '2023-07-19', '2023-07-18', '2023-07-17', '2023-07-14', '2023-07-13', '2023-07-12', '2023-07-11', '2023-07-10', '2023-07-07', '2023-07-06', '2023-07-05', '2023-07-03', '2023-06-30', '2023-06-29', '2023-06-28', '2023-06-27', '2023-06-26', '2023-06-23', '2023-06-22', '2023-06-21', '2023-06-20', '2023-06-16', '2023-06-15', '2023-06-14', '2023-06-13', '2023-06-12', '2023-06-09', '2023-06-08', '2023-06-07', '2023-06-06', '2023-06-05', '2023-06-02', '2023-06-01', '2023-05-31', '2023-05-30', '2023-05-26', '2023-05-25', '2023-05-24', '2023-05-23', '2023-05-22', '2023-05-19', '2023-05-18', '2023-05-17', '2023-05-16', '2023-05-15', '2023-05-12', '2023-05-11', '2023-05-10', '2023-05-09', '2023-05-08', '2023-05-05', '2023-05-04', '2023-05-03', '2023-05-02', '2023-05-01', '2023-04-28', '2023-04-27', '2023-04-26', '2023-04-25', '2023-04-24', '2023-04-21', '2023-04-20', '2023-04-19', '2023-04-18', '2023-04-17', '2023-04-14', '2023-04-13', '2023-04-12', '2023-04-11', '2023-04-10', '2023-04-06', '2023-04-05', '2023-04-04', '2023-04-03', '2023-03-31', '2023-03-30', '2023-03-29', '2023-03-28', '2023-03-27', '2023-03-24', '2023-03-23', '2023-03-22', '2023-03-21', '2023-03-20', '2023-03-17', '2023-03-16', '2023-03-15']\n"
     ]
    }
   ],
   "source": [
    "l_dates_df_picks = df_picks.date_end_df_train.unique().tolist()  # unique dates in df_picks\n",
    "print(f'l_dates_df_picks, len({len(l_dates_df_picks)}):\\n{l_dates_df_picks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_dates_df_close, (len=190):\n",
      "Index(['2023-03-15', '2023-03-16', '2023-03-17', '2023-03-20', '2023-03-21', '2023-03-22', '2023-03-23', '2023-03-24', '2023-03-27', '2023-03-28',\n",
      "       ...\n",
      "       '2023-11-30', '2023-12-01', '2023-12-04', '2023-12-05', '2023-12-06', '2023-12-07', '2023-12-08', '2023-12-11', '2023-12-12', '2023-12-13'],\n",
      "      dtype='object', name='Date', length=190)\n"
     ]
    }
   ],
   "source": [
    "# Select rows in df_close_clean between the start_date and end_date in df_picks\n",
    "mask = (df_close_clean.index >= start_date) & (df_close_clean.index <= end_date)\n",
    "l_dates_df_close = df_close_clean[mask].index\n",
    "# list of date index in 'yyyy-mm-dd' format\n",
    "l_dates_df_close = l_dates_df_close.strftime('%Y-%m-%d')\n",
    "print(f'l_dates_df_close, (len={len(l_dates_df_close )}):\\n{l_dates_df_close }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates_missing_in_df_picks, (len=0):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "dates_missing_in_df_picks  = compare_lists(l_dates_df_close, l_dates_df_picks)\n",
    "# pickle_dump(dates_missing_in_df_picks, path_data_dump, fp_dates_missing_in_df_picks, verbose=verbose)\n",
    "print(f'dates_missing_in_df_picks, (len={len(dates_missing_in_df_picks )}):\\n{dates_missing_in_df_picks }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dates in df_close_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_close_clean:\n",
      "                     A         AA        AAL         AAP        AAPL  ...       ZION        ZTO         ZTS       ZUMZ        ZWS\n",
      "Date                                                                  ...                                                        \n",
      "2017-12-28   64.448524  52.993481  51.130989   91.800758   40.443913  ...  42.645565  14.851050   69.558945  21.200001  25.326429\n",
      "2017-12-29   64.131584  52.729191  50.711876   91.782356   40.006577  ...  42.221935  14.795045   69.222618  20.830000  25.142834\n",
      "2018-01-02   64.734856  54.001663  51.647560   97.674690   40.722874  ...  42.113949  15.327106   68.963165  20.000000  25.104179\n",
      "2018-01-03   66.381973  53.345856  51.014030   98.558540   40.715786  ...  42.064114  15.224427   69.280281  20.500000  25.336088\n",
      "2018-01-04   65.884010  53.541622  51.335659  102.195236   40.904911  ...  42.238541  15.047073   69.693481  22.950001  25.452044\n",
      "...                ...        ...        ...         ...         ...  ...        ...        ...         ...        ...        ...\n",
      "2023-12-07  128.679993  24.940001  13.910000   56.250000  194.270004  ...  38.380001  21.260000  181.830002  19.469999  29.480000\n",
      "2023-12-08  127.199997  25.020000  13.760000   56.270000  195.710007  ...  38.930000  21.309999  184.600006  19.350000  29.389999\n",
      "2023-12-11  128.970001  24.930000  13.720000   57.080002  193.179993  ...  38.630001  21.020000  189.460007  18.969999  29.610001\n",
      "2023-12-12  128.789993  24.049999  14.040000   56.250000  194.710007  ...  38.130001  21.020000  191.470001  18.570000  29.230000\n",
      "2023-12-13  133.740005  25.990000  14.090000   60.830002  197.960007  ...  41.820000  20.400000  197.410004  19.670000  29.320000\n",
      "\n",
      "[1500 rows x 1268 columns]\n",
      "\n",
      "df_close_clean_index:\n",
      "DatetimeIndex(['2017-12-28', '2017-12-29', '2018-01-02', '2018-01-03', '2018-01-04', '2018-01-05', '2018-01-08', '2018-01-09', '2018-01-10', '2018-01-11',\n",
      "               ...\n",
      "               '2023-11-30', '2023-12-01', '2023-12-04', '2023-12-05', '2023-12-06', '2023-12-07', '2023-12-08', '2023-12-11', '2023-12-12', '2023-12-13'],\n",
      "              dtype='datetime64[ns]', name='Date', length=1500, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# df_close_clean = pickle_load(path_data_dump, fp_df_close_clean)\n",
    "# Sort the DataFrame by the date index in place\n",
    "df_close_clean.sort_index(inplace=True)\n",
    "print(f'df_close_clean:\\n{df_close_clean}\\n')\n",
    "df_close_clean_index = df_close_clean.index\n",
    "print(f'df_close_clean_index:\\n{df_close_clean_index}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a missing date in df_picks, calculate the number of days_to_drop from df_close_clean s.t. the last date in df_close_clean is the missing date in df_picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates_missing_in_df_picks, len=0: []\n",
      "l_days_to_drop, len=0: []\n"
     ]
    }
   ],
   "source": [
    "# list to store days_to_drop from df_close_clean\n",
    "l_days_to_drop = []\n",
    "\n",
    "# Calculate the number of days to drop from the date index.\n",
    "for i in range(len(dates_missing_in_df_picks)):\n",
    "  date = dates_missing_in_df_picks[i]\n",
    "  last_date_index = df_close_clean_index.get_loc(date)\n",
    "  # number of days to drop from df such that the last date is a missing date    \n",
    "  days_to_drop = len(df_close_clean_index) - last_date_index - 1\n",
    "  l_days_to_drop.append(days_to_drop)  \n",
    "\n",
    "print(f'dates_missing_in_df_picks, len={len(dates_missing_in_df_picks)}: {dates_missing_in_df_picks}')\n",
    "print(f'l_days_to_drop, len={len(l_days_to_drop)}: {l_days_to_drop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get picks for the missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = df_close_clean.copy()\n",
    "\n",
    "# list to store the last df date\n",
    "l_date_last_index = []\n",
    "\n",
    "# list to sore date in zipped dates_missing_in_df_picks\n",
    "l_date_missing_in_df_picks = []\n",
    "\n",
    "# total number of iteration\n",
    "i_total = len(l_days_to_drop) * len(l_sorted_days_lookbacks)\n",
    "\n",
    "for i, values in enumerate(itertools.product(zip(l_days_to_drop, dates_missing_in_df_picks), l_sorted_days_lookbacks)):\n",
    "    print(f'Start of For_Loop  {i+1} of {i_total} loops{\"=\"*40:>42}')\n",
    "    drop_last_n_rows = values[0][0]\n",
    "    date_missing_in_df_picks = values[0][1] \n",
    "    days_lookbacks = values[1]\n",
    "    l_date_missing_in_df_picks.append(date)\n",
    " \n",
    "    print(f'i: {i+1}')    \n",
    "    print(f'drop_last_n_rows: {drop_last_n_rows}') \n",
    "    print(f'date_missing_in_df_picks: {date_missing_in_df_picks}')\n",
    "    print(f'days_lookbacks: {days_lookbacks}\\n')\n",
    "    print(f'{i+1} of {len(l_sorted_days_lookbacks)} days_lookbacks: {days_lookbacks} in l_sorted_days_lookbacks: {l_sorted_days_lookbacks}')    \n",
    "\n",
    "    # drops df rows by drop_last_n_rows, limits df length to max_days_lookbacks \n",
    "    # e.g. days_lookbacks: [15, 30, 60, 120] => max_days_lookbacks: 120\n",
    "    max_days_lookbacks = max(days_lookbacks)\n",
    "    print(f\"max_days_lookbacks: {max_days_lookbacks}\\n\")\n",
    "    slice_start = -(max_days_lookbacks + drop_last_n_rows)\n",
    "    slice_end = -drop_last_n_rows\n",
    "    if drop_last_n_rows == 0:  # return df with all rows\n",
    "        df = df_current[slice_start:].copy()\n",
    "    else:  # return df with dropped drop_last_n_rows rows\n",
    "        df = df_current[slice_start:slice_end].copy()\n",
    "    print(f\"dropped last {drop_last_n_rows} row(s) from df since drop_last_n_rows = {drop_last_n_rows}\")\n",
    "    print(f\"df.head(1):\\n{df.head(1)}\\n\")\n",
    "    print(f\"df.tail(1):\\n{df.tail(1)}\\n\")\n",
    "\n",
    "    date_last_index = df.index[-1].strftime('%Y-%m-%d')\n",
    "    print(f'date_last_index: {date_last_index}')\n",
    "\n",
    "    # Error check\n",
    "    if date_missing_in_df_picks == date_last_index:\n",
    "        print(f\"Passed Error Check: date_missing_in_df_picks {date_missing_in_df_picks} == date_last_index {date_last_index}\")  \n",
    "    else:\n",
    "        print(\"ERROR: date_missing_in_df_picks != date_last_index\")\n",
    "        print(f'date_missing_in_df_picks: {date_missing_in_df_picks}')\n",
    "        print(f'date_last_index: {date_last_index}')\n",
    "        sys.exit(1)  \n",
    "\n",
    "    l_date_last_index.append(date_last_index)\n",
    "    len_df = len(df)\n",
    "    len_slice = slice_end - slice_start\n",
    "    # print(f\"len(df): {len(df)}\\n\")\n",
    "    print(f\"len_df: {len_df}, len_slice: {len_slice}\\n\")    \n",
    "\n",
    "    # Since df rows has been sliced to max_days_lookbacks,\n",
    "    #   n_samples > 1, will always return more copies of the same slice. \n",
    "    # Returns a list of random tuples of start_train, end_train, end_eval,\n",
    "    # where iloc[start_train:end_train] is used for training,\n",
    "    # and iloc[end_train:end_eval] is used for evaluation.  The length of the\n",
    "    # list is equal to n_samples.\n",
    "    max_lookback_slices = random_slices(\n",
    "        len_df = len_df,\n",
    "        n_samples=n_samples,\n",
    "        days_lookback=max(days_lookbacks),\n",
    "        days_eval=days_eval,\n",
    "        verbose=False,\n",
    "    )    \n",
    "\n",
    "    # Create sets of sub-slices from max_slices and days_lookbacks. A slice is\n",
    "    # a tuple of iloc values for start_train:end_train=start_eval:end_eval.\n",
    "    # Given 2 max_slices of [(104, 224, 234), (626, 746, 756)], it returns 2 sets\n",
    "    # [[(194, 224, 234), (164, 224, 234), (104, 224, 234)],\n",
    "    # [(716, 746, 756), (686, 746, 756), (626, 746, 756)]]. End_train is constant\n",
    "    # for each set. End_train - start_train is the value of the maximum slice.     \n",
    "    sets_lookback_slices = lookback_slices(\n",
    "        max_slices=max_lookback_slices, days_lookbacks=days_lookbacks, verbose=False\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"number of random samples of max_lookback_slices taken is n_samples = {n_samples}\")\n",
    "        print(f\"max_lookback_slices: {max_lookback_slices}\\n\")\n",
    "        print(f\"days_lookbacks: {days_lookbacks}\")\n",
    "        print(f\"sets_lookback_slices, e.g. (start_train:end_train:end_eval): {sets_lookback_slices}\\n\")\n",
    "\n",
    "        print(f\"number of sets in sets_lookback_slices is equal to n_samples = {n_samples}\")\n",
    "        print(\n",
    "            f'number of tuples in each \"set of lookback slices\" is equal to len(days_lookbacks): {len(days_lookbacks)}'\n",
    "        )\n",
    "\n",
    "    # If given:\n",
    "    #  performance metric: r_CAGR/UI, r_CAGR/retnStd, r_retnStd/UI\n",
    "    #  l_sorted_days_lookbacks: [[30, 60, 120], [15, 30, 60, 120]]\n",
    "    #   => days_lookbacks: [30, 60, 120]\n",
    "    #    => sets_lookback_slices: [[(90, 120, 120), (60, 120, 120), (0, 120, 120)]]\n",
    "    # Then, grp_top_set_syms_n_freq is a list of lists of the top n_top_syms of the\n",
    "    # best performing symbols and their number of occurrence for sets_lookback_slices.\n",
    "    # The list of lists corresponds to days_lookbacks in l_sorted_days_lookbacks.  \n",
    "    #  e.g. grp_top_set_syms_n_freq:\n",
    "    #   [[('GPS', 8), ('SHV', 8), ('FTSM', 7), ('GBTC', 7), ('BTC-USD', 6), ('CBOE', 6), ('ANF', 5), ('NRG', 5), ('WING', 5), ('DELL', 4), ('EDU', 4), ('HIBB', 4), ('LRN', 4), ('ALL', 3), ('CAH', 3), ('CMG', 3), ('GDDY', 3), ('HRB', 3), ('MDLZ', 3), ('PGR', 3)]]\n",
    "    # grp_top_set_syms is grp_top_set_syms_n_freq with number of occurrence dropped\n",
    "    #   e.g. [['GPS', 'SHV', 'FTSM', 'GBTC', 'BTC-USD', 'CBOE', 'ANF', 'NRG', 'WING', 'DELL']]\n",
    "    # date_end_df_train, e.g. 2023-11-22\n",
    "    (\n",
    "        grp_top_set_syms_n_freq,\n",
    "        grp_top_set_syms,\n",
    "        date_end_df_train,\n",
    "    ) = best_perf_syms_sets_lookback_slices(\n",
    "        df_close=df,\n",
    "        sets_lookback_slices=sets_lookback_slices,\n",
    "        n_top_syms=20,\n",
    "        syms_start=0,\n",
    "        syms_end=10,\n",
    "        verbose=verbose\n",
    "        )\n",
    "\n",
    "    print(f'\\nOutput from function best_perf_syms_sets_lookback_slices')\n",
    "    print(f'{\"`\"*60}')\n",
    "    print(f'sets_lookback_slices: {sets_lookback_slices}\\n')\n",
    "    print(f'grp_top_set_syms_n_freq:\\n{grp_top_set_syms_n_freq}\\n')\n",
    "    print(f'grp_top_set_syms:\\n{grp_top_set_syms}\\n')\n",
    "    print(f'date_end_df_train:\\n{date_end_df_train}')    \n",
    "    print(f'{\"`\"*60}\\n\\n')\n",
    "\n",
    "    for j, top_set_syms_n_freq in enumerate(grp_top_set_syms_n_freq):\n",
    "        # If given top_set_syms_n_freq:\n",
    "        #  [('GPS', 10), ('SHV', 9), ('FTSM', 7), ('GBTC', 7), ('WING', 7), ('CBOE', 6),\n",
    "        #  ('ANF', 5), ('BTC-USD', 5), ('NRG', 5), ('TSEM', 5), ('BURL', 4), ('CRSP', 4),\n",
    "        #  ('EDU', 4), ('GDDY', 4), ('LRN', 4), ('NFLX', 4), ('PI', 4), ('WRB', 4),\n",
    "        #  ('AXGN', 3), ('CAH', 3)]\n",
    "        # Then, l_sym_freq_cnt, where symbol frequency count is from 15, 14, ..., 2:\n",
    "        #  [[], [], [], [], [], ['GPS'], ['SHV'], [], ['FTSM', 'GBTC', 'WING'], ['CBOE'],\n",
    "        #  ['ANF', 'BTC-USD', 'NRG', 'TSEM'],\n",
    "        #  ['BURL', 'CRSP', 'EDU', 'GDDY', 'LRN', 'NFLX', 'PI', 'WRB'],\n",
    "        #  ['AXGN', 'CAH'], []]\n",
    "        l_sym_freq_cnt = top_set_sym_freq_cnt(top_set_syms_n_freq)\n",
    "        print(f'{j}, grp_top_set_syms_n_freq:\\n{grp_top_set_syms_n_freq}')\n",
    "        print(f'{j}, top_set_syms_n_freq:\\n{top_set_syms_n_freq}')\n",
    "        print(f'{j}, l_sym_freq_cnt:\\n{l_sym_freq_cnt}\\n')                \n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"set_lookback_slices: {sets_lookback_slices[j]}\")\n",
    "            print(f\"max_lookback_slices: {max_lookback_slices}\\n\")\n",
    "            print(f\"data below will be added to {fp_df_picks}\")\n",
    "            print(f\"date_end_df_train:   {date_end_df_train}\")\n",
    "            print(f\"max_days_lookbacks:  {max_days_lookbacks}\")\n",
    "            print(f\"days_lookbacks:      {days_lookbacks}\")\n",
    "            print(f\"sym_freq_15:         {l_sym_freq_cnt[0]}\")\n",
    "            print(f\"sym_freq_14:         {l_sym_freq_cnt[1]}\")\n",
    "            print(f\"sym_freq_13:         {l_sym_freq_cnt[2]}\")\n",
    "            print(f\"sym_freq_12:         {l_sym_freq_cnt[3]}\")\n",
    "            print(f\"sym_freq_11:         {l_sym_freq_cnt[4]}\")\n",
    "            print(f\"sym_freq_10:         {l_sym_freq_cnt[5]}\")\n",
    "            print(f\"sym_freq_9:          {l_sym_freq_cnt[6]}\")\n",
    "            print(f\"sym_freq_8:          {l_sym_freq_cnt[7]}\")\n",
    "            print(f\"sym_freq_7:          {l_sym_freq_cnt[8]}\")\n",
    "            print(f\"sym_freq_6:          {l_sym_freq_cnt[9]}\")\n",
    "            print(f\"sym_freq_5:          {l_sym_freq_cnt[10]}\")\n",
    "            print(f\"sym_freq_4:          {l_sym_freq_cnt[11]}\")\n",
    "            print(f\"sym_freq_3:          {l_sym_freq_cnt[12]}\")\n",
    "            print(f\"sym_freq_2:          {l_sym_freq_cnt[13]}\\n\")\n",
    "\n",
    "    if store_results:\n",
    "        row_picks0 = [date_end_df_train, max_days_lookbacks, str(days_lookbacks)]\n",
    "        row_picks1 = [\n",
    "            str(l_sym_freq_cnt[0]),\n",
    "            str(l_sym_freq_cnt[1]),\n",
    "            str(l_sym_freq_cnt[2]),\n",
    "            str(l_sym_freq_cnt[3]),\n",
    "        ]\n",
    "        row_picks2 = [\n",
    "            str(l_sym_freq_cnt[4]),\n",
    "            str(l_sym_freq_cnt[5]),\n",
    "            str(l_sym_freq_cnt[6]),\n",
    "            str(l_sym_freq_cnt[7]),\n",
    "        ]\n",
    "        row_picks3 = [\n",
    "            str(l_sym_freq_cnt[8]),\n",
    "            str(l_sym_freq_cnt[9]),\n",
    "            str(l_sym_freq_cnt[10]),\n",
    "            str(l_sym_freq_cnt[11]),\n",
    "        ]\n",
    "        row_picks4 = [str(l_sym_freq_cnt[12]), str(l_sym_freq_cnt[13])]\n",
    "        row_picks_total = row_picks0 + row_picks1 + row_picks2 + row_picks3 + row_picks4\n",
    "        print(f\"row_picks_total: {row_picks_total}\")\n",
    "\n",
    "        df_picks.loc[len(df_picks)] = row_picks_total\n",
    "        pickle_dump(df_picks, path_data_dump, fp_df_picks)\n",
    "        print(f\"appended row_picks_total to df_picks:\\n{row_picks_total}\\n\")\n",
    "\n",
    "    print(f'End of For_Loop  {i+1} of {i_total} loops{\"=\"*40:>44}\\n\\n')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
