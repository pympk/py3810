{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/pandas-groupby-a-simple-but-detailed-tutorial-314b8f37005d\n",
    "# https://towardsdatascience.com/accessing-data-in-a-multiindex-dataframe-in-pandas-569e8767201d\n",
    "# https://towardsdatascience.com/summarizing-data-with-pandas-crosstab-efc8b9abecf\n",
    "# https://towardsdatascience.com/how-to-flatten-multiindex-columns-and-rows-in-pandas-f5406c50e569\n",
    "# https://datascientyst.com/list-aggregation-functions-aggfunc-groupby-pandas/\n",
    "# https://stackoverflow.com/questions/25929319/how-to-iterate-over-pandas-multiindex-dataframe-using-index\n",
    "# https://stackoverflow.com/questions/24495695/pandas-get-unique-multiindex-level-values-by-label\n",
    "# https://stackoverflow.com/questions/55706391/pandas-crosstab-on-multiple-columns-then-groupby\n",
    "\n",
    "# https://matplotlib.org/stable/gallery/pyplots/pyplot_text.html#sphx-glr-gallery-pyplots-pyplot-text-py\n",
    "\n",
    "# https://stats.stackexchange.com/questions/346907/splitting-time-series-data-into-train-test-validation-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yf_utils import split_train_val_test # NOQA\n",
    "from myUtils import pickle_load\n",
    "\n",
    "path_dir = \"C:/Users/ping/MyDrive/stocks/yfinance/\"\n",
    "path_data_dump = path_dir + \"VSCode_dump/\"\n",
    "verbose = False  # True prints more output\n",
    "filename_pickled_df_c = 'df_close_clean'  # df close, dropped symbols with no vol and close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pickle_load(path_data_dump, filename_pickled_df_c, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 300 300\n"
     ]
    }
   ],
   "source": [
    "_train, _val, _test = split_train_val_test(df_c, s_train=.6, s_val=.2, s_test=.2)\n",
    "print(len(_train), len(_val), len(_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_train_val_test(df, s_train=.7, s_val=.2, s_test=.1, verbose=False):\n",
    "#   ''''\n",
    "#   Split df into df_train, df_val and df_test and returns the splitted dfs\n",
    "#   '''\n",
    "#   if (s_train + s_val + s_test) - 1 >.0001:  # allow 0.01% error\n",
    "#     _sum = s_train + s_val + s_test\n",
    "#     raise Exception(f\"s_train({s_train}) + s_val({s_val}) + s_test({s_test}) must sums to 1\") \n",
    "#   n_train = round(len(df_c) * s_train)\n",
    "#   n_val = round(len(df_c) * s_val)\n",
    "#   n_test = round(len(df_c) * s_test)\n",
    "#   df_train = df_c.iloc[0:n_train]\n",
    "#   df_val = df_c.iloc[n_train:(n_train+n_val)]\n",
    "#   df_test = df_c.iloc[(n_train+n_val)::]\n",
    "\n",
    "#   len_df = len(df)\n",
    "#   len_train = len(df_train)\n",
    "#   len_val = len(df_val)\n",
    "#   len_test = len(df_test)\n",
    "#   sum_train_val_test = len_df + len_train + len_test\n",
    "\n",
    "#   if verbose: \n",
    "#     print(f'len(df): {len_df}')\n",
    "#     print(f'len(df_train): {len_train}')\n",
    "#     print(f'len(df_val): {len_val}')\n",
    "#     print(f'len(df_test): {len_test}')\n",
    "#     print(f'sum_train_val_test: {sum_train_val_test}')\n",
    "\n",
    "#   return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from myUtils import pickle_load, pickle_dump, symb_perf_stats_vectorized\n",
    "\n",
    "path_dir = \"C:/Users/ping/MyDrive/stocks/yfinance/\"\n",
    "path_data_dump = path_dir + \"VSCode_dump/\"\n",
    "\n",
    "filename_symbols = path_data_dump + 'vg_symbols_4chars_max.csv'  # symbols text file\n",
    "# _filename_pickled_df_OHLCVA_downloaded = 'df_OHLCVA_downloaded '  # OHLCVA downloaded from Yahoo\n",
    "filename_pickled_df_adjOHLCV = 'df_adjOHLCV'  # adjusted OHLCV\n",
    "filename_pickled_df_symbols_close = \"df_symbols_close\"  # symbols' adjusted close\n",
    "filename_pickled_symbols_df_adjOHLCV =  'symbols_df_adjOHLCV'  # symbols in df_adjOHLCV\n",
    "filename_pickled_perf_rank_dict =  'perf_rank_dict'  # store symbols from performance rank results\n",
    "filename_pickled_r_all_ranks =  'r_all_ranks'  # list of top 100 most common symbols from performance rank results\n",
    "filename_pickled_df_a = 'df_OHLCV_clean'  # df adjusted OHLCV, dropped symbols with no vol and close\n",
    "filename_pickled_df_c = 'df_close_clean'  # df close, dropped symbols with no vol and close\n",
    "\n",
    "verbose = False  # True prints more output\n",
    "look_back_days = -250 * 6  # subset df iloc days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Full path to pickled df_close_clean:  {path_data_dump}{filename_pickled_df_c}\")\n",
    "df_c = pickle_load(path_data_dump, filename_pickled_df_c, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train, s_val, s_test = .7, .2, .1\n",
    "# if s_train + s_val + s_test != 1.0:\n",
    "if (s_train + s_val + s_test) - 1 >.0001:  # allow 0.01% error\n",
    "  _sum = s_train + s_val + s_test\n",
    "  raise Exception(f\"s_train({s_train}) + s_val({s_val}) + s_test({s_test}) must sums to 1\") \n",
    "n_train = round(len(df_c) * s_train)\n",
    "n_val = round(len(df_c) * s_val)\n",
    "n_test = round(len(df_c) * s_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_c.iloc[0:n_train]\n",
    "df_val = df_c.iloc[n_train:(n_train+n_val)]\n",
    "df_test = df_c.iloc[(n_train+n_val)::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'len(df_train): {len(df_train)}')\n",
    "print(f'len(df_val): {len(df_val)}')\n",
    "print(f'len(df_test): {len(df_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(df_train, path_data_dump, 'df_train')\n",
    "print(f\"Full path to pickled df_train: {path_data_dump}df_train\")\n",
    "pickle_dump(df_val, path_data_dump, 'df_val')\n",
    "print(f\"Full path to pickled df_val: {path_data_dump}df_val\")\n",
    "pickle_dump(df_test, path_data_dump, 'df_test')\n",
    "print(f\"Full path to pickled df_test: {path_data_dump}df_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55a3812d65f91e7e7447da6b5cfc60716e82f91e6a92533fb27b46796ad1962a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
