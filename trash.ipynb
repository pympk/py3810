{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from myUtils import pickle_load, pickle_dump\n",
    "from yf_utils import split_train_val_test, random_slices, lookback_slices\n",
    "from yf_utils import top_set_sym_freq_cnt, get_grp_top_syms_n_freq\n",
    "from yf_utils import eval_grp_top_syms_n_freq\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"display.max_colwidth\", 16)\n",
    "pd.set_option(\"display.width\", 140)  # code-runner format\n",
    "\n",
    "path_dir = \"C:/Users/ping/MyDrive/stocks/yfinance/\"\n",
    "path_data_dump = path_dir + \"VSCode_dump/\"\n",
    "\n",
    "fp_df_close_clean = \"df_close_clean\"\n",
    "\n",
    "# SELECT RUN PARAMETERS.async Parameters can also be passed using papermill by running yf_7_freq_cnt_pm_.ipynb\n",
    "# verbose = True  # True prints more output\n",
    "verbose = False  # True prints more output\n",
    "\n",
    "# write run results to df_eval_results\n",
    "# store_results = False\n",
    "store_results = True\n",
    "\n",
    "# select run type\n",
    "run_type = \"train\"\n",
    "# run_type = 'validate'\n",
    "# run_type = \"test\"\n",
    "\n",
    "# number of max lookback tuples to create for iloc iloc_start_train:iloc_end_train:iloc_end_eval\n",
    "# i.e. number of grp_top_set_syms_n_freq and grp_top_set_syms\n",
    "# n_samples = 400\n",
    "n_samples = 2\n",
    "\n",
    "# for training, the number of days to lookback from iloc max-lookback iloc_end_train\n",
    "# days_lookbacks = [15, 30, 60]\n",
    "# days_lookbacks = [30, 60, 120]\n",
    "days_lookbacks = [15, 30, 60, 120]\n",
    "days_lookbacks.sort()\n",
    "\n",
    "# number of days in dataframe to evaluate effectiveness of the training, days_eval = len(df_eval)\n",
    "#  days_eval = 4 means buy at close on 1st day after the signal, hold for 2nd and 3rd day, sell at close on 4th day\n",
    "days_eval = 4\n",
    "# days_eval = 5\n",
    "\n",
    "# number of the most-common symbols from days_lookbacks' performance rankings to keep\n",
    "n_top_syms = 20\n",
    "# n_top_syms = 5\n",
    "\n",
    "# slice starts and ends for selecting the best performing symbols\n",
    "syms_start = 0\n",
    "syms_end = 10\n",
    "\n",
    "\n",
    "##########################################################\n",
    "# fp_df_eval_results = f\"df_eval_results_{run_type}\"\n",
    "fp_df_eval_results = \"df_eval_trash_new\"\n",
    "##########################################################\n",
    "\n",
    "\n",
    "fp_df_picks = \"df_picks\"\n",
    "\n",
    "print(f\"verbose : {verbose }\")\n",
    "print(f\"store_results: {store_results}\")\n",
    "print(f\"run_type: {run_type}\")\n",
    "print(f\"n_samples: {n_samples}\")\n",
    "print(f\"days_lookbacks: {days_lookbacks}\")\n",
    "print(f\"days_eval: {days_eval}\")\n",
    "print(f\"n_top_syms: {n_top_syms}\")\n",
    "print(f\"syms_start: {syms_start}\")\n",
    "print(f\"syms_end: {syms_end}\")\n",
    "print(f\"fp_df_eval_results: {fp_df_eval_results}\")\n",
    "print(f\"fp_df_picks: {fp_df_picks}\")\n",
    "\n",
    "df_picks = pickle_load(path_data_dump, fp_df_picks)\n",
    "df_close_clean = pickle_load(path_data_dump, fp_df_close_clean)\n",
    "\n",
    "# Split df_close_clean into training (df_train), validation (df_val) and test (df_test) set.\n",
    "# The default split is 0.7, 0.2, 0.1 respectively.\n",
    "df_train, df_val, df_test = split_train_val_test(df_close_clean)\n",
    "\n",
    "max_days_lookbacks = max(days_lookbacks)\n",
    "print(f\"max_days_lookbacks: {max_days_lookbacks}\")\n",
    "\n",
    "# Load df according to run_type\n",
    "if run_type == \"train\":\n",
    "    df = df_train.copy()\n",
    "elif run_type == \"validate\":\n",
    "    df = df_val.copy()\n",
    "elif run_type == \"test\":\n",
    "    df = df_test.copy()\n",
    "else:\n",
    "    msg_stop = f\"ERROR run_type must be 'train', 'validate', or 'test', run_type is: {run_type}\"\n",
    "    raise SystemExit(msg_stop)\n",
    "\n",
    "# Print dataframe for the run, and lengths of other dataframes\n",
    "print(f\"run_type: {run_type}, df.tail(3):\\n{df.tail(3)}\\n\")\n",
    "len_df = len(df)\n",
    "len_df_train = len(df_train)\n",
    "len_df_val = len(df_val)\n",
    "len_df_test = len(df_test)\n",
    "print(f\"run_type: {run_type}, len(df): {len(df)}\")\n",
    "print(\n",
    "    f\"len_df_train: {len_df_train}, len_df_val: {len_df_val}, len_df_test: {len_df_test}\"\n",
    ")\n",
    "\n",
    "# return n_samples slices\n",
    "max_lookback_slices = random_slices(\n",
    "    len_df,\n",
    "    n_samples=n_samples,\n",
    "    days_lookback=max(days_lookbacks),\n",
    "    days_eval=days_eval,\n",
    "    verbose=False,\n",
    ")\n",
    "# return n_samples * len(days_lookbacks) slices\n",
    "sets_lookback_slices = lookback_slices(\n",
    "    max_slices=max_lookback_slices, days_lookbacks=days_lookbacks, verbose=False\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "    print(f\"number of max_lookback_slices is equal to n_samples = {n_samples}\")\n",
    "    print(f\"max_lookback_slices:\\n{max_lookback_slices}\\n\")\n",
    "    print(f\"number of sets in sets_lookback_slices is equal to n_samples = {n_samples}\")\n",
    "    print(f\"sets_lookback_slices:\\n{sets_lookback_slices}\\n\")\n",
    "    print(f\"days_lookbacks: {days_lookbacks}\")\n",
    "    print(\n",
    "        f'number of tuples in each \"set of lookback slices\" is equal to len(days_lookbacks): {len(days_lookbacks)}'\n",
    "    )\n",
    "\n",
    "# #### Generate grp_top_set_syms_n_freq. It is a list of sub-lists, e.g.:\n",
    "#  - [[('AGY', 7), ('PCG', 7), ('KDN', 6), ..., ('CYT', 3)], ..., [('FCN', 9), ('HIG', 9), ('SJR', 8), ..., ('BFH', 2)]]\n",
    "# #### grp_top_set_syms_n_freq has n_samples sub-lists. Each sub-list corresponds to a tuple in the max_lookback_slices. Each sub-list has n_top_syms tuples of (symbol, frequency) pairs, and is sorted in descending order of frequency. The frequency is the number of times the symbol appears in the top n_top_syms performance rankings of CAGR/UI, CAGR/retnStd and retnStd/UI.\n",
    "# #### Therefore, symbols in the sub-list are the best performing symbols for the periods in days_lookbacks. Each sub-list corresponds to a tuple in max_lookback_slices. There are as many sub-lists as there are tuples in max_lookback_slices.\n",
    "grp_top_set_syms_n_freq, grp_top_set_syms, dates_end_df_train = get_grp_top_syms_n_freq(\n",
    "    df, sets_lookback_slices, days_lookbacks, n_top_syms, syms_start, syms_end, verbose\n",
    ")\n",
    "\n",
    "# #### print the best performing symbols for each set in sets_lookback_slices\n",
    "for i, top_set_syms_n_freq in enumerate(grp_top_set_syms_n_freq):\n",
    "    l_sym_freq_cnt = top_set_sym_freq_cnt(top_set_syms_n_freq)\n",
    "    if verbose:\n",
    "        print(f\"max_lookback_slices:             {max_lookback_slices}\")\n",
    "        # print(f'set_lookback_slices: {sets_lookback_slices[i]}\\n')\n",
    "        print(\n",
    "            f\"set_lookback_slices {i + 1} of {len(sets_lookback_slices):>3}:    {sets_lookback_slices[i]}\\n\"\n",
    "        )\n",
    "        print(f\"max_days_lookbacks:              {max_days_lookbacks}\")\n",
    "        print(f\"df end date for days_lookbacks:  {dates_end_df_train[i]}\")\n",
    "        print(f\"days_lookbacks:                  {days_lookbacks}\")\n",
    "        print(f\"sym_freq_15:                     {l_sym_freq_cnt[0]}\")\n",
    "        print(f\"sym_freq_14:                     {l_sym_freq_cnt[1]}\")\n",
    "        print(f\"sym_freq_13:                     {l_sym_freq_cnt[2]}\")\n",
    "        print(f\"sym_freq_12:                     {l_sym_freq_cnt[3]}\")\n",
    "        print(f\"sym_freq_11:                     {l_sym_freq_cnt[4]}\")\n",
    "        print(f\"sym_freq_10:                     {l_sym_freq_cnt[5]}\")\n",
    "        print(f\"sym_freq_9:                      {l_sym_freq_cnt[6]}\")\n",
    "        print(f\"sym_freq_8:                      {l_sym_freq_cnt[7]}\")\n",
    "        print(f\"sym_freq_7:                      {l_sym_freq_cnt[8]}\")\n",
    "        print(f\"sym_freq_6:                      {l_sym_freq_cnt[9]}\")\n",
    "        print(f\"sym_freq_5:                      {l_sym_freq_cnt[10]}\")\n",
    "        print(f\"sym_freq_4:                      {l_sym_freq_cnt[11]}\")\n",
    "        print(f\"sym_freq_3:                      {l_sym_freq_cnt[12]}\")\n",
    "        print(f\"sym_freq_2:                      {l_sym_freq_cnt[13]}\\n\")\n",
    "\n",
    "\n",
    "# #### Evaluate performance of symbols in set_lookback_slices versus SPY\n",
    "l_row_add_total = eval_grp_top_syms_n_freq(\n",
    "    df, max_lookback_slices, sets_lookback_slices, grp_top_set_syms_n_freq, days_lookbacks, days_eval, n_samples, n_top_syms, syms_start, syms_end, verbose\n",
    ")\n",
    "\n",
    "for row_add_total in l_row_add_total:\n",
    "    print(f\"row_add_total: {row_add_total}\")\n",
    "\n",
    "if store_results:  # record results to df\n",
    "    df_eval_results = pickle_load(path_data_dump, fp_df_eval_results)\n",
    "    print(f\"df_eval_results BEFORW store results:\\n{df_eval_results}\\n\")\n",
    "\n",
    "    for row_add_total in l_row_add_total:\n",
    "        # print(f'row_add_total: {row_add_total}')\n",
    "        df_eval_results.loc[len(df_eval_results)] = row_add_total\n",
    "        print(f\"appended row_add_total to df_eval_results:\\n{row_add_total}\\n\")\n",
    "\n",
    "    pickle_dump(df_eval_results, path_data_dump, fp_df_eval_results)\n",
    "    print(f\"Save results to: {fp_df_eval_results}\")\n",
    "    df_eval_results = pickle_load(path_data_dump, fp_df_eval_results)\n",
    "    print(f\"df_eval_results AFTER store results:\\n{df_eval_results}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from myUtils import pickle_load, pickle_dump\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_colwidth', 16)\n",
    "pd.set_option('display.width', 790)\n",
    "\n",
    "path_dir = \"C:/Users/ping/MyDrive/stocks/yfinance/\"\n",
    "path_data_dump = path_dir + \"VSCode_dump/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp_df_eval_results = f'df_eval_trash_old'\n",
    "fp_df_eval_results = f'df_eval_trash_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_add_total:\n",
      "['n_samples', 'days_lookbacks', 'days_eval', 'n_top_syms', 'syms_start', 'syms_end', 'sym_freq_cnt', 'grp(CAGR)_mean', 'grp(CAGR)_std', 'grp(CAGR)_mean/std', 'grp(CAGR/UI)_mean', 'grp(CAGR/UI)_std', 'grp(CAGR/UI)_mean/std', 'grp(CAGR/retnStd)_mean', 'grp(CAGR/retnStd)_std', 'grp(CAGR/retnStd)_mean/std', 'grp(retnStd/UI)_mean', 'grp(retnStd/UI)_std', 'grp(retnStd/UI)_mean/std', 'SPY_CAGR', 'SPY_CAGR/UI', 'SPY_CAGR/retnStd', 'SPY_retnStd/UI'], total columns: 23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_samples</th>\n",
       "      <th>days_lookbacks</th>\n",
       "      <th>days_eval</th>\n",
       "      <th>n_top_syms</th>\n",
       "      <th>syms_start</th>\n",
       "      <th>syms_end</th>\n",
       "      <th>sym_freq_cnt</th>\n",
       "      <th>grp(CAGR)_mean</th>\n",
       "      <th>grp(CAGR)_std</th>\n",
       "      <th>grp(CAGR)_mean/std</th>\n",
       "      <th>grp(CAGR/UI)_mean</th>\n",
       "      <th>grp(CAGR/UI)_std</th>\n",
       "      <th>grp(CAGR/UI)_mean/std</th>\n",
       "      <th>grp(CAGR/retnStd)_mean</th>\n",
       "      <th>grp(CAGR/retnStd)_std</th>\n",
       "      <th>grp(CAGR/retnStd)_mean/std</th>\n",
       "      <th>grp(retnStd/UI)_mean</th>\n",
       "      <th>grp(retnStd/UI)_std</th>\n",
       "      <th>grp(retnStd/UI)_mean/std</th>\n",
       "      <th>SPY_CAGR</th>\n",
       "      <th>SPY_CAGR/UI</th>\n",
       "      <th>SPY_CAGR/retnStd</th>\n",
       "      <th>SPY_retnStd/UI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [n_samples, days_lookbacks, days_eval, n_top_syms, syms_start, syms_end, sym_freq_cnt, grp(CAGR)_mean, grp(CAGR)_std, grp(CAGR)_mean/std, grp(CAGR/UI)_mean, grp(CAGR/UI)_std, grp(CAGR/UI)_mean/std, grp(CAGR/retnStd)_mean, grp(CAGR/retnStd)_std, grp(CAGR/retnStd)_mean/std, grp(retnStd/UI)_mean, grp(retnStd/UI)_std, grp(retnStd/UI)_mean/std, SPY_CAGR, SPY_CAGR/UI, SPY_CAGR/retnStd, SPY_retnStd/UI]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_add0 = ['n_samples', 'days_lookbacks', 'days_eval', 'n_top_syms', 'syms_start', 'syms_end', 'sym_freq_cnt']\n",
    "col_add1 = ['grp(CAGR)_mean',         'grp(CAGR)_std',         'grp(CAGR)_mean/std']\n",
    "col_add2 = ['grp(CAGR/UI)_mean',      'grp(CAGR/UI)_std',      'grp(CAGR/UI)_mean/std']\n",
    "col_add3 = ['grp(CAGR/retnStd)_mean', 'grp(CAGR/retnStd)_std', 'grp(CAGR/retnStd)_mean/std']\n",
    "col_add4 = ['grp(retnStd/UI)_mean',   'grp(retnStd/UI)_std',   'grp(retnStd/UI)_mean/std']\n",
    "col_add5 = ['SPY_CAGR', 'SPY_CAGR/UI', 'SPY_CAGR/retnStd', 'SPY_retnStd/UI']\n",
    "\n",
    "col_add_total = col_add0 + col_add1 + col_add2 + col_add3 + col_add4 + col_add5\n",
    "print(f'col_add_total:\\n{col_add_total}, total columns: {len(col_add_total)}')\n",
    "\n",
    "# # Creating Empty DataFrame and save it to file\n",
    "df = pd.DataFrame(columns=col_add_total)\n",
    "pickle_dump(df, path_data_dump, fp_df_eval_results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d848e2535a99fe7c7346179acd9000b04da131f0f89ee41d962201c665cb28e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
