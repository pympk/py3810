{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_set_sym_freq_cnt(top_set_syms_n_freq):\n",
    "    # accommodate upto 5 periods of days_lookbacks(i.e. days_lookbacks = [5, 10, 15, 20, 25], len(days_lookbacks)*3 = 15)\n",
    "    sym_freq_cnt_15 = []\n",
    "    sym_freq_cnt_14 = []\n",
    "    sym_freq_cnt_13 = []\n",
    "    sym_freq_cnt_12 = []\n",
    "    sym_freq_cnt_11 = []\n",
    "    sym_freq_cnt_10 = []\n",
    "    sym_freq_cnt_9 = []\n",
    "    sym_freq_cnt_8 = []\n",
    "    sym_freq_cnt_7 = []\n",
    "    sym_freq_cnt_6 = []\n",
    "    sym_freq_cnt_5 = []\n",
    "    sym_freq_cnt_4 = []\n",
    "    sym_freq_cnt_3 = []\n",
    "    sym_freq_cnt_2 = []\n",
    "\n",
    "    for sym_n_freq in top_set_syms_n_freq:\n",
    "        _sym = sym_n_freq[0]\n",
    "        _freq = sym_n_freq[1]\n",
    "        if _freq == 15:\n",
    "            sym_freq_cnt_15.append(_sym)\n",
    "        elif _freq == 14:\n",
    "            sym_freq_cnt_14.append(_sym)\n",
    "        elif _freq == 13:\n",
    "            sym_freq_cnt_13.append(_sym)\n",
    "        elif _freq == 12:\n",
    "            sym_freq_cnt_12.append(_sym)                        \n",
    "        elif _freq == 11:\n",
    "            sym_freq_cnt_11.append(_sym)\n",
    "        elif _freq == 10:\n",
    "            sym_freq_cnt_10.append(_sym)            \n",
    "        elif _freq == 9:\n",
    "            sym_freq_cnt_9.append(_sym)\n",
    "        elif _freq == 8:\n",
    "            sym_freq_cnt_8.append(_sym)\n",
    "        elif _freq == 7:\n",
    "            sym_freq_cnt_7.append(_sym)  \n",
    "        elif _freq == 6:\n",
    "            sym_freq_cnt_6.append(_sym)\n",
    "        elif _freq == 5:\n",
    "            sym_freq_cnt_5.append(_sym)\n",
    "        elif _freq == 4:\n",
    "            sym_freq_cnt_4.append(_sym)\n",
    "        elif _freq == 3:\n",
    "            sym_freq_cnt_3.append(_sym)          \n",
    "        else:\n",
    "            sym_freq_cnt_2.append(_sym)\n",
    "\n",
    "    l_sym_freq_cnt = []\n",
    "\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_15)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_14)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_13)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_12)    \n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_11)   \n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_10)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_9)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_8)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_7)    \n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_6)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_5)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_4)\n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_3)    \n",
    "    l_sym_freq_cnt.append(sym_freq_cnt_2)    \n",
    "\n",
    "    return l_sym_freq_cnt    \n",
    "\n",
    "def best_perf_syms_sets_lookback_slices(sets_lookback_slices, verbose=False):\n",
    "\n",
    "  # grp_top_set_syms_n_freq is a list of lists of top_set_syms_n_freq, e.g.\n",
    "  #   [[('AGY', 7), ('PCG', 7), ('KDN', 6), ..., ('CYT', 3)],\n",
    "  #    [('FCN', 9), ('HIG', 9), ('SJR', 8), ..., ('BFH', 2)]]\n",
    "  #   where each list is the best performing symbols from a lb_slices, e.g.\n",
    "  #     [(483, 513, 523), (453, 513, 523), (393, 513, 523)]  \n",
    "  grp_top_set_syms_n_freq = []  # list of lists of top_set_symbols_n_freq, there are n_samples lists in list\n",
    "  grp_top_set_syms = []  # grp_top_set_syms_n_freq without the frequency count\n",
    "\n",
    "  # lb_slices, e.g  [(483, 513, 523), (453, 513, 523), (393, 513, 523)],\n",
    "  #  is one max_lookback_slice, e.g. (393, 513, 523), along with\n",
    "  #  the remaining slices of the days_lookbacks, e.g. (483, 513, 523), (453, 513, 523)  \n",
    "  for i, lb_slices in enumerate(sets_lookback_slices):\n",
    "    print(f'\\n########## {i + 1} of {len(sets_lookback_slices)} lb_slices in sets_lookcak_slices ##########')\n",
    "    # unsorted list of the most frequent symbols in performance metrics of the lb_slices  \n",
    "    grp_most_freq_syms = []\n",
    "    for j, lb_slice in enumerate(lb_slices):  # lb_slice, e.g. (246, 276, 286)\n",
    "      iloc_start_train = lb_slice[0]     # iloc of start of training period\n",
    "      iloc_end_train   = lb_slice[1]     # iloc of end of training period\n",
    "      iloc_start_eval  = iloc_end_train  # iloc of start of evaluation period\n",
    "      iloc_end_eval    = lb_slice[2]     # iloc of end of evaluation period\n",
    "      lookback         = iloc_end_train - iloc_start_train\n",
    "      d_eval           = iloc_end_eval - iloc_start_eval\n",
    "\n",
    "      _df = df.iloc[iloc_start_train:iloc_end_train]\n",
    "      date_start_df_train = _df.index[0].strftime('%Y-%m-%d')\n",
    "      date_end_df_train = _df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "      if verbose:\n",
    "        print(f'days lookback:       {lookback},  {j + 1} of {len(days_lookbacks)} days_lookbacks: {days_lookbacks}')\n",
    "        print(f'lb_slices:           {lb_slices}')\n",
    "        print(f'lb_slice:            {lb_slice}')\n",
    "        print(f'days eval:           {d_eval}')    \n",
    "        print(f'iloc_start_train:    {iloc_start_train}')\n",
    "        print(f'iloc_end_train:      {iloc_end_train}')\n",
    "        print(f'date_start_df_train: {date_start_df_train}')\n",
    "        print(f'date_end_df_train:   {date_end_df_train}')\n",
    "\n",
    "\n",
    "      perf_ranks, most_freq_syms = _5_perf_ranks(_df, n_top_syms=n_top_syms)\n",
    "      # unsorted list of the most frequent symbols in performance metrics of the lb_slices  \n",
    "      grp_most_freq_syms.append(most_freq_syms)  \n",
    "      if verbose:    \n",
    "        # 1 lookback of r_CAGR/UI, r_CAGR/retnStd, r_retnStd/UI\n",
    "        print(f'perf_ranks: {perf_ranks}')  \n",
    "        # most common symbols of perf_ranks \n",
    "        print(f'most_freq_syms: {most_freq_syms}')     \n",
    "        # grp_perf_ranks[lookback] = perf_ranks\n",
    "        print(f'+++ finish lookback slice {lookback} +++\\n')\n",
    "\n",
    "    if verbose:\n",
    "      print(f'grp_most_freq_syms: {grp_most_freq_syms}')\n",
    "      # grp_most_freq_syms a is list of lists of tuples of \n",
    "      #  the most-common-symbols symbol:frequency cumulated from\n",
    "      #  each days_lookback  \n",
    "      print(f'**** finish lookback slices {lb_slices} ****\\n')\n",
    "\n",
    "    # flatten list of lists of (symbol:frequency)\n",
    "    flat_grp_most_freq_syms = [val for sublist in grp_most_freq_syms for val in sublist]\n",
    "    # return \"symbol, frequency\" pairs of the most frequent symbols, i.e. best performing symbols,\n",
    "    #  in flat_grp_most_freq_syms. The paris are sorted in descending frequency.   \n",
    "    set_most_freq_syms = _6_grp_tuples_sort_sum(flat_grp_most_freq_syms, reverse=True)\n",
    "    # get the top n_top_syms of the most frequent \"symbol, frequency\" pairs\n",
    "    top_set_syms_n_freq = set_most_freq_syms[0:n_top_syms]\n",
    "    # get symbols from top_set_syms_n_freq, i[0] = symbol, i[1]=symbol's frequency count\n",
    "    top_set_syms = [i[0] for i in top_set_syms_n_freq[syms_start:syms_end]]  \n",
    "\n",
    "    # grp_top_set_syms_n_freq is a list of lists of top_set_syms_n_freq, e.g.\n",
    "    #   [[('AGY', 7), ('PCG', 7), ('KDN', 6), ..., ('CYT', 3)],\n",
    "    #    [('FCN', 9), ('HIG', 9), ('SJR', 8), ..., ('BFH', 2)]]\n",
    "    #   where each list is the best performing symbols from a lb_slices, e.g.\n",
    "    #     [(483, 513, 523), (453, 513, 523), (393, 513, 523)]    \n",
    "    grp_top_set_syms_n_freq.append(top_set_syms_n_freq)\n",
    "    grp_top_set_syms.append(top_set_syms)\n",
    "\n",
    "    if verbose:  \n",
    "      print(f'top {n_top_syms} ranked symbols and frequency from set {lb_slices}:\\n{top_set_syms_n_freq}')\n",
    "      print(f'top {n_top_syms} ranked symbols from set {lb_slices}:\\n{top_set_syms}')  \n",
    "      print(f'===== finish top {n_top_syms} ranked symbols from days_lookback set {lb_slices} =====\\n\\n')\n",
    "\n",
    "  return   grp_top_set_syms_n_freq, grp_top_set_syms, date_end_df_train     \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import datetime\n",
    "# from IPython.display import display, HTML\n",
    "from yf_utils import _2_split_train_val_test, _3_random_slices, _4_lookback_slices\n",
    "from yf_utils import _5_perf_ranks, _6_grp_tuples_sort_sum\n",
    "from myUtils import pickle_load, pickle_dump\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_colwidth', 16)\n",
    "pd.set_option('display.width', 790)\n",
    "\n",
    "path_dir = \"C:/Users/ping/MyDrive/stocks/yfinance/\"\n",
    "path_data_dump = path_dir + \"VSCode_dump/\"\n",
    "\n",
    "fp_df_close_clean = 'df_close_clean'\n",
    "\n",
    "# verbose = False  # True prints more output\n",
    "verbose = True  # True prints more output\n",
    "\n",
    "# store_results = True\n",
    "store_results = False\n",
    "\n",
    "n_samples = 20\n",
    "\n",
    "# for training, the number of days to lookback from iloc max-lookback iloc_end_train\n",
    "days_lookbacks = [30, 60, 120]\n",
    "# days_lookbacks = [15, 30, 60, 120]\n",
    "days_lookbacks.sort()\n",
    "\n",
    "# number of days from iloc_end_train are used to evaluate effectiveness of the training\n",
    "days_eval = 4\n",
    "# days_eval = 5\n",
    "\n",
    "# number of the most-common symbols from days_lookbacks' performance rankings to keep\n",
    "n_top_syms = 20  \n",
    "\n",
    "# slice starts and ends for selecting the best performing symbols\n",
    "syms_start = 0\n",
    "syms_end = 10\n",
    "\n",
    "# get picks of previous days by dropping the last n rows from df_current\n",
    "#  drop_last_n_rows = 1 drops the last row from df_current\n",
    "drop_last_n_rows = 0\n",
    "\n",
    "# over-ride parameters for run_type 'current'\n",
    "# if run_type == 'current':\n",
    "days_eval = 0  # no need to eval, df_eval will be empty\n",
    "n_samples = 1  # no need to repeat sample the current result, repeat sample will yield the same tuple\n",
    "\n",
    "\n",
    "# fp_df_eval_results = f'df_eval_results_{run_type}'\n",
    "fp_df_picks = f'df_picks'\n",
    "\n",
    "print(f'verbose : {verbose }')\n",
    "print(f'store_results: {store_results}')\n",
    "print(f'n_samples: {n_samples}')\n",
    "print(f'days_lookbacks: {days_lookbacks}')\n",
    "print(f'days_eval: {days_eval}')\n",
    "print(f'n_top_syms: {n_top_syms}')\n",
    "print(f'syms_start: {syms_start}')\n",
    "print(f'syms_end: {syms_end}')\n",
    "print(f'fp_df_picks: {fp_df_picks}')\n",
    "\n",
    "df_picks = pickle_load(path_data_dump, fp_df_picks)\n",
    "df_close_clean = pickle_load(path_data_dump, fp_df_close_clean)\n",
    "\n",
    "max_days_lookbacks = max(days_lookbacks)\n",
    "print(f'max_days_lookbacks: {max_days_lookbacks}')\n",
    "\n",
    "df_current = df_close_clean.copy()\n",
    "\n",
    "print(f'run_type: current')\n",
    "slice_start = -(max_days_lookbacks + drop_last_n_rows)  \n",
    "slice_end = -drop_last_n_rows\n",
    "if drop_last_n_rows == 0:  # return df with all rows\n",
    "  df = df_current[slice_start:].copy()\n",
    "else:  # return df with dropped drop_last_n_rows rows  \n",
    "  df = df_current[slice_start:slice_end].copy()            \n",
    "print(f'dropped last {drop_last_n_rows} row(s) from df')          \n",
    "print(f'df.tail():\\n{df.tail()}\\n')\n",
    "print(f'df.tail(3):\\n{df.tail(3)}\\n')\n",
    "len_df = len(df)\n",
    "len_df_current = len(df_current)\n",
    "print(f'len(df): {len(df)}')\n",
    "\n",
    "\n",
    "# return n_samples slices\n",
    "max_lookback_slices = _3_random_slices(len_df, n_samples=n_samples, days_lookback=max(days_lookbacks), days_eval=days_eval, verbose=False)\n",
    "# return n_samples * len(days_lookbacks) slices\n",
    "sets_lookback_slices = _4_lookback_slices(max_slices=max_lookback_slices, days_lookbacks=days_lookbacks, verbose=False)\n",
    "\n",
    "if verbose:\n",
    "  print(f'number of max_lookback_slices is equal to n_samples = {n_samples}')\n",
    "  print(f'max_lookback_slices:\\n{max_lookback_slices}\\n')\n",
    "  print(f'days_lookbacks: {days_lookbacks}')  \n",
    "  print(f'sets_lookback_slices:\\n{sets_lookback_slices}\\n')\n",
    "  print(f'number of sets in sets_lookback_slices is equal to n_samples = {n_samples}')\n",
    "  print(f'number of tuples in each \"set of lookback slices\" is equal to len(days_lookbacks): {len(days_lookbacks)}')    \n",
    "\n",
    "grp_top_set_syms_n_freq, grp_top_set_syms, date_end_df_train = best_perf_syms_sets_lookback_slices(sets_lookback_slices, verbose=verbose)\n",
    "\n",
    "for i, top_set_syms_n_freq in enumerate(grp_top_set_syms_n_freq):\n",
    "  l_sym_freq_cnt = top_set_sym_freq_cnt(top_set_syms_n_freq)\n",
    "  if verbose:\n",
    "    print(f'set_lookback_slices: {sets_lookback_slices[i]}')\n",
    "    print(f'max_lookback_slices: {max_lookback_slices}\\n')\n",
    "    print(f'data below will be added to {fp_df_picks}')\n",
    "    print(f'date_end_df_train:   {date_end_df_train}')    \n",
    "    print(f'max_days_lookbacks:  {max_days_lookbacks}')   \n",
    "    print(f'days_lookbacks:      {days_lookbacks}')\n",
    "    print(f'sym_freq_15:         {l_sym_freq_cnt[0]}')\n",
    "    print(f'sym_freq_14:         {l_sym_freq_cnt[1]}')\n",
    "    print(f'sym_freq_13:         {l_sym_freq_cnt[2]}')\n",
    "    print(f'sym_freq_12:         {l_sym_freq_cnt[3]}')\n",
    "    print(f'sym_freq_11:         {l_sym_freq_cnt[4]}')\n",
    "    print(f'sym_freq_10:         {l_sym_freq_cnt[5]}')\n",
    "    print(f'sym_freq_9:          {l_sym_freq_cnt[6]}')\n",
    "    print(f'sym_freq_8:          {l_sym_freq_cnt[7]}')\n",
    "    print(f'sym_freq_7:          {l_sym_freq_cnt[8]}')\n",
    "    print(f'sym_freq_6:          {l_sym_freq_cnt[9]}')\n",
    "    print(f'sym_freq_5:          {l_sym_freq_cnt[10]}')\n",
    "    print(f'sym_freq_4:          {l_sym_freq_cnt[11]}')\n",
    "    print(f'sym_freq_3:          {l_sym_freq_cnt[12]}')\n",
    "    print(f'sym_freq_2:          {l_sym_freq_cnt[13]}\\n')\n",
    "\n",
    "if store_results:\n",
    "  row_picks0      = [date_end_df_train, max_days_lookbacks, str(days_lookbacks)]\n",
    "  row_picks1      = [str(l_sym_freq_cnt[0]),  str(l_sym_freq_cnt[1]), str(l_sym_freq_cnt[2]),  str(l_sym_freq_cnt[3])]\n",
    "  row_picks2      = [str(l_sym_freq_cnt[4]),  str(l_sym_freq_cnt[5]), str(l_sym_freq_cnt[6]),  str(l_sym_freq_cnt[7])]\n",
    "  row_picks3      = [str(l_sym_freq_cnt[8]),  str(l_sym_freq_cnt[9]), str(l_sym_freq_cnt[10]), str(l_sym_freq_cnt[11])]\n",
    "  row_picks4      = [str(l_sym_freq_cnt[12]), str(l_sym_freq_cnt[13])]\n",
    "  row_picks_total = row_picks0 + row_picks1 + row_picks2 + row_picks3 + row_picks4\n",
    "  print(f'row_picks_total: {row_picks_total}')\n",
    "\n",
    "  df_picks.loc[len(df_picks)] = row_picks_total\n",
    "  pickle_dump(df_picks, path_data_dump, fp_df_picks)\n",
    "  print(f'appended row_picks_total to df_picks:\\n{row_picks_total}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
